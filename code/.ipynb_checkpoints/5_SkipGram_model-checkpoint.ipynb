{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size):\n",
    "    idx_pairs = []\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            # for each context word within window\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = sentence[context_word_pos]\n",
    "                idx_pairs.append((center_word_idx, context_word_idx))\n",
    "                    \n",
    "    return idx_pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 46007576 word pairs\n"
     ]
    }
   ],
   "source": [
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "idx_pairs_SG = create_word_pairs(corpus, window_size = 5)\n",
    "print('totally {0} word pairs'.format(len(idx_pairs_SG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "wordlist.append(['UNK','0'])\n",
    "word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w[0] for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 10000 pivot words\n"
     ]
    }
   ],
   "source": [
    "f = open('pivots_google_10000.txt','r')\n",
    "a = f.read()\n",
    "pivots_vec = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words'.format(len(list(pivots_vec.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "pivots_100 = dict_slice(pivots_vec, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivots_idx = []\n",
    "pivots_vec = []\n",
    "for i in pivots_100.keys():\n",
    "    pivots_idx.append(i)\n",
    "    pivots_vec.append(pivots_100[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_shuffle = corpus[:]\n",
    "random.shuffle(corpus_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_shuffle = idx_pairs_SG[:]\n",
    "random.shuffle(idx_pairs_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_10m = idx_pairs_shuffle[:10000000]\n",
    "idx_pairs_5m = idx_pairs_shuffle[:5000000]\n",
    "idx_pairs_1m = idx_pairs_shuffle[:1000000]\n",
    "idx_pairs_500k = idx_pairs_shuffle[:500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 1\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1000 # number of samples each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    # embedding layer\n",
    "    init_width = 0.5 / n_embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # add regularization term\n",
    "    embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "    update_embed_op = tf.scatter_update(embedding_copy,pivots_idx,pivots_vec)\n",
    "    reg_loss = tf.reduce_sum((embedding-embedding_copy)**2)\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2018-07-01 22:29:01.569067\n",
      "Epoch 1/10 Iteration: 100 Avg. Training loss: 143.1327 Avg. Reg. loss: 134.7475 0.0511 sec/batch\n",
      "Epoch 1/10 Iteration: 200 Avg. Training loss: 78.6602 Avg. Reg. loss: 70.8882 0.0444 sec/batch\n",
      "Epoch 1/10 Iteration: 300 Avg. Training loss: 47.9476 Avg. Reg. loss: 40.3660 0.0572 sec/batch\n",
      "Epoch 1/10 Iteration: 400 Avg. Training loss: 31.2946 Avg. Reg. loss: 23.9538 0.0468 sec/batch\n",
      "Epoch 1/10 Iteration: 500 Avg. Training loss: 21.6965 Avg. Reg. loss: 14.4811 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 600 Avg. Training loss: 15.8977 Avg. Reg. loss: 8.8075 0.0467 sec/batch\n",
      "Epoch 1/10 Iteration: 700 Avg. Training loss: 12.3788 Avg. Reg. loss: 5.3570 0.0512 sec/batch\n",
      "Epoch 1/10 Iteration: 800 Avg. Training loss: 10.1588 Avg. Reg. loss: 3.2565 0.0468 sec/batch\n",
      "Epoch 1/10 Iteration: 900 Avg. Training loss: 8.7913 Avg. Reg. loss: 1.9846 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 7.9647 Avg. Reg. loss: 1.2181 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 1100 Avg. Training loss: 7.4312 Avg. Reg. loss: 0.7607 0.0622 sec/batch\n",
      "Epoch 1/10 Iteration: 1200 Avg. Training loss: 7.0598 Avg. Reg. loss: 0.4882 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 1300 Avg. Training loss: 6.8918 Avg. Reg. loss: 0.3253 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 1400 Avg. Training loss: 6.7221 Avg. Reg. loss: 0.2268 0.0506 sec/batch\n",
      "Epoch 1/10 Iteration: 1500 Avg. Training loss: 6.6266 Avg. Reg. loss: 0.1637 0.0511 sec/batch\n",
      "Epoch 1/10 Iteration: 1600 Avg. Training loss: 6.4903 Avg. Reg. loss: 0.1257 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 1700 Avg. Training loss: 6.4312 Avg. Reg. loss: 0.0993 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 1800 Avg. Training loss: 6.4008 Avg. Reg. loss: 0.0820 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 1900 Avg. Training loss: 6.3518 Avg. Reg. loss: 0.0685 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 6.2698 Avg. Reg. loss: 0.0601 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 2100 Avg. Training loss: 6.2373 Avg. Reg. loss: 0.0531 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 2200 Avg. Training loss: 6.1895 Avg. Reg. loss: 0.0492 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 2300 Avg. Training loss: 6.1580 Avg. Reg. loss: 0.0459 0.0495 sec/batch\n",
      "Epoch 1/10 Iteration: 2400 Avg. Training loss: 6.1553 Avg. Reg. loss: 0.0434 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 2500 Avg. Training loss: 6.1090 Avg. Reg. loss: 0.0424 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 2600 Avg. Training loss: 6.0813 Avg. Reg. loss: 0.0409 0.0492 sec/batch\n",
      "Epoch 1/10 Iteration: 2700 Avg. Training loss: 6.0501 Avg. Reg. loss: 0.0407 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 2800 Avg. Training loss: 6.0418 Avg. Reg. loss: 0.0400 0.0624 sec/batch\n",
      "Epoch 1/10 Iteration: 2900 Avg. Training loss: 6.0095 Avg. Reg. loss: 0.0395 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.9738 Avg. Reg. loss: 0.0389 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 3100 Avg. Training loss: 5.9421 Avg. Reg. loss: 0.0385 0.0509 sec/batch\n",
      "Epoch 1/10 Iteration: 3200 Avg. Training loss: 5.9144 Avg. Reg. loss: 0.0389 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 3300 Avg. Training loss: 5.9208 Avg. Reg. loss: 0.0386 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 3400 Avg. Training loss: 5.8816 Avg. Reg. loss: 0.0382 0.0496 sec/batch\n",
      "Epoch 1/10 Iteration: 3500 Avg. Training loss: 5.8408 Avg. Reg. loss: 0.0378 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 3600 Avg. Training loss: 5.8373 Avg. Reg. loss: 0.0395 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 3700 Avg. Training loss: 5.8145 Avg. Reg. loss: 0.0376 0.0473 sec/batch\n",
      "Epoch 1/10 Iteration: 3800 Avg. Training loss: 5.7741 Avg. Reg. loss: 0.0380 0.0471 sec/batch\n",
      "Epoch 1/10 Iteration: 3900 Avg. Training loss: 5.7648 Avg. Reg. loss: 0.0370 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 5.7588 Avg. Reg. loss: 0.0377 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 4100 Avg. Training loss: 5.7329 Avg. Reg. loss: 0.0378 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 4200 Avg. Training loss: 5.7084 Avg. Reg. loss: 0.0375 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 4300 Avg. Training loss: 5.7123 Avg. Reg. loss: 0.0370 0.0471 sec/batch\n",
      "Epoch 1/10 Iteration: 4400 Avg. Training loss: 5.6642 Avg. Reg. loss: 0.0362 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 4500 Avg. Training loss: 5.6432 Avg. Reg. loss: 0.0370 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 4600 Avg. Training loss: 5.6579 Avg. Reg. loss: 0.0374 0.0477 sec/batch\n",
      "Epoch 1/10 Iteration: 4700 Avg. Training loss: 5.6649 Avg. Reg. loss: 0.0364 0.0463 sec/batch\n",
      "Epoch 1/10 Iteration: 4800 Avg. Training loss: 5.6015 Avg. Reg. loss: 0.0363 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 4900 Avg. Training loss: 5.5548 Avg. Reg. loss: 0.0364 0.0461 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 5.5642 Avg. Reg. loss: 0.0359 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 5100 Avg. Training loss: 5.5124 Avg. Reg. loss: 0.0362 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 5200 Avg. Training loss: 5.5333 Avg. Reg. loss: 0.0365 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 5300 Avg. Training loss: 5.5048 Avg. Reg. loss: 0.0361 0.0473 sec/batch\n",
      "Epoch 1/10 Iteration: 5400 Avg. Training loss: 5.5088 Avg. Reg. loss: 0.0353 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 5500 Avg. Training loss: 5.4778 Avg. Reg. loss: 0.0362 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 5600 Avg. Training loss: 5.4503 Avg. Reg. loss: 0.0356 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 5700 Avg. Training loss: 5.4456 Avg. Reg. loss: 0.0351 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 5800 Avg. Training loss: 5.4518 Avg. Reg. loss: 0.0352 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 5900 Avg. Training loss: 5.4598 Avg. Reg. loss: 0.0351 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 5.4194 Avg. Reg. loss: 0.0352 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 6100 Avg. Training loss: 5.3906 Avg. Reg. loss: 0.0341 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 6200 Avg. Training loss: 5.3899 Avg. Reg. loss: 0.0347 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 6300 Avg. Training loss: 5.3704 Avg. Reg. loss: 0.0332 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 6400 Avg. Training loss: 5.3418 Avg. Reg. loss: 0.0344 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 6500 Avg. Training loss: 5.3486 Avg. Reg. loss: 0.0335 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 6600 Avg. Training loss: 5.3238 Avg. Reg. loss: 0.0337 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 6700 Avg. Training loss: 5.3175 Avg. Reg. loss: 0.0345 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 6800 Avg. Training loss: 5.2892 Avg. Reg. loss: 0.0339 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 6900 Avg. Training loss: 5.2812 Avg. Reg. loss: 0.0328 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 5.2759 Avg. Reg. loss: 0.0328 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 7100 Avg. Training loss: 5.2326 Avg. Reg. loss: 0.0334 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 7200 Avg. Training loss: 5.2337 Avg. Reg. loss: 0.0327 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 7300 Avg. Training loss: 5.2447 Avg. Reg. loss: 0.0335 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 7400 Avg. Training loss: 5.2370 Avg. Reg. loss: 0.0334 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 7500 Avg. Training loss: 5.2345 Avg. Reg. loss: 0.0330 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 7600 Avg. Training loss: 5.1878 Avg. Reg. loss: 0.0331 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 7700 Avg. Training loss: 5.1710 Avg. Reg. loss: 0.0321 0.0511 sec/batch\n",
      "Epoch 1/10 Iteration: 7800 Avg. Training loss: 5.1752 Avg. Reg. loss: 0.0323 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 7900 Avg. Training loss: 5.1571 Avg. Reg. loss: 0.0317 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 5.1580 Avg. Reg. loss: 0.0323 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 8100 Avg. Training loss: 5.1455 Avg. Reg. loss: 0.0314 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 8200 Avg. Training loss: 5.1261 Avg. Reg. loss: 0.0315 0.0495 sec/batch\n",
      "Epoch 1/10 Iteration: 8300 Avg. Training loss: 5.1081 Avg. Reg. loss: 0.0306 0.0495 sec/batch\n",
      "Epoch 1/10 Iteration: 8400 Avg. Training loss: 5.1031 Avg. Reg. loss: 0.0312 0.0497 sec/batch\n",
      "Epoch 1/10 Iteration: 8500 Avg. Training loss: 5.0995 Avg. Reg. loss: 0.0309 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 8600 Avg. Training loss: 5.0830 Avg. Reg. loss: 0.0300 0.0492 sec/batch\n",
      "Epoch 1/10 Iteration: 8700 Avg. Training loss: 5.0592 Avg. Reg. loss: 0.0305 0.0486 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 8800 Avg. Training loss: 5.0737 Avg. Reg. loss: 0.0307 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 8900 Avg. Training loss: 5.0953 Avg. Reg. loss: 0.0303 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 5.0479 Avg. Reg. loss: 0.0299 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 9100 Avg. Training loss: 5.0493 Avg. Reg. loss: 0.0304 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 9200 Avg. Training loss: 5.0312 Avg. Reg. loss: 0.0299 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 9300 Avg. Training loss: 5.0313 Avg. Reg. loss: 0.0297 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 9400 Avg. Training loss: 5.0180 Avg. Reg. loss: 0.0301 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 9500 Avg. Training loss: 5.0046 Avg. Reg. loss: 0.0290 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 9600 Avg. Training loss: 4.9806 Avg. Reg. loss: 0.0290 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 9700 Avg. Training loss: 5.0163 Avg. Reg. loss: 0.0295 0.0492 sec/batch\n",
      "Epoch 1/10 Iteration: 9800 Avg. Training loss: 4.9799 Avg. Reg. loss: 0.0298 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 9900 Avg. Training loss: 4.9652 Avg. Reg. loss: 0.0294 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.9598 Avg. Reg. loss: 0.0289 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 10100 Avg. Training loss: 4.9696 Avg. Reg. loss: 0.0295 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 10200 Avg. Training loss: 4.9522 Avg. Reg. loss: 0.0289 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 10300 Avg. Training loss: 4.9502 Avg. Reg. loss: 0.0292 0.0527 sec/batch\n",
      "Epoch 1/10 Iteration: 10400 Avg. Training loss: 4.9233 Avg. Reg. loss: 0.0276 0.0465 sec/batch\n",
      "Epoch 1/10 Iteration: 10500 Avg. Training loss: 4.9301 Avg. Reg. loss: 0.0284 0.0461 sec/batch\n",
      "Epoch 1/10 Iteration: 10600 Avg. Training loss: 4.9230 Avg. Reg. loss: 0.0284 0.0475 sec/batch\n",
      "Epoch 1/10 Iteration: 10700 Avg. Training loss: 4.9203 Avg. Reg. loss: 0.0280 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 10800 Avg. Training loss: 4.9339 Avg. Reg. loss: 0.0275 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 10900 Avg. Training loss: 4.9243 Avg. Reg. loss: 0.0281 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.8946 Avg. Reg. loss: 0.0286 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 11100 Avg. Training loss: 4.9021 Avg. Reg. loss: 0.0279 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 11200 Avg. Training loss: 4.9114 Avg. Reg. loss: 0.0280 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 11300 Avg. Training loss: 4.8919 Avg. Reg. loss: 0.0276 0.0475 sec/batch\n",
      "Epoch 1/10 Iteration: 11400 Avg. Training loss: 4.8804 Avg. Reg. loss: 0.0278 0.0478 sec/batch\n",
      "Epoch 1/10 Iteration: 11500 Avg. Training loss: 4.8892 Avg. Reg. loss: 0.0284 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 11600 Avg. Training loss: 4.8778 Avg. Reg. loss: 0.0276 0.0497 sec/batch\n",
      "Epoch 1/10 Iteration: 11700 Avg. Training loss: 4.8746 Avg. Reg. loss: 0.0273 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 11800 Avg. Training loss: 4.8724 Avg. Reg. loss: 0.0273 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 11900 Avg. Training loss: 4.8537 Avg. Reg. loss: 0.0269 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.8637 Avg. Reg. loss: 0.0270 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 12100 Avg. Training loss: 4.8502 Avg. Reg. loss: 0.0273 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 12200 Avg. Training loss: 4.8593 Avg. Reg. loss: 0.0269 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 12300 Avg. Training loss: 4.8290 Avg. Reg. loss: 0.0267 0.0478 sec/batch\n",
      "Epoch 1/10 Iteration: 12400 Avg. Training loss: 4.8459 Avg. Reg. loss: 0.0267 0.0474 sec/batch\n",
      "Epoch 1/10 Iteration: 12500 Avg. Training loss: 4.8170 Avg. Reg. loss: 0.0271 0.0476 sec/batch\n",
      "Epoch 1/10 Iteration: 12600 Avg. Training loss: 4.8702 Avg. Reg. loss: 0.0268 0.0473 sec/batch\n",
      "Epoch 1/10 Iteration: 12700 Avg. Training loss: 4.8244 Avg. Reg. loss: 0.0271 0.0471 sec/batch\n",
      "Epoch 1/10 Iteration: 12800 Avg. Training loss: 4.8378 Avg. Reg. loss: 0.0271 0.0471 sec/batch\n",
      "Epoch 1/10 Iteration: 12900 Avg. Training loss: 4.8380 Avg. Reg. loss: 0.0274 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.8502 Avg. Reg. loss: 0.0273 0.0475 sec/batch\n",
      "Epoch 1/10 Iteration: 13100 Avg. Training loss: 4.8012 Avg. Reg. loss: 0.0270 0.0471 sec/batch\n",
      "Epoch 1/10 Iteration: 13200 Avg. Training loss: 4.7869 Avg. Reg. loss: 0.0260 0.0474 sec/batch\n",
      "Epoch 1/10 Iteration: 13300 Avg. Training loss: 4.7983 Avg. Reg. loss: 0.0264 0.0474 sec/batch\n",
      "Epoch 1/10 Iteration: 13400 Avg. Training loss: 4.8218 Avg. Reg. loss: 0.0262 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 13500 Avg. Training loss: 4.8050 Avg. Reg. loss: 0.0261 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 13600 Avg. Training loss: 4.7873 Avg. Reg. loss: 0.0265 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 13700 Avg. Training loss: 4.7824 Avg. Reg. loss: 0.0266 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 13800 Avg. Training loss: 4.7928 Avg. Reg. loss: 0.0265 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 13900 Avg. Training loss: 4.7983 Avg. Reg. loss: 0.0270 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.7677 Avg. Reg. loss: 0.0261 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 14100 Avg. Training loss: 4.7649 Avg. Reg. loss: 0.0263 0.0522 sec/batch\n",
      "Epoch 1/10 Iteration: 14200 Avg. Training loss: 4.7904 Avg. Reg. loss: 0.0266 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 14300 Avg. Training loss: 4.7804 Avg. Reg. loss: 0.0264 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 14400 Avg. Training loss: 4.7626 Avg. Reg. loss: 0.0260 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 14500 Avg. Training loss: 4.7480 Avg. Reg. loss: 0.0259 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 14600 Avg. Training loss: 4.8225 Avg. Reg. loss: 0.0274 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 14700 Avg. Training loss: 4.7512 Avg. Reg. loss: 0.0258 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 14800 Avg. Training loss: 4.7527 Avg. Reg. loss: 0.0258 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 14900 Avg. Training loss: 4.7452 Avg. Reg. loss: 0.0263 0.0476 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.7221 Avg. Reg. loss: 0.0249 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 15100 Avg. Training loss: 4.7679 Avg. Reg. loss: 0.0256 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 15200 Avg. Training loss: 4.7419 Avg. Reg. loss: 0.0255 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 15300 Avg. Training loss: 4.7362 Avg. Reg. loss: 0.0263 0.0626 sec/batch\n",
      "Epoch 1/10 Iteration: 15400 Avg. Training loss: 4.7386 Avg. Reg. loss: 0.0253 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 15500 Avg. Training loss: 4.7250 Avg. Reg. loss: 0.0261 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 15600 Avg. Training loss: 4.7088 Avg. Reg. loss: 0.0254 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 15700 Avg. Training loss: 4.7269 Avg. Reg. loss: 0.0250 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 15800 Avg. Training loss: 4.7149 Avg. Reg. loss: 0.0259 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 15900 Avg. Training loss: 4.7351 Avg. Reg. loss: 0.0255 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.7312 Avg. Reg. loss: 0.0259 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 16100 Avg. Training loss: 4.7130 Avg. Reg. loss: 0.0251 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 16200 Avg. Training loss: 4.6992 Avg. Reg. loss: 0.0258 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 16300 Avg. Training loss: 4.7136 Avg. Reg. loss: 0.0250 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 16400 Avg. Training loss: 4.7178 Avg. Reg. loss: 0.0258 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 16500 Avg. Training loss: 4.7396 Avg. Reg. loss: 0.0264 0.0477 sec/batch\n",
      "Epoch 1/10 Iteration: 16600 Avg. Training loss: 4.7118 Avg. Reg. loss: 0.0253 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 16700 Avg. Training loss: 4.6843 Avg. Reg. loss: 0.0250 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 16800 Avg. Training loss: 4.6741 Avg. Reg. loss: 0.0256 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 16900 Avg. Training loss: 4.6900 Avg. Reg. loss: 0.0247 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 17000 Avg. Training loss: 4.6998 Avg. Reg. loss: 0.0255 0.0496 sec/batch\n",
      "Epoch 1/10 Iteration: 17100 Avg. Training loss: 4.7016 Avg. Reg. loss: 0.0251 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 17200 Avg. Training loss: 4.6979 Avg. Reg. loss: 0.0258 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 17300 Avg. Training loss: 4.6684 Avg. Reg. loss: 0.0256 0.0492 sec/batch\n",
      "Epoch 1/10 Iteration: 17400 Avg. Training loss: 4.6925 Avg. Reg. loss: 0.0263 0.0508 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 17500 Avg. Training loss: 4.6882 Avg. Reg. loss: 0.0256 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 17600 Avg. Training loss: 4.6640 Avg. Reg. loss: 0.0251 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 17700 Avg. Training loss: 4.6548 Avg. Reg. loss: 0.0252 0.0522 sec/batch\n",
      "Epoch 1/10 Iteration: 17800 Avg. Training loss: 4.6890 Avg. Reg. loss: 0.0253 0.0508 sec/batch\n",
      "Epoch 1/10 Iteration: 17900 Avg. Training loss: 4.6558 Avg. Reg. loss: 0.0247 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 18000 Avg. Training loss: 4.6617 Avg. Reg. loss: 0.0245 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 18100 Avg. Training loss: 4.6721 Avg. Reg. loss: 0.0247 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 18200 Avg. Training loss: 4.6778 Avg. Reg. loss: 0.0249 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 18300 Avg. Training loss: 4.6746 Avg. Reg. loss: 0.0245 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 18400 Avg. Training loss: 4.6517 Avg. Reg. loss: 0.0247 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 18500 Avg. Training loss: 4.6487 Avg. Reg. loss: 0.0251 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 18600 Avg. Training loss: 4.6784 Avg. Reg. loss: 0.0252 0.0497 sec/batch\n",
      "Epoch 1/10 Iteration: 18700 Avg. Training loss: 4.6624 Avg. Reg. loss: 0.0248 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 18800 Avg. Training loss: 4.6591 Avg. Reg. loss: 0.0241 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 18900 Avg. Training loss: 4.6529 Avg. Reg. loss: 0.0251 0.0443 sec/batch\n",
      "Epoch 1/10 Iteration: 19000 Avg. Training loss: 4.6633 Avg. Reg. loss: 0.0246 0.0425 sec/batch\n",
      "Epoch 1/10 Iteration: 19100 Avg. Training loss: 4.6513 Avg. Reg. loss: 0.0246 0.0423 sec/batch\n",
      "Epoch 1/10 Iteration: 19200 Avg. Training loss: 4.6352 Avg. Reg. loss: 0.0249 0.0429 sec/batch\n",
      "Epoch 1/10 Iteration: 19300 Avg. Training loss: 4.6546 Avg. Reg. loss: 0.0249 0.0440 sec/batch\n",
      "Epoch 1/10 Iteration: 19400 Avg. Training loss: 4.6763 Avg. Reg. loss: 0.0253 0.0436 sec/batch\n",
      "Epoch 1/10 Iteration: 19500 Avg. Training loss: 4.6404 Avg. Reg. loss: 0.0249 0.0473 sec/batch\n",
      "Epoch 1/10 Iteration: 19600 Avg. Training loss: 4.6340 Avg. Reg. loss: 0.0250 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 19700 Avg. Training loss: 4.6271 Avg. Reg. loss: 0.0243 0.0512 sec/batch\n",
      "Epoch 1/10 Iteration: 19800 Avg. Training loss: 4.6430 Avg. Reg. loss: 0.0243 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 19900 Avg. Training loss: 4.6382 Avg. Reg. loss: 0.0249 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 20000 Avg. Training loss: 4.6246 Avg. Reg. loss: 0.0248 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 20100 Avg. Training loss: 4.6221 Avg. Reg. loss: 0.0244 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 20200 Avg. Training loss: 4.6409 Avg. Reg. loss: 0.0252 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 20300 Avg. Training loss: 4.6320 Avg. Reg. loss: 0.0242 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 20400 Avg. Training loss: 4.6401 Avg. Reg. loss: 0.0246 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 20500 Avg. Training loss: 4.6387 Avg. Reg. loss: 0.0242 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 20600 Avg. Training loss: 4.6291 Avg. Reg. loss: 0.0239 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 20700 Avg. Training loss: 4.6228 Avg. Reg. loss: 0.0247 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 20800 Avg. Training loss: 4.6151 Avg. Reg. loss: 0.0242 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 20900 Avg. Training loss: 4.6224 Avg. Reg. loss: 0.0252 0.0492 sec/batch\n",
      "Epoch 1/10 Iteration: 21000 Avg. Training loss: 4.6083 Avg. Reg. loss: 0.0254 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 21100 Avg. Training loss: 4.6286 Avg. Reg. loss: 0.0251 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 21200 Avg. Training loss: 4.6024 Avg. Reg. loss: 0.0238 0.0497 sec/batch\n",
      "Epoch 1/10 Iteration: 21300 Avg. Training loss: 4.6112 Avg. Reg. loss: 0.0243 0.0514 sec/batch\n",
      "Epoch 1/10 Iteration: 21400 Avg. Training loss: 4.6169 Avg. Reg. loss: 0.0251 0.0532 sec/batch\n",
      "Epoch 1/10 Iteration: 21500 Avg. Training loss: 4.6212 Avg. Reg. loss: 0.0248 0.0517 sec/batch\n",
      "Epoch 1/10 Iteration: 21600 Avg. Training loss: 4.6176 Avg. Reg. loss: 0.0246 0.0512 sec/batch\n",
      "Epoch 1/10 Iteration: 21700 Avg. Training loss: 4.6172 Avg. Reg. loss: 0.0247 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 21800 Avg. Training loss: 4.5944 Avg. Reg. loss: 0.0245 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 21900 Avg. Training loss: 4.6482 Avg. Reg. loss: 0.0247 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 22000 Avg. Training loss: 4.5814 Avg. Reg. loss: 0.0245 0.0510 sec/batch\n",
      "Epoch 1/10 Iteration: 22100 Avg. Training loss: 4.5902 Avg. Reg. loss: 0.0247 0.0506 sec/batch\n",
      "Epoch 1/10 Iteration: 22200 Avg. Training loss: 4.6077 Avg. Reg. loss: 0.0240 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 22300 Avg. Training loss: 4.6019 Avg. Reg. loss: 0.0239 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 22400 Avg. Training loss: 4.6045 Avg. Reg. loss: 0.0241 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 22500 Avg. Training loss: 4.5853 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 22600 Avg. Training loss: 4.5939 Avg. Reg. loss: 0.0252 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 22700 Avg. Training loss: 4.5899 Avg. Reg. loss: 0.0247 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 22800 Avg. Training loss: 4.6084 Avg. Reg. loss: 0.0244 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 22900 Avg. Training loss: 4.5946 Avg. Reg. loss: 0.0241 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 23000 Avg. Training loss: 4.5855 Avg. Reg. loss: 0.0244 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 23100 Avg. Training loss: 4.5905 Avg. Reg. loss: 0.0239 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 23200 Avg. Training loss: 4.6026 Avg. Reg. loss: 0.0240 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 23300 Avg. Training loss: 4.5796 Avg. Reg. loss: 0.0240 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 23400 Avg. Training loss: 4.5719 Avg. Reg. loss: 0.0244 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 23500 Avg. Training loss: 4.5933 Avg. Reg. loss: 0.0249 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 23600 Avg. Training loss: 4.5682 Avg. Reg. loss: 0.0236 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 23700 Avg. Training loss: 4.6281 Avg. Reg. loss: 0.0250 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 23800 Avg. Training loss: 4.5862 Avg. Reg. loss: 0.0244 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 23900 Avg. Training loss: 4.5911 Avg. Reg. loss: 0.0243 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 24000 Avg. Training loss: 4.5868 Avg. Reg. loss: 0.0238 0.0469 sec/batch\n",
      "Epoch 1/10 Iteration: 24100 Avg. Training loss: 4.5899 Avg. Reg. loss: 0.0243 0.0429 sec/batch\n",
      "Epoch 1/10 Iteration: 24200 Avg. Training loss: 4.5852 Avg. Reg. loss: 0.0235 0.0436 sec/batch\n",
      "Epoch 1/10 Iteration: 24300 Avg. Training loss: 4.5734 Avg. Reg. loss: 0.0242 0.0437 sec/batch\n",
      "Epoch 1/10 Iteration: 24400 Avg. Training loss: 4.5674 Avg. Reg. loss: 0.0242 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 24500 Avg. Training loss: 4.5601 Avg. Reg. loss: 0.0249 0.0512 sec/batch\n",
      "Epoch 1/10 Iteration: 24600 Avg. Training loss: 4.5832 Avg. Reg. loss: 0.0245 0.0547 sec/batch\n",
      "Epoch 1/10 Iteration: 24700 Avg. Training loss: 4.5666 Avg. Reg. loss: 0.0235 0.0535 sec/batch\n",
      "Epoch 1/10 Iteration: 24800 Avg. Training loss: 4.5678 Avg. Reg. loss: 0.0240 0.0512 sec/batch\n",
      "Epoch 1/10 Iteration: 24900 Avg. Training loss: 4.5526 Avg. Reg. loss: 0.0237 0.0551 sec/batch\n",
      "Epoch 1/10 Iteration: 25000 Avg. Training loss: 4.5732 Avg. Reg. loss: 0.0239 0.0513 sec/batch\n",
      "Epoch 1/10 Iteration: 25100 Avg. Training loss: 4.5587 Avg. Reg. loss: 0.0242 0.0511 sec/batch\n",
      "Epoch 1/10 Iteration: 25200 Avg. Training loss: 4.5604 Avg. Reg. loss: 0.0239 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 25300 Avg. Training loss: 4.5596 Avg. Reg. loss: 0.0239 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 25400 Avg. Training loss: 4.5561 Avg. Reg. loss: 0.0238 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 25500 Avg. Training loss: 4.5613 Avg. Reg. loss: 0.0243 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 25600 Avg. Training loss: 4.5515 Avg. Reg. loss: 0.0238 0.0519 sec/batch\n",
      "Epoch 1/10 Iteration: 25700 Avg. Training loss: 4.5725 Avg. Reg. loss: 0.0240 0.0561 sec/batch\n",
      "Epoch 1/10 Iteration: 25800 Avg. Training loss: 4.5449 Avg. Reg. loss: 0.0239 0.0545 sec/batch\n",
      "Epoch 1/10 Iteration: 25900 Avg. Training loss: 4.5612 Avg. Reg. loss: 0.0241 0.0546 sec/batch\n",
      "Epoch 1/10 Iteration: 26000 Avg. Training loss: 4.5612 Avg. Reg. loss: 0.0242 0.0525 sec/batch\n",
      "Epoch 1/10 Iteration: 26100 Avg. Training loss: 4.5288 Avg. Reg. loss: 0.0236 0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 26200 Avg. Training loss: 4.5582 Avg. Reg. loss: 0.0243 0.0523 sec/batch\n",
      "Epoch 1/10 Iteration: 26300 Avg. Training loss: 4.5464 Avg. Reg. loss: 0.0238 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 26400 Avg. Training loss: 4.5486 Avg. Reg. loss: 0.0238 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 26500 Avg. Training loss: 4.5623 Avg. Reg. loss: 0.0240 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 26600 Avg. Training loss: 4.5528 Avg. Reg. loss: 0.0240 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 26700 Avg. Training loss: 4.5484 Avg. Reg. loss: 0.0240 0.0547 sec/batch\n",
      "Epoch 1/10 Iteration: 26800 Avg. Training loss: 4.5435 Avg. Reg. loss: 0.0240 0.0541 sec/batch\n",
      "Epoch 1/10 Iteration: 26900 Avg. Training loss: 4.5449 Avg. Reg. loss: 0.0239 0.0537 sec/batch\n",
      "Epoch 1/10 Iteration: 27000 Avg. Training loss: 4.5450 Avg. Reg. loss: 0.0233 0.0522 sec/batch\n",
      "Epoch 1/10 Iteration: 27100 Avg. Training loss: 4.5145 Avg. Reg. loss: 0.0232 0.0560 sec/batch\n",
      "Epoch 1/10 Iteration: 27200 Avg. Training loss: 4.5532 Avg. Reg. loss: 0.0233 0.0537 sec/batch\n",
      "Epoch 1/10 Iteration: 27300 Avg. Training loss: 4.5445 Avg. Reg. loss: 0.0231 0.0566 sec/batch\n",
      "Epoch 1/10 Iteration: 27400 Avg. Training loss: 4.5291 Avg. Reg. loss: 0.0244 0.0566 sec/batch\n",
      "Epoch 1/10 Iteration: 27500 Avg. Training loss: 4.5377 Avg. Reg. loss: 0.0248 0.0542 sec/batch\n",
      "Epoch 1/10 Iteration: 27600 Avg. Training loss: 4.5435 Avg. Reg. loss: 0.0239 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 27700 Avg. Training loss: 4.5475 Avg. Reg. loss: 0.0242 0.0506 sec/batch\n",
      "Epoch 1/10 Iteration: 27800 Avg. Training loss: 4.5293 Avg. Reg. loss: 0.0235 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 27900 Avg. Training loss: 4.5406 Avg. Reg. loss: 0.0238 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 28000 Avg. Training loss: 4.5373 Avg. Reg. loss: 0.0233 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 28100 Avg. Training loss: 4.5146 Avg. Reg. loss: 0.0237 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 28200 Avg. Training loss: 4.5263 Avg. Reg. loss: 0.0232 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 28300 Avg. Training loss: 4.5300 Avg. Reg. loss: 0.0236 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 28400 Avg. Training loss: 4.5114 Avg. Reg. loss: 0.0244 0.0506 sec/batch\n",
      "Epoch 1/10 Iteration: 28500 Avg. Training loss: 4.5319 Avg. Reg. loss: 0.0235 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 28600 Avg. Training loss: 4.5384 Avg. Reg. loss: 0.0241 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 28700 Avg. Training loss: 4.5256 Avg. Reg. loss: 0.0237 0.0526 sec/batch\n",
      "Epoch 1/10 Iteration: 28800 Avg. Training loss: 4.5279 Avg. Reg. loss: 0.0232 0.0514 sec/batch\n",
      "Epoch 1/10 Iteration: 28900 Avg. Training loss: 4.5282 Avg. Reg. loss: 0.0239 0.0515 sec/batch\n",
      "Epoch 1/10 Iteration: 29000 Avg. Training loss: 4.5406 Avg. Reg. loss: 0.0246 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 29100 Avg. Training loss: 4.5375 Avg. Reg. loss: 0.0244 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 29200 Avg. Training loss: 4.5139 Avg. Reg. loss: 0.0238 0.0493 sec/batch\n",
      "Epoch 1/10 Iteration: 29300 Avg. Training loss: 4.5263 Avg. Reg. loss: 0.0239 0.0510 sec/batch\n",
      "Epoch 1/10 Iteration: 29400 Avg. Training loss: 4.5312 Avg. Reg. loss: 0.0239 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 29500 Avg. Training loss: 4.5304 Avg. Reg. loss: 0.0239 0.0528 sec/batch\n",
      "Epoch 1/10 Iteration: 29600 Avg. Training loss: 4.5133 Avg. Reg. loss: 0.0238 0.0531 sec/batch\n",
      "Epoch 1/10 Iteration: 29700 Avg. Training loss: 4.5250 Avg. Reg. loss: 0.0243 0.0510 sec/batch\n",
      "Epoch 1/10 Iteration: 29800 Avg. Training loss: 4.5237 Avg. Reg. loss: 0.0242 0.0539 sec/batch\n",
      "Epoch 1/10 Iteration: 29900 Avg. Training loss: 4.5554 Avg. Reg. loss: 0.0247 0.0520 sec/batch\n",
      "Epoch 1/10 Iteration: 30000 Avg. Training loss: 4.5215 Avg. Reg. loss: 0.0229 0.0529 sec/batch\n",
      "Epoch 1/10 Iteration: 30100 Avg. Training loss: 4.5218 Avg. Reg. loss: 0.0233 0.0552 sec/batch\n",
      "Epoch 1/10 Iteration: 30200 Avg. Training loss: 4.5147 Avg. Reg. loss: 0.0239 0.0547 sec/batch\n",
      "Epoch 1/10 Iteration: 30300 Avg. Training loss: 4.4989 Avg. Reg. loss: 0.0239 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 30400 Avg. Training loss: 4.5192 Avg. Reg. loss: 0.0237 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 30500 Avg. Training loss: 4.4999 Avg. Reg. loss: 0.0237 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 30600 Avg. Training loss: 4.5200 Avg. Reg. loss: 0.0241 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 30700 Avg. Training loss: 4.5155 Avg. Reg. loss: 0.0232 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 30800 Avg. Training loss: 4.5189 Avg. Reg. loss: 0.0241 0.0459 sec/batch\n",
      "Epoch 1/10 Iteration: 30900 Avg. Training loss: 4.5208 Avg. Reg. loss: 0.0243 0.0436 sec/batch\n",
      "Epoch 1/10 Iteration: 31000 Avg. Training loss: 4.5150 Avg. Reg. loss: 0.0240 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 31100 Avg. Training loss: 4.5107 Avg. Reg. loss: 0.0241 0.0479 sec/batch\n",
      "Epoch 1/10 Iteration: 31200 Avg. Training loss: 4.5202 Avg. Reg. loss: 0.0243 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 31300 Avg. Training loss: 4.5074 Avg. Reg. loss: 0.0231 0.0480 sec/batch\n",
      "Epoch 1/10 Iteration: 31400 Avg. Training loss: 4.5233 Avg. Reg. loss: 0.0239 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 31500 Avg. Training loss: 4.5053 Avg. Reg. loss: 0.0237 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 31600 Avg. Training loss: 4.5000 Avg. Reg. loss: 0.0238 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 31700 Avg. Training loss: 4.4994 Avg. Reg. loss: 0.0236 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 31800 Avg. Training loss: 4.5038 Avg. Reg. loss: 0.0238 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 31900 Avg. Training loss: 4.5062 Avg. Reg. loss: 0.0238 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 32000 Avg. Training loss: 4.5082 Avg. Reg. loss: 0.0233 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 32100 Avg. Training loss: 4.4860 Avg. Reg. loss: 0.0239 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 32200 Avg. Training loss: 4.5029 Avg. Reg. loss: 0.0236 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 32300 Avg. Training loss: 4.4994 Avg. Reg. loss: 0.0243 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 32400 Avg. Training loss: 4.5545 Avg. Reg. loss: 0.0245 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 32500 Avg. Training loss: 4.5086 Avg. Reg. loss: 0.0230 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 32600 Avg. Training loss: 4.4906 Avg. Reg. loss: 0.0237 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 32700 Avg. Training loss: 4.4997 Avg. Reg. loss: 0.0236 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 32800 Avg. Training loss: 4.5050 Avg. Reg. loss: 0.0241 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 32900 Avg. Training loss: 4.4967 Avg. Reg. loss: 0.0238 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 33000 Avg. Training loss: 4.4823 Avg. Reg. loss: 0.0241 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 33100 Avg. Training loss: 4.5104 Avg. Reg. loss: 0.0231 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 33200 Avg. Training loss: 4.4935 Avg. Reg. loss: 0.0242 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 33300 Avg. Training loss: 4.4901 Avg. Reg. loss: 0.0241 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 33400 Avg. Training loss: 4.4898 Avg. Reg. loss: 0.0241 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 33500 Avg. Training loss: 4.4999 Avg. Reg. loss: 0.0233 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 33600 Avg. Training loss: 4.4957 Avg. Reg. loss: 0.0235 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 33700 Avg. Training loss: 4.4920 Avg. Reg. loss: 0.0235 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 33800 Avg. Training loss: 4.4835 Avg. Reg. loss: 0.0235 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 33900 Avg. Training loss: 4.4765 Avg. Reg. loss: 0.0230 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 34000 Avg. Training loss: 4.4980 Avg. Reg. loss: 0.0240 0.0481 sec/batch\n",
      "Epoch 1/10 Iteration: 34100 Avg. Training loss: 4.4983 Avg. Reg. loss: 0.0245 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 34200 Avg. Training loss: 4.4781 Avg. Reg. loss: 0.0247 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 34300 Avg. Training loss: 4.4926 Avg. Reg. loss: 0.0231 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 34400 Avg. Training loss: 4.5258 Avg. Reg. loss: 0.0248 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 34500 Avg. Training loss: 4.4861 Avg. Reg. loss: 0.0242 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 34600 Avg. Training loss: 4.5042 Avg. Reg. loss: 0.0232 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 34700 Avg. Training loss: 4.4957 Avg. Reg. loss: 0.0236 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 34800 Avg. Training loss: 4.4840 Avg. Reg. loss: 0.0241 0.0487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 34900 Avg. Training loss: 4.4879 Avg. Reg. loss: 0.0242 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 35000 Avg. Training loss: 4.4905 Avg. Reg. loss: 0.0236 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 35100 Avg. Training loss: 4.4995 Avg. Reg. loss: 0.0237 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 35200 Avg. Training loss: 4.4875 Avg. Reg. loss: 0.0236 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 35300 Avg. Training loss: 4.4903 Avg. Reg. loss: 0.0234 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 35400 Avg. Training loss: 4.4772 Avg. Reg. loss: 0.0235 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 35500 Avg. Training loss: 4.4844 Avg. Reg. loss: 0.0236 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 35600 Avg. Training loss: 4.5023 Avg. Reg. loss: 0.0239 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 35700 Avg. Training loss: 4.4776 Avg. Reg. loss: 0.0236 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 35800 Avg. Training loss: 4.4840 Avg. Reg. loss: 0.0231 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 35900 Avg. Training loss: 4.4703 Avg. Reg. loss: 0.0242 0.0483 sec/batch\n",
      "Epoch 1/10 Iteration: 36000 Avg. Training loss: 4.4711 Avg. Reg. loss: 0.0232 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 36100 Avg. Training loss: 4.4749 Avg. Reg. loss: 0.0237 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 36200 Avg. Training loss: 4.4583 Avg. Reg. loss: 0.0240 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 36300 Avg. Training loss: 4.4643 Avg. Reg. loss: 0.0240 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 36400 Avg. Training loss: 4.4765 Avg. Reg. loss: 0.0238 0.0487 sec/batch\n",
      "Epoch 1/10 Iteration: 36500 Avg. Training loss: 4.4823 Avg. Reg. loss: 0.0240 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 36600 Avg. Training loss: 4.4645 Avg. Reg. loss: 0.0232 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 36700 Avg. Training loss: 4.4781 Avg. Reg. loss: 0.0237 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 36800 Avg. Training loss: 4.4784 Avg. Reg. loss: 0.0238 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 36900 Avg. Training loss: 4.4620 Avg. Reg. loss: 0.0242 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 37000 Avg. Training loss: 4.4782 Avg. Reg. loss: 0.0234 0.0484 sec/batch\n",
      "Epoch 1/10 Iteration: 37100 Avg. Training loss: 4.4667 Avg. Reg. loss: 0.0241 0.0485 sec/batch\n",
      "Epoch 1/10 Iteration: 37200 Avg. Training loss: 4.4708 Avg. Reg. loss: 0.0232 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 37300 Avg. Training loss: 4.4816 Avg. Reg. loss: 0.0249 0.0428 sec/batch\n",
      "Epoch 1/10 Iteration: 37400 Avg. Training loss: 4.4855 Avg. Reg. loss: 0.0242 0.0418 sec/batch\n",
      "Epoch 1/10 Iteration: 37500 Avg. Training loss: 4.4653 Avg. Reg. loss: 0.0241 0.0417 sec/batch\n",
      "Epoch 1/10 Iteration: 37600 Avg. Training loss: 4.4827 Avg. Reg. loss: 0.0232 0.0472 sec/batch\n",
      "Epoch 1/10 Iteration: 37700 Avg. Training loss: 4.4847 Avg. Reg. loss: 0.0238 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 37800 Avg. Training loss: 4.4605 Avg. Reg. loss: 0.0243 0.0486 sec/batch\n",
      "Epoch 1/10 Iteration: 37900 Avg. Training loss: 4.4896 Avg. Reg. loss: 0.0241 0.0488 sec/batch\n",
      "Epoch 1/10 Iteration: 38000 Avg. Training loss: 4.4804 Avg. Reg. loss: 0.0245 0.0496 sec/batch\n",
      "Epoch 1/10 Iteration: 38100 Avg. Training loss: 4.4665 Avg. Reg. loss: 0.0239 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 38200 Avg. Training loss: 4.4782 Avg. Reg. loss: 0.0243 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 38300 Avg. Training loss: 4.4614 Avg. Reg. loss: 0.0234 0.0491 sec/batch\n",
      "Epoch 1/10 Iteration: 38400 Avg. Training loss: 4.4740 Avg. Reg. loss: 0.0235 0.0490 sec/batch\n",
      "Epoch 1/10 Iteration: 38500 Avg. Training loss: 4.4525 Avg. Reg. loss: 0.0241 0.0489 sec/batch\n",
      "Epoch 1/10 Iteration: 38600 Avg. Training loss: 4.4730 Avg. Reg. loss: 0.0244 0.0494 sec/batch\n",
      "Epoch 1/10 Iteration: 38700 Avg. Training loss: 4.4768 Avg. Reg. loss: 0.0239 0.0535 sec/batch\n",
      "Epoch 1/10 Iteration: 38800 Avg. Training loss: 4.5288 Avg. Reg. loss: 0.0261 0.0536 sec/batch\n",
      "Epoch 1/10 Iteration: 38900 Avg. Training loss: 4.4710 Avg. Reg. loss: 0.0241 0.0534 sec/batch\n",
      "Epoch 1/10 Iteration: 39000 Avg. Training loss: 4.4744 Avg. Reg. loss: 0.0244 0.0543 sec/batch\n",
      "Epoch 1/10 Iteration: 39100 Avg. Training loss: 4.4639 Avg. Reg. loss: 0.0235 0.0541 sec/batch\n",
      "Epoch 1/10 Iteration: 39200 Avg. Training loss: 4.4653 Avg. Reg. loss: 0.0243 0.0524 sec/batch\n",
      "Epoch 1/10 Iteration: 39300 Avg. Training loss: 4.4565 Avg. Reg. loss: 0.0235 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 39400 Avg. Training loss: 4.4615 Avg. Reg. loss: 0.0240 0.0520 sec/batch\n",
      "Epoch 1/10 Iteration: 39500 Avg. Training loss: 4.4587 Avg. Reg. loss: 0.0237 0.0523 sec/batch\n",
      "Epoch 1/10 Iteration: 39600 Avg. Training loss: 4.4744 Avg. Reg. loss: 0.0244 0.0519 sec/batch\n",
      "Epoch 1/10 Iteration: 39700 Avg. Training loss: 4.4716 Avg. Reg. loss: 0.0240 0.0518 sec/batch\n",
      "Epoch 1/10 Iteration: 39800 Avg. Training loss: 4.4877 Avg. Reg. loss: 0.0245 0.0515 sec/batch\n",
      "Epoch 1/10 Iteration: 39900 Avg. Training loss: 4.4498 Avg. Reg. loss: 0.0241 0.0525 sec/batch\n",
      "Epoch 1/10 Iteration: 40000 Avg. Training loss: 4.4702 Avg. Reg. loss: 0.0243 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 40100 Avg. Training loss: 4.4577 Avg. Reg. loss: 0.0229 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 40200 Avg. Training loss: 4.4695 Avg. Reg. loss: 0.0243 0.0522 sec/batch\n",
      "Epoch 1/10 Iteration: 40300 Avg. Training loss: 4.4549 Avg. Reg. loss: 0.0242 0.0536 sec/batch\n",
      "Epoch 1/10 Iteration: 40400 Avg. Training loss: 4.4675 Avg. Reg. loss: 0.0237 0.0532 sec/batch\n",
      "Epoch 1/10 Iteration: 40500 Avg. Training loss: 4.4560 Avg. Reg. loss: 0.0235 0.0523 sec/batch\n",
      "Epoch 1/10 Iteration: 40600 Avg. Training loss: 4.4565 Avg. Reg. loss: 0.0238 0.0517 sec/batch\n",
      "Epoch 1/10 Iteration: 40700 Avg. Training loss: 4.4592 Avg. Reg. loss: 0.0242 0.0527 sec/batch\n",
      "Epoch 1/10 Iteration: 40800 Avg. Training loss: 4.4652 Avg. Reg. loss: 0.0234 0.0526 sec/batch\n",
      "Epoch 1/10 Iteration: 40900 Avg. Training loss: 4.4459 Avg. Reg. loss: 0.0228 0.0515 sec/batch\n",
      "Epoch 1/10 Iteration: 41000 Avg. Training loss: 4.4744 Avg. Reg. loss: 0.0238 0.0515 sec/batch\n",
      "Epoch 1/10 Iteration: 41100 Avg. Training loss: 4.4386 Avg. Reg. loss: 0.0243 0.0520 sec/batch\n",
      "Epoch 1/10 Iteration: 41200 Avg. Training loss: 4.4527 Avg. Reg. loss: 0.0240 0.0522 sec/batch\n",
      "Epoch 1/10 Iteration: 41300 Avg. Training loss: 4.4578 Avg. Reg. loss: 0.0244 0.0508 sec/batch\n",
      "Epoch 1/10 Iteration: 41400 Avg. Training loss: 4.4669 Avg. Reg. loss: 0.0240 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 41500 Avg. Training loss: 4.4518 Avg. Reg. loss: 0.0238 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 41600 Avg. Training loss: 4.4368 Avg. Reg. loss: 0.0241 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 41700 Avg. Training loss: 4.4590 Avg. Reg. loss: 0.0237 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 41800 Avg. Training loss: 4.4895 Avg. Reg. loss: 0.0248 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 41900 Avg. Training loss: 4.4721 Avg. Reg. loss: 0.0236 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 42000 Avg. Training loss: 4.4583 Avg. Reg. loss: 0.0241 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 42100 Avg. Training loss: 4.4372 Avg. Reg. loss: 0.0245 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 42200 Avg. Training loss: 4.4501 Avg. Reg. loss: 0.0241 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 42300 Avg. Training loss: 4.4477 Avg. Reg. loss: 0.0239 0.0482 sec/batch\n",
      "Epoch 1/10 Iteration: 42400 Avg. Training loss: 4.4460 Avg. Reg. loss: 0.0249 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 42500 Avg. Training loss: 4.4432 Avg. Reg. loss: 0.0241 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 42600 Avg. Training loss: 4.4524 Avg. Reg. loss: 0.0241 0.0499 sec/batch\n",
      "Epoch 1/10 Iteration: 42700 Avg. Training loss: 4.4683 Avg. Reg. loss: 0.0236 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 42800 Avg. Training loss: 4.4455 Avg. Reg. loss: 0.0241 0.0507 sec/batch\n",
      "Epoch 1/10 Iteration: 42900 Avg. Training loss: 4.4622 Avg. Reg. loss: 0.0237 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 43000 Avg. Training loss: 4.4422 Avg. Reg. loss: 0.0239 0.0497 sec/batch\n",
      "Epoch 1/10 Iteration: 43100 Avg. Training loss: 4.4446 Avg. Reg. loss: 0.0242 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 43200 Avg. Training loss: 4.4595 Avg. Reg. loss: 0.0242 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 43300 Avg. Training loss: 4.4440 Avg. Reg. loss: 0.0238 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 43400 Avg. Training loss: 4.4514 Avg. Reg. loss: 0.0239 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 43500 Avg. Training loss: 4.4420 Avg. Reg. loss: 0.0235 0.0501 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 43600 Avg. Training loss: 4.4672 Avg. Reg. loss: 0.0240 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 43700 Avg. Training loss: 4.4536 Avg. Reg. loss: 0.0241 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 43800 Avg. Training loss: 4.4609 Avg. Reg. loss: 0.0244 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 43900 Avg. Training loss: 4.4454 Avg. Reg. loss: 0.0241 0.0500 sec/batch\n",
      "Epoch 1/10 Iteration: 44000 Avg. Training loss: 4.4386 Avg. Reg. loss: 0.0242 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 44100 Avg. Training loss: 4.4434 Avg. Reg. loss: 0.0241 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 44200 Avg. Training loss: 4.4431 Avg. Reg. loss: 0.0242 0.0501 sec/batch\n",
      "Epoch 1/10 Iteration: 44300 Avg. Training loss: 4.4455 Avg. Reg. loss: 0.0243 0.0505 sec/batch\n",
      "Epoch 1/10 Iteration: 44400 Avg. Training loss: 4.5171 Avg. Reg. loss: 0.0253 0.0504 sec/batch\n",
      "Epoch 1/10 Iteration: 44500 Avg. Training loss: 4.4425 Avg. Reg. loss: 0.0240 0.0498 sec/batch\n",
      "Epoch 1/10 Iteration: 44600 Avg. Training loss: 4.4532 Avg. Reg. loss: 0.0236 0.0502 sec/batch\n",
      "Epoch 1/10 Iteration: 44700 Avg. Training loss: 4.4726 Avg. Reg. loss: 0.0246 0.0506 sec/batch\n",
      "Epoch 1/10 Iteration: 44800 Avg. Training loss: 4.4371 Avg. Reg. loss: 0.0240 0.0503 sec/batch\n",
      "Epoch 1/10 Iteration: 44900 Avg. Training loss: 4.4961 Avg. Reg. loss: 0.0235 0.0527 sec/batch\n",
      "Epoch 1/10 Iteration: 45000 Avg. Training loss: 4.4557 Avg. Reg. loss: 0.0245 0.0578 sec/batch\n",
      "Epoch 1/10 Iteration: 45100 Avg. Training loss: 4.4440 Avg. Reg. loss: 0.0246 0.0539 sec/batch\n",
      "Epoch 1/10 Iteration: 45200 Avg. Training loss: 4.4358 Avg. Reg. loss: 0.0245 0.0562 sec/batch\n",
      "Epoch 1/10 Iteration: 45300 Avg. Training loss: 4.4534 Avg. Reg. loss: 0.0241 0.0563 sec/batch\n",
      "Epoch 1/10 Iteration: 45400 Avg. Training loss: 4.4435 Avg. Reg. loss: 0.0239 0.0538 sec/batch\n",
      "Epoch 1/10 Iteration: 45500 Avg. Training loss: 4.4454 Avg. Reg. loss: 0.0235 0.0553 sec/batch\n",
      "Epoch 1/10 Iteration: 45600 Avg. Training loss: 4.4473 Avg. Reg. loss: 0.0244 0.0533 sec/batch\n",
      "Epoch 1/10 Iteration: 45700 Avg. Training loss: 4.4363 Avg. Reg. loss: 0.0238 0.0524 sec/batch\n",
      "Epoch 1/10 Iteration: 45800 Avg. Training loss: 4.4365 Avg. Reg. loss: 0.0238 0.0526 sec/batch\n",
      "Epoch 1/10 Iteration: 45900 Avg. Training loss: 4.5004 Avg. Reg. loss: 0.0251 0.0534 sec/batch\n",
      "Epoch 1/10 Iteration: 46000 Avg. Training loss: 4.4594 Avg. Reg. loss: 0.0248 0.0526 sec/batch\n",
      "Epoch 2/10 Iteration: 46100 Avg. Training loss: 4.4353 Avg. Reg. loss: 0.0238 0.0537 sec/batch\n",
      "Epoch 2/10 Iteration: 46200 Avg. Training loss: 4.4387 Avg. Reg. loss: 0.0242 0.0527 sec/batch\n",
      "Epoch 2/10 Iteration: 46300 Avg. Training loss: 4.4412 Avg. Reg. loss: 0.0237 0.0525 sec/batch\n",
      "Epoch 2/10 Iteration: 46400 Avg. Training loss: 4.4406 Avg. Reg. loss: 0.0245 0.0530 sec/batch\n",
      "Epoch 2/10 Iteration: 46500 Avg. Training loss: 4.4340 Avg. Reg. loss: 0.0240 0.0529 sec/batch\n",
      "Epoch 2/10 Iteration: 46600 Avg. Training loss: 4.4541 Avg. Reg. loss: 0.0241 0.0536 sec/batch\n",
      "Epoch 2/10 Iteration: 46700 Avg. Training loss: 4.4368 Avg. Reg. loss: 0.0235 0.0520 sec/batch\n",
      "Epoch 2/10 Iteration: 46800 Avg. Training loss: 4.4817 Avg. Reg. loss: 0.0241 0.0525 sec/batch\n",
      "Epoch 2/10 Iteration: 46900 Avg. Training loss: 4.4475 Avg. Reg. loss: 0.0251 0.0517 sec/batch\n",
      "Epoch 2/10 Iteration: 47000 Avg. Training loss: 4.4444 Avg. Reg. loss: 0.0232 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 47100 Avg. Training loss: 4.4233 Avg. Reg. loss: 0.0232 0.0515 sec/batch\n",
      "Epoch 2/10 Iteration: 47200 Avg. Training loss: 4.4228 Avg. Reg. loss: 0.0246 0.0519 sec/batch\n",
      "Epoch 2/10 Iteration: 47300 Avg. Training loss: 4.4440 Avg. Reg. loss: 0.0240 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 47400 Avg. Training loss: 4.4500 Avg. Reg. loss: 0.0247 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 47500 Avg. Training loss: 4.4244 Avg. Reg. loss: 0.0234 0.0521 sec/batch\n",
      "Epoch 2/10 Iteration: 47600 Avg. Training loss: 4.4240 Avg. Reg. loss: 0.0243 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 47700 Avg. Training loss: 4.4205 Avg. Reg. loss: 0.0243 0.0519 sec/batch\n",
      "Epoch 2/10 Iteration: 47800 Avg. Training loss: 4.4595 Avg. Reg. loss: 0.0242 0.0514 sec/batch\n",
      "Epoch 2/10 Iteration: 47900 Avg. Training loss: 4.4970 Avg. Reg. loss: 0.0249 0.0522 sec/batch\n",
      "Epoch 2/10 Iteration: 48000 Avg. Training loss: 4.4422 Avg. Reg. loss: 0.0247 0.0524 sec/batch\n",
      "Epoch 2/10 Iteration: 48100 Avg. Training loss: 4.4331 Avg. Reg. loss: 0.0239 0.0519 sec/batch\n",
      "Epoch 2/10 Iteration: 48200 Avg. Training loss: 4.4312 Avg. Reg. loss: 0.0252 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 48300 Avg. Training loss: 4.4227 Avg. Reg. loss: 0.0241 0.0515 sec/batch\n",
      "Epoch 2/10 Iteration: 48400 Avg. Training loss: 4.4314 Avg. Reg. loss: 0.0245 0.0517 sec/batch\n",
      "Epoch 2/10 Iteration: 48500 Avg. Training loss: 4.4195 Avg. Reg. loss: 0.0240 0.0515 sec/batch\n",
      "Epoch 2/10 Iteration: 48600 Avg. Training loss: 4.4750 Avg. Reg. loss: 0.0244 0.0515 sec/batch\n",
      "Epoch 2/10 Iteration: 48700 Avg. Training loss: 4.4440 Avg. Reg. loss: 0.0247 0.0520 sec/batch\n",
      "Epoch 2/10 Iteration: 48800 Avg. Training loss: 4.4563 Avg. Reg. loss: 0.0244 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 48900 Avg. Training loss: 4.4223 Avg. Reg. loss: 0.0240 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 49000 Avg. Training loss: 4.4330 Avg. Reg. loss: 0.0244 0.0523 sec/batch\n",
      "Epoch 2/10 Iteration: 49100 Avg. Training loss: 4.4223 Avg. Reg. loss: 0.0238 0.0528 sec/batch\n",
      "Epoch 2/10 Iteration: 49200 Avg. Training loss: 4.4408 Avg. Reg. loss: 0.0243 0.0517 sec/batch\n",
      "Epoch 2/10 Iteration: 49300 Avg. Training loss: 4.4362 Avg. Reg. loss: 0.0245 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 49400 Avg. Training loss: 4.4235 Avg. Reg. loss: 0.0238 0.0521 sec/batch\n",
      "Epoch 2/10 Iteration: 49500 Avg. Training loss: 4.4329 Avg. Reg. loss: 0.0239 0.0512 sec/batch\n",
      "Epoch 2/10 Iteration: 49600 Avg. Training loss: 4.4387 Avg. Reg. loss: 0.0252 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 49700 Avg. Training loss: 4.4291 Avg. Reg. loss: 0.0239 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 49800 Avg. Training loss: 4.4116 Avg. Reg. loss: 0.0239 0.0519 sec/batch\n",
      "Epoch 2/10 Iteration: 49900 Avg. Training loss: 4.4313 Avg. Reg. loss: 0.0239 0.0525 sec/batch\n",
      "Epoch 2/10 Iteration: 50000 Avg. Training loss: 4.4224 Avg. Reg. loss: 0.0245 0.0511 sec/batch\n",
      "Epoch 2/10 Iteration: 50100 Avg. Training loss: 4.4182 Avg. Reg. loss: 0.0239 0.0473 sec/batch\n",
      "Epoch 2/10 Iteration: 50200 Avg. Training loss: 4.4074 Avg. Reg. loss: 0.0244 0.0456 sec/batch\n",
      "Epoch 2/10 Iteration: 50300 Avg. Training loss: 4.4764 Avg. Reg. loss: 0.0247 0.0529 sec/batch\n",
      "Epoch 2/10 Iteration: 50400 Avg. Training loss: 4.4140 Avg. Reg. loss: 0.0235 0.0517 sec/batch\n",
      "Epoch 2/10 Iteration: 50500 Avg. Training loss: 4.4273 Avg. Reg. loss: 0.0242 0.0521 sec/batch\n",
      "Epoch 2/10 Iteration: 50600 Avg. Training loss: 4.4321 Avg. Reg. loss: 0.0247 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 50700 Avg. Training loss: 4.4239 Avg. Reg. loss: 0.0236 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 50800 Avg. Training loss: 4.4259 Avg. Reg. loss: 0.0242 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 50900 Avg. Training loss: 4.4108 Avg. Reg. loss: 0.0243 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 51000 Avg. Training loss: 4.4159 Avg. Reg. loss: 0.0242 0.0506 sec/batch\n",
      "Epoch 2/10 Iteration: 51100 Avg. Training loss: 4.4041 Avg. Reg. loss: 0.0238 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 51200 Avg. Training loss: 4.4197 Avg. Reg. loss: 0.0244 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 51300 Avg. Training loss: 4.4238 Avg. Reg. loss: 0.0246 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 51400 Avg. Training loss: 4.4283 Avg. Reg. loss: 0.0244 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 51500 Avg. Training loss: 4.4174 Avg. Reg. loss: 0.0241 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 51600 Avg. Training loss: 4.4026 Avg. Reg. loss: 0.0244 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 51700 Avg. Training loss: 4.4335 Avg. Reg. loss: 0.0241 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 51800 Avg. Training loss: 4.4234 Avg. Reg. loss: 0.0245 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 51900 Avg. Training loss: 4.4389 Avg. Reg. loss: 0.0243 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 52000 Avg. Training loss: 4.4148 Avg. Reg. loss: 0.0249 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 52100 Avg. Training loss: 4.4103 Avg. Reg. loss: 0.0240 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 52200 Avg. Training loss: 4.4300 Avg. Reg. loss: 0.0244 0.0502 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Iteration: 52300 Avg. Training loss: 4.4071 Avg. Reg. loss: 0.0237 0.0506 sec/batch\n",
      "Epoch 2/10 Iteration: 52400 Avg. Training loss: 4.4170 Avg. Reg. loss: 0.0247 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 52500 Avg. Training loss: 4.4137 Avg. Reg. loss: 0.0246 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 52600 Avg. Training loss: 4.4229 Avg. Reg. loss: 0.0236 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 52700 Avg. Training loss: 4.4126 Avg. Reg. loss: 0.0246 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 52800 Avg. Training loss: 4.4218 Avg. Reg. loss: 0.0248 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 52900 Avg. Training loss: 4.4585 Avg. Reg. loss: 0.0247 0.0508 sec/batch\n",
      "Epoch 2/10 Iteration: 53000 Avg. Training loss: 4.4227 Avg. Reg. loss: 0.0239 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 53100 Avg. Training loss: 4.4015 Avg. Reg. loss: 0.0247 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 53200 Avg. Training loss: 4.4259 Avg. Reg. loss: 0.0241 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 53300 Avg. Training loss: 4.4371 Avg. Reg. loss: 0.0245 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 53400 Avg. Training loss: 4.4264 Avg. Reg. loss: 0.0242 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 53500 Avg. Training loss: 4.4338 Avg. Reg. loss: 0.0246 0.0508 sec/batch\n",
      "Epoch 2/10 Iteration: 53600 Avg. Training loss: 4.4084 Avg. Reg. loss: 0.0249 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 53700 Avg. Training loss: 4.4108 Avg. Reg. loss: 0.0240 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 53800 Avg. Training loss: 4.4234 Avg. Reg. loss: 0.0248 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 53900 Avg. Training loss: 4.4183 Avg. Reg. loss: 0.0243 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 54000 Avg. Training loss: 4.4095 Avg. Reg. loss: 0.0239 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 54100 Avg. Training loss: 4.4110 Avg. Reg. loss: 0.0245 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 54200 Avg. Training loss: 4.4090 Avg. Reg. loss: 0.0239 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 54300 Avg. Training loss: 4.4055 Avg. Reg. loss: 0.0238 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 54400 Avg. Training loss: 4.4148 Avg. Reg. loss: 0.0240 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 54500 Avg. Training loss: 4.4117 Avg. Reg. loss: 0.0248 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 54600 Avg. Training loss: 4.4085 Avg. Reg. loss: 0.0236 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 54700 Avg. Training loss: 4.4179 Avg. Reg. loss: 0.0245 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 54800 Avg. Training loss: 4.4270 Avg. Reg. loss: 0.0247 0.0506 sec/batch\n",
      "Epoch 2/10 Iteration: 54900 Avg. Training loss: 4.4277 Avg. Reg. loss: 0.0241 0.0508 sec/batch\n",
      "Epoch 2/10 Iteration: 55000 Avg. Training loss: 4.4244 Avg. Reg. loss: 0.0248 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 55100 Avg. Training loss: 4.4090 Avg. Reg. loss: 0.0249 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 55200 Avg. Training loss: 4.4098 Avg. Reg. loss: 0.0239 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 55300 Avg. Training loss: 4.4243 Avg. Reg. loss: 0.0246 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 55400 Avg. Training loss: 4.4065 Avg. Reg. loss: 0.0247 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 55500 Avg. Training loss: 4.4025 Avg. Reg. loss: 0.0236 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 55600 Avg. Training loss: 4.3881 Avg. Reg. loss: 0.0239 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 55700 Avg. Training loss: 4.4242 Avg. Reg. loss: 0.0243 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 55800 Avg. Training loss: 4.4149 Avg. Reg. loss: 0.0250 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 55900 Avg. Training loss: 4.4193 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 56000 Avg. Training loss: 4.4180 Avg. Reg. loss: 0.0242 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 56100 Avg. Training loss: 4.4124 Avg. Reg. loss: 0.0251 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 56200 Avg. Training loss: 4.4207 Avg. Reg. loss: 0.0247 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 56300 Avg. Training loss: 4.4133 Avg. Reg. loss: 0.0254 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 56400 Avg. Training loss: 4.3969 Avg. Reg. loss: 0.0241 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 56500 Avg. Training loss: 4.4067 Avg. Reg. loss: 0.0246 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 56600 Avg. Training loss: 4.4151 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 56700 Avg. Training loss: 4.4069 Avg. Reg. loss: 0.0244 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 56800 Avg. Training loss: 4.4131 Avg. Reg. loss: 0.0238 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 56900 Avg. Training loss: 4.4227 Avg. Reg. loss: 0.0250 0.0466 sec/batch\n",
      "Epoch 2/10 Iteration: 57000 Avg. Training loss: 4.4109 Avg. Reg. loss: 0.0248 0.0429 sec/batch\n",
      "Epoch 2/10 Iteration: 57100 Avg. Training loss: 4.4026 Avg. Reg. loss: 0.0241 0.0438 sec/batch\n",
      "Epoch 2/10 Iteration: 57200 Avg. Training loss: 4.4082 Avg. Reg. loss: 0.0245 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 57300 Avg. Training loss: 4.4138 Avg. Reg. loss: 0.0248 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 57400 Avg. Training loss: 4.4072 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 57500 Avg. Training loss: 4.4134 Avg. Reg. loss: 0.0249 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 57600 Avg. Training loss: 4.3980 Avg. Reg. loss: 0.0245 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 57700 Avg. Training loss: 4.4039 Avg. Reg. loss: 0.0245 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 57800 Avg. Training loss: 4.4086 Avg. Reg. loss: 0.0245 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 57900 Avg. Training loss: 4.4485 Avg. Reg. loss: 0.0256 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 58000 Avg. Training loss: 4.4069 Avg. Reg. loss: 0.0241 0.0499 sec/batch\n",
      "Epoch 2/10 Iteration: 58100 Avg. Training loss: 4.4150 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 58200 Avg. Training loss: 4.4041 Avg. Reg. loss: 0.0238 0.0506 sec/batch\n",
      "Epoch 2/10 Iteration: 58300 Avg. Training loss: 4.3900 Avg. Reg. loss: 0.0246 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 58400 Avg. Training loss: 4.4101 Avg. Reg. loss: 0.0245 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 58500 Avg. Training loss: 4.4071 Avg. Reg. loss: 0.0251 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 58600 Avg. Training loss: 4.4026 Avg. Reg. loss: 0.0241 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 58700 Avg. Training loss: 4.4111 Avg. Reg. loss: 0.0250 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 58800 Avg. Training loss: 4.4304 Avg. Reg. loss: 0.0252 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 58900 Avg. Training loss: 4.4126 Avg. Reg. loss: 0.0252 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 59000 Avg. Training loss: 4.3962 Avg. Reg. loss: 0.0250 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 59100 Avg. Training loss: 4.4180 Avg. Reg. loss: 0.0247 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 59200 Avg. Training loss: 4.3860 Avg. Reg. loss: 0.0242 0.0498 sec/batch\n",
      "Epoch 2/10 Iteration: 59300 Avg. Training loss: 4.4070 Avg. Reg. loss: 0.0250 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 59400 Avg. Training loss: 4.4117 Avg. Reg. loss: 0.0245 0.0500 sec/batch\n",
      "Epoch 2/10 Iteration: 59500 Avg. Training loss: 4.4002 Avg. Reg. loss: 0.0238 0.0502 sec/batch\n",
      "Epoch 2/10 Iteration: 59600 Avg. Training loss: 4.4059 Avg. Reg. loss: 0.0249 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 59700 Avg. Training loss: 4.4204 Avg. Reg. loss: 0.0250 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 59800 Avg. Training loss: 4.4172 Avg. Reg. loss: 0.0247 0.0501 sec/batch\n",
      "Epoch 2/10 Iteration: 59900 Avg. Training loss: 4.4157 Avg. Reg. loss: 0.0249 0.0519 sec/batch\n",
      "Epoch 2/10 Iteration: 60000 Avg. Training loss: 4.4021 Avg. Reg. loss: 0.0252 0.0511 sec/batch\n",
      "Epoch 2/10 Iteration: 60100 Avg. Training loss: 4.3871 Avg. Reg. loss: 0.0253 0.0521 sec/batch\n",
      "Epoch 2/10 Iteration: 60200 Avg. Training loss: 4.4260 Avg. Reg. loss: 0.0255 0.0545 sec/batch\n",
      "Epoch 2/10 Iteration: 60300 Avg. Training loss: 4.4181 Avg. Reg. loss: 0.0249 0.0532 sec/batch\n",
      "Epoch 2/10 Iteration: 60400 Avg. Training loss: 4.4046 Avg. Reg. loss: 0.0247 0.0522 sec/batch\n",
      "Epoch 2/10 Iteration: 60500 Avg. Training loss: 4.3991 Avg. Reg. loss: 0.0248 0.0524 sec/batch\n",
      "Epoch 2/10 Iteration: 60600 Avg. Training loss: 4.4226 Avg. Reg. loss: 0.0253 0.0525 sec/batch\n",
      "Epoch 2/10 Iteration: 60700 Avg. Training loss: 4.4082 Avg. Reg. loss: 0.0247 0.0510 sec/batch\n",
      "Epoch 2/10 Iteration: 60800 Avg. Training loss: 4.3976 Avg. Reg. loss: 0.0248 0.0525 sec/batch\n",
      "Epoch 2/10 Iteration: 60900 Avg. Training loss: 4.4012 Avg. Reg. loss: 0.0251 0.0530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Iteration: 61000 Avg. Training loss: 4.3874 Avg. Reg. loss: 0.0242 0.0508 sec/batch\n",
      "Epoch 2/10 Iteration: 61100 Avg. Training loss: 4.4087 Avg. Reg. loss: 0.0246 0.0505 sec/batch\n",
      "Epoch 2/10 Iteration: 61200 Avg. Training loss: 4.4012 Avg. Reg. loss: 0.0249 0.0518 sec/batch\n",
      "Epoch 2/10 Iteration: 61300 Avg. Training loss: 4.3859 Avg. Reg. loss: 0.0252 0.0512 sec/batch\n",
      "Epoch 2/10 Iteration: 61400 Avg. Training loss: 4.4059 Avg. Reg. loss: 0.0242 0.0517 sec/batch\n",
      "Epoch 2/10 Iteration: 61500 Avg. Training loss: 4.4097 Avg. Reg. loss: 0.0258 0.0508 sec/batch\n",
      "Epoch 2/10 Iteration: 61600 Avg. Training loss: 4.3866 Avg. Reg. loss: 0.0247 0.0515 sec/batch\n",
      "Epoch 2/10 Iteration: 61700 Avg. Training loss: 4.4108 Avg. Reg. loss: 0.0241 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 61800 Avg. Training loss: 4.4065 Avg. Reg. loss: 0.0255 0.0512 sec/batch\n",
      "Epoch 2/10 Iteration: 61900 Avg. Training loss: 4.4000 Avg. Reg. loss: 0.0254 0.0511 sec/batch\n",
      "Epoch 2/10 Iteration: 62000 Avg. Training loss: 4.4268 Avg. Reg. loss: 0.0249 0.0511 sec/batch\n",
      "Epoch 2/10 Iteration: 62100 Avg. Training loss: 4.3964 Avg. Reg. loss: 0.0250 0.0507 sec/batch\n",
      "Epoch 2/10 Iteration: 62200 Avg. Training loss: 4.3892 Avg. Reg. loss: 0.0251 0.0514 sec/batch\n",
      "Epoch 2/10 Iteration: 62300 Avg. Training loss: 4.3929 Avg. Reg. loss: 0.0244 0.0511 sec/batch\n",
      "Epoch 2/10 Iteration: 62400 Avg. Training loss: 4.4037 Avg. Reg. loss: 0.0257 0.0521 sec/batch\n",
      "Epoch 2/10 Iteration: 62500 Avg. Training loss: 4.4002 Avg. Reg. loss: 0.0254 0.0516 sec/batch\n",
      "Epoch 2/10 Iteration: 62600 Avg. Training loss: 4.4051 Avg. Reg. loss: 0.0246 0.0514 sec/batch\n",
      "Epoch 2/10 Iteration: 62700 Avg. Training loss: 4.3914 Avg. Reg. loss: 0.0246 0.0503 sec/batch\n",
      "Epoch 2/10 Iteration: 62800 Avg. Training loss: 4.3780 Avg. Reg. loss: 0.0250 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 62900 Avg. Training loss: 4.3968 Avg. Reg. loss: 0.0247 0.0504 sec/batch\n",
      "Epoch 2/10 Iteration: 63000 Avg. Training loss: 4.4102 Avg. Reg. loss: 0.0251 0.0522 sec/batch\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    regular_loss = 0\n",
    "    loss_best = 100\n",
    "    loss_list = []\n",
    "    iteration_best = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        batches = get_batches(idx_pairs_SG, batch_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            sess.run(update_embed_op)\n",
    "            train_loss, _ = sess.run([total_cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            regular_loss += sess.run(reg_loss)\n",
    "\n",
    "            if loss < loss_best:\n",
    "                W = sess.run(embedding).tolist()\n",
    "                iteration_best = iteration\n",
    "                loss_best = loss\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                end = time.time()\n",
    "                loss_list.append(loss / 100)\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss / 100),\n",
    "                      \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                      \"{:.4f} sec/batch\".format((end - start) / 100))\n",
    "\n",
    "\n",
    "                loss = 0\n",
    "                regular_loss = 0\n",
    "                start = time.time()\n",
    "            iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49601"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save embedding matrics\n",
    "np.save('w2v_pivots100_50d.npy',np.array(W))\n",
    "np.save('loss_pivots100_50d.npy',np.array(loss_list))\n",
    "\n",
    "print('best result at iteration:{0}'.format(iteration_best))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
