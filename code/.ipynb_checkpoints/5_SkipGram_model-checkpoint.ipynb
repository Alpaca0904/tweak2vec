{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size, stop_size):\n",
    "    idx_pairs = []\n",
    "    tokens = 0\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            tokens += 1\n",
    "            if tokens >= stop_size:\n",
    "                return idx_pairs, tokens\n",
    "            else:\n",
    "                # for each context word within window\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = sentence[context_word_pos]\n",
    "                    idx_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "                    \n",
    "    return idx_pairs, tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 26324522 word pairs\n",
      "totally 6975371 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "#corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/pubmed_corpus_int5.npy\").tolist()\n",
    "\n",
    "corpus_shuffle = corpus[:]\n",
    "\n",
    "random.shuffle(corpus_shuffle)\n",
    "quora_idx_pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size=7000000)\n",
    "print('totally {0} word pairs'.format(len(pubmed_idx_pairs)))\n",
    "print('totally {0} tokens'.format(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split Quora corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 46007576 word pairs\n",
      "totally 6975371 tokens\n",
      "totally 32977208 word pairs\n",
      "totally 5000000 tokens\n",
      "totally 19791565 word pairs\n",
      "totally 3000000 tokens\n",
      "totally 6606864 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 3298821 word pairs\n",
      "totally 500000 tokens\n",
      "totally 660021 word pairs\n",
      "totally 100000 tokens\n",
      "totally 330586 word pairs\n",
      "totally 50000 tokens\n",
      "totally 66163 word pairs\n",
      "totally 10000 tokens\n"
     ]
    }
   ],
   "source": [
    "quo_tokens_lst = [7,5,3,1,0.5,0.1,0.05,0.01]\n",
    "quo_idx_pairs = []\n",
    "for i in quo_tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    quo_idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split PubMed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 26324522 word pairs\n",
      "totally 3258438 tokens\n",
      "totally 24235016 word pairs\n",
      "totally 3000000 tokens\n",
      "totally 16156584 word pairs\n",
      "totally 2000000 tokens\n",
      "totally 8082335 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 4038440 word pairs\n",
      "totally 500000 tokens\n",
      "totally 806713 word pairs\n",
      "totally 100000 tokens\n",
      "totally 403567 word pairs\n",
      "totally 50000 tokens\n",
      "totally 80929 word pairs\n",
      "totally 10000 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [4,3,2,1,0.5,0.1,0.05,0.01]\n",
    "idx_pairs = []\n",
    "for i in tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "wordlist.append(['UNK',0])\n",
    "word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w[0] for w in wordlist }\n",
    "\n",
    "# wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_vocab5.npy').tolist()\n",
    "# wordlist.append('UNK')\n",
    "# word2idx = {w: wordlist.index(w) for w in wordlist }\n",
    "# idx2word = {wordlist.index(w): w for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 24965 pivot words\n"
     ]
    }
   ],
   "source": [
    "f = open('/Users/zhang/MscProject_tweak2vec/corpus/quora_pivots_google_full.txt','r')\n",
    "# f = open('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_pivots_google_full.txt','r')\n",
    "a = f.read()\n",
    "pivots_dict = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words'.format(len(pivots_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "def get_pivots_slice(pivots_dict, size):\n",
    "    pivots = deepcopy(pivots_dict)\n",
    "    pivots_slice = dict_slice(pivots, 0, size)\n",
    "    pivots_idx = []\n",
    "    pivots_vec = []\n",
    "    for i in pivots_slice.keys():\n",
    "        pivots_idx.append(i)\n",
    "        pivots_vec.append(pivots_slice[i])\n",
    "    return pivots_idx, pivots_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24965"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivots_idx = []\n",
    "pivots_vec = []\n",
    "for i in pivots_dict.keys():\n",
    "    pivots_idx.append(i)\n",
    "    pivots_vec.append(pivots_dict[i])\n",
    "len(pivots_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pivots = 5000\n",
    "pivots_idx, pivots_vec = get_pivots_slice(pivots_dict, n_pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_pretrain = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/w2v_google_50d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 0.0001\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1000 # number of samples each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    # embedding layer\n",
    "    init_width = 0.5 / n_embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "#     embedding = tf.Variable(google_pretrain)\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # add regularization term\n",
    "    embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "    update_embed_op = tf.scatter_update(embedding_copy,pivots_idx,pivots_vec)\n",
    "    embed_copy = tf.nn.embedding_lookup(embedding_copy, inputs)\n",
    "    reg_loss = reg_constant * tf.reduce_sum((embed-embed_copy)**2)\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "#     total_cost = cost \n",
    "    total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  7000000\n",
      "Starting training at  2018-07-21 17:13:49.827191\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 6.7379 Avg. Reg. loss: 2.8513 0.0425 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 5.3986 Avg. Reg. loss: 2.8845 0.0278 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.0377 Avg. Reg. loss: 2.7677 0.0277 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.8716 Avg. Reg. loss: 2.6760 0.0275 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.7755 Avg. Reg. loss: 2.5883 0.0273 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.7029 Avg. Reg. loss: 2.5097 0.0273 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.6407 Avg. Reg. loss: 2.4571 0.0267 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.6066 Avg. Reg. loss: 2.3913 0.0250 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.5775 Avg. Reg. loss: 2.3389 0.0242 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.5448 Avg. Reg. loss: 2.2667 0.0256 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.5079 Avg. Reg. loss: 2.2350 0.0259 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.4977 Avg. Reg. loss: 2.1835 0.0258 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.4648 Avg. Reg. loss: 2.1403 0.1356 sec/batch\n",
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.4473 Avg. Reg. loss: 2.1085 0.0261 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.4298 Avg. Reg. loss: 2.0614 0.0252 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.4201 Avg. Reg. loss: 2.0292 0.0264 sec/batch\n",
      "Epoch 1/10 Iteration: 17000 Avg. Training loss: 4.4064 Avg. Reg. loss: 1.9877 0.0278 sec/batch\n",
      "Epoch 1/10 Iteration: 18000 Avg. Training loss: 4.3882 Avg. Reg. loss: 1.9504 0.0258 sec/batch\n",
      "Epoch 1/10 Iteration: 19000 Avg. Training loss: 4.3777 Avg. Reg. loss: 1.9256 0.0256 sec/batch\n",
      "Epoch 1/10 Iteration: 20000 Avg. Training loss: 4.3719 Avg. Reg. loss: 1.8985 0.0254 sec/batch\n",
      "Epoch 1/10 Iteration: 21000 Avg. Training loss: 4.3553 Avg. Reg. loss: 1.8588 0.0248 sec/batch\n",
      "Epoch 1/10 Iteration: 22000 Avg. Training loss: 4.3403 Avg. Reg. loss: 1.8319 0.0246 sec/batch\n",
      "Epoch 1/10 Iteration: 23000 Avg. Training loss: 4.3283 Avg. Reg. loss: 1.8017 0.0247 sec/batch\n",
      "Epoch 1/10 Iteration: 24000 Avg. Training loss: 4.3324 Avg. Reg. loss: 1.7858 0.0249 sec/batch\n",
      "Epoch 1/10 Iteration: 25000 Avg. Training loss: 4.3104 Avg. Reg. loss: 1.7520 0.0254 sec/batch\n",
      "Epoch 1/10 Iteration: 26000 Avg. Training loss: 4.3037 Avg. Reg. loss: 1.7324 0.0253 sec/batch\n",
      "Epoch 1/10 Iteration: 27000 Avg. Training loss: 4.2900 Avg. Reg. loss: 1.7078 0.0245 sec/batch\n",
      "Epoch 1/10 Iteration: 28000 Avg. Training loss: 4.2841 Avg. Reg. loss: 1.6851 0.0253 sec/batch\n",
      "Epoch 1/10 Iteration: 29000 Avg. Training loss: 4.2732 Avg. Reg. loss: 1.6662 0.0247 sec/batch\n",
      "Epoch 1/10 Iteration: 30000 Avg. Training loss: 4.2534 Avg. Reg. loss: 1.6440 0.0254 sec/batch\n",
      "Epoch 1/10 Iteration: 31000 Avg. Training loss: 4.2672 Avg. Reg. loss: 1.6273 0.0256 sec/batch\n",
      "Epoch 1/10 Iteration: 32000 Avg. Training loss: 4.2541 Avg. Reg. loss: 1.5995 0.0253 sec/batch\n",
      "Epoch 1/10 Iteration: 33000 Avg. Training loss: 4.2422 Avg. Reg. loss: 1.5834 0.0252 sec/batch\n",
      "Epoch 1/10 Iteration: 34000 Avg. Training loss: 4.2413 Avg. Reg. loss: 1.5594 0.0248 sec/batch\n",
      "Epoch 1/10 Iteration: 35000 Avg. Training loss: 4.2402 Avg. Reg. loss: 1.5411 0.0248 sec/batch\n",
      "Epoch 1/10 Iteration: 36000 Avg. Training loss: 4.2270 Avg. Reg. loss: 1.5279 0.0241 sec/batch\n",
      "Epoch 1/10 Iteration: 37000 Avg. Training loss: 4.2195 Avg. Reg. loss: 1.5133 0.0254 sec/batch\n",
      "Epoch 1/10 Iteration: 38000 Avg. Training loss: 4.2155 Avg. Reg. loss: 1.5001 0.0251 sec/batch\n",
      "Epoch 1/10 Iteration: 39000 Avg. Training loss: 4.2138 Avg. Reg. loss: 1.4818 0.0253 sec/batch\n",
      "Epoch 1/10 Iteration: 40000 Avg. Training loss: 4.2132 Avg. Reg. loss: 1.4639 0.0264 sec/batch\n",
      "Epoch 1/10 Iteration: 41000 Avg. Training loss: 4.2128 Avg. Reg. loss: 1.4517 0.0258 sec/batch\n",
      "Epoch 1/10 Iteration: 42000 Avg. Training loss: 4.1959 Avg. Reg. loss: 1.4329 0.0253 sec/batch\n",
      "Epoch 1/10 Iteration: 43000 Avg. Training loss: 4.1926 Avg. Reg. loss: 1.4237 0.0250 sec/batch\n",
      "Epoch 1/10 Iteration: 44000 Avg. Training loss: 4.1811 Avg. Reg. loss: 1.4053 0.0246 sec/batch\n",
      "Epoch 1/10 Iteration: 45000 Avg. Training loss: 4.1931 Avg. Reg. loss: 1.4033 0.0247 sec/batch\n",
      "Epoch 1/10 Iteration: 46000 Avg. Training loss: 4.1685 Avg. Reg. loss: 1.3843 0.0244 sec/batch\n",
      "Epoch 2/10 Iteration: 47000 Avg. Training loss: 4.1627 Avg. Reg. loss: 1.3604 0.0245 sec/batch\n",
      "Epoch 2/10 Iteration: 48000 Avg. Training loss: 4.1504 Avg. Reg. loss: 1.3589 0.0241 sec/batch\n",
      "Epoch 2/10 Iteration: 49000 Avg. Training loss: 4.1705 Avg. Reg. loss: 1.3476 0.0225 sec/batch\n",
      "Epoch 2/10 Iteration: 50000 Avg. Training loss: 4.1481 Avg. Reg. loss: 1.3391 0.0240 sec/batch\n",
      "Epoch 2/10 Iteration: 51000 Avg. Training loss: 4.1557 Avg. Reg. loss: 1.3231 0.0242 sec/batch\n",
      "Epoch 2/10 Iteration: 52000 Avg. Training loss: 4.1515 Avg. Reg. loss: 1.3066 0.0243 sec/batch\n",
      "Epoch 2/10 Iteration: 53000 Avg. Training loss: 4.1320 Avg. Reg. loss: 1.3032 0.0244 sec/batch\n",
      "Epoch 2/10 Iteration: 54000 Avg. Training loss: 4.1384 Avg. Reg. loss: 1.2901 0.0245 sec/batch\n",
      "Epoch 2/10 Iteration: 55000 Avg. Training loss: 4.1332 Avg. Reg. loss: 1.2848 0.0245 sec/batch\n",
      "Epoch 2/10 Iteration: 56000 Avg. Training loss: 4.1332 Avg. Reg. loss: 1.2613 0.0246 sec/batch\n",
      "Epoch 2/10 Iteration: 57000 Avg. Training loss: 4.1134 Avg. Reg. loss: 1.2615 0.0246 sec/batch\n",
      "Epoch 2/10 Iteration: 58000 Avg. Training loss: 4.1267 Avg. Reg. loss: 1.2519 0.0249 sec/batch\n",
      "Epoch 2/10 Iteration: 59000 Avg. Training loss: 4.1030 Avg. Reg. loss: 1.2394 0.0238 sec/batch\n",
      "Epoch 2/10 Iteration: 60000 Avg. Training loss: 4.1098 Avg. Reg. loss: 1.2392 0.0242 sec/batch\n",
      "Epoch 2/10 Iteration: 61000 Avg. Training loss: 4.1039 Avg. Reg. loss: 1.2241 0.0246 sec/batch\n",
      "Epoch 2/10 Iteration: 62000 Avg. Training loss: 4.1066 Avg. Reg. loss: 1.2207 0.0246 sec/batch\n",
      "Epoch 2/10 Iteration: 63000 Avg. Training loss: 4.1045 Avg. Reg. loss: 1.2068 0.0246 sec/batch\n",
      "Epoch 2/10 Iteration: 64000 Avg. Training loss: 4.0927 Avg. Reg. loss: 1.1989 0.0245 sec/batch\n",
      "Epoch 2/10 Iteration: 65000 Avg. Training loss: 4.0959 Avg. Reg. loss: 1.1979 0.0244 sec/batch\n",
      "Epoch 2/10 Iteration: 66000 Avg. Training loss: 4.1023 Avg. Reg. loss: 1.1878 0.0236 sec/batch\n",
      "Epoch 2/10 Iteration: 67000 Avg. Training loss: 4.0921 Avg. Reg. loss: 1.1750 0.0233 sec/batch\n",
      "Epoch 2/10 Iteration: 68000 Avg. Training loss: 4.0826 Avg. Reg. loss: 1.1709 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 69000 Avg. Training loss: 4.0870 Avg. Reg. loss: 1.1611 0.0237 sec/batch\n",
      "Epoch 2/10 Iteration: 70000 Avg. Training loss: 4.0910 Avg. Reg. loss: 1.1572 0.0212 sec/batch\n",
      "Epoch 2/10 Iteration: 71000 Avg. Training loss: 4.0757 Avg. Reg. loss: 1.1449 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 72000 Avg. Training loss: 4.0794 Avg. Reg. loss: 1.1413 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 73000 Avg. Training loss: 4.0719 Avg. Reg. loss: 1.1323 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 74000 Avg. Training loss: 4.0729 Avg. Reg. loss: 1.1271 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 75000 Avg. Training loss: 4.0614 Avg. Reg. loss: 1.1239 0.0232 sec/batch\n",
      "Epoch 2/10 Iteration: 76000 Avg. Training loss: 4.0546 Avg. Reg. loss: 1.1153 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 77000 Avg. Training loss: 4.0653 Avg. Reg. loss: 1.1119 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 78000 Avg. Training loss: 4.0629 Avg. Reg. loss: 1.0995 0.0202 sec/batch\n",
      "Epoch 2/10 Iteration: 79000 Avg. Training loss: 4.0591 Avg. Reg. loss: 1.0934 0.0221 sec/batch\n",
      "Epoch 2/10 Iteration: 80000 Avg. Training loss: 4.0555 Avg. Reg. loss: 1.0851 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 81000 Avg. Training loss: 4.0675 Avg. Reg. loss: 1.0788 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 82000 Avg. Training loss: 4.0599 Avg. Reg. loss: 1.0763 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 83000 Avg. Training loss: 4.0568 Avg. Reg. loss: 1.0689 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 84000 Avg. Training loss: 4.0407 Avg. Reg. loss: 1.0670 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 85000 Avg. Training loss: 4.0514 Avg. Reg. loss: 1.0627 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 86000 Avg. Training loss: 4.0500 Avg. Reg. loss: 1.0549 0.0233 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Iteration: 87000 Avg. Training loss: 4.0533 Avg. Reg. loss: 1.0518 0.0247 sec/batch\n",
      "Epoch 2/10 Iteration: 88000 Avg. Training loss: 4.0494 Avg. Reg. loss: 1.0433 0.0231 sec/batch\n",
      "Epoch 2/10 Iteration: 89000 Avg. Training loss: 4.0426 Avg. Reg. loss: 1.0406 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 90000 Avg. Training loss: 4.0358 Avg. Reg. loss: 1.0321 0.0229 sec/batch\n",
      "Epoch 2/10 Iteration: 91000 Avg. Training loss: 4.0395 Avg. Reg. loss: 1.0387 0.0229 sec/batch\n",
      "Epoch 2/10 Iteration: 92000 Avg. Training loss: 4.0292 Avg. Reg. loss: 1.0280 0.0225 sec/batch\n",
      "Epoch 3/10 Iteration: 93000 Avg. Training loss: 4.0359 Avg. Reg. loss: 1.0132 0.0237 sec/batch\n",
      "Epoch 3/10 Iteration: 94000 Avg. Training loss: 4.0195 Avg. Reg. loss: 1.0186 0.0228 sec/batch\n",
      "Epoch 3/10 Iteration: 95000 Avg. Training loss: 4.0306 Avg. Reg. loss: 1.0174 0.0229 sec/batch\n",
      "Epoch 3/10 Iteration: 96000 Avg. Training loss: 4.0253 Avg. Reg. loss: 1.0132 0.0223 sec/batch\n",
      "Epoch 3/10 Iteration: 97000 Avg. Training loss: 4.0371 Avg. Reg. loss: 1.0057 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 98000 Avg. Training loss: 4.0251 Avg. Reg. loss: 0.9949 0.0210 sec/batch\n",
      "Epoch 3/10 Iteration: 99000 Avg. Training loss: 4.0171 Avg. Reg. loss: 0.9956 0.0226 sec/batch\n",
      "Epoch 3/10 Iteration: 100000 Avg. Training loss: 4.0283 Avg. Reg. loss: 0.9909 0.0230 sec/batch\n",
      "Epoch 3/10 Iteration: 101000 Avg. Training loss: 4.0230 Avg. Reg. loss: 0.9887 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 102000 Avg. Training loss: 4.0236 Avg. Reg. loss: 0.9757 0.0229 sec/batch\n",
      "Epoch 3/10 Iteration: 103000 Avg. Training loss: 4.0022 Avg. Reg. loss: 0.9787 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 104000 Avg. Training loss: 4.0288 Avg. Reg. loss: 0.9746 0.0228 sec/batch\n",
      "Epoch 3/10 Iteration: 105000 Avg. Training loss: 4.0050 Avg. Reg. loss: 0.9687 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 106000 Avg. Training loss: 4.0120 Avg. Reg. loss: 0.9710 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 107000 Avg. Training loss: 4.0015 Avg. Reg. loss: 0.9618 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 108000 Avg. Training loss: 4.0064 Avg. Reg. loss: 0.9631 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 109000 Avg. Training loss: 4.0039 Avg. Reg. loss: 0.9571 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 110000 Avg. Training loss: 3.9941 Avg. Reg. loss: 0.9540 0.0229 sec/batch\n",
      "Epoch 3/10 Iteration: 111000 Avg. Training loss: 4.0008 Avg. Reg. loss: 0.9549 0.0222 sec/batch\n",
      "Epoch 3/10 Iteration: 112000 Avg. Training loss: 4.0141 Avg. Reg. loss: 0.9495 0.0230 sec/batch\n",
      "Epoch 3/10 Iteration: 113000 Avg. Training loss: 4.0003 Avg. Reg. loss: 0.9422 0.0221 sec/batch\n",
      "Epoch 3/10 Iteration: 114000 Avg. Training loss: 3.9919 Avg. Reg. loss: 0.9435 0.0224 sec/batch\n",
      "Epoch 3/10 Iteration: 115000 Avg. Training loss: 3.9954 Avg. Reg. loss: 0.9383 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 116000 Avg. Training loss: 4.0049 Avg. Reg. loss: 0.9370 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 117000 Avg. Training loss: 3.9927 Avg. Reg. loss: 0.9318 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 118000 Avg. Training loss: 3.9998 Avg. Reg. loss: 0.9288 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 119000 Avg. Training loss: 3.9891 Avg. Reg. loss: 0.9239 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 120000 Avg. Training loss: 3.9936 Avg. Reg. loss: 0.9226 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 121000 Avg. Training loss: 3.9810 Avg. Reg. loss: 0.9238 0.0232 sec/batch\n",
      "Epoch 3/10 Iteration: 122000 Avg. Training loss: 3.9772 Avg. Reg. loss: 0.9188 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 123000 Avg. Training loss: 3.9895 Avg. Reg. loss: 0.9192 0.0232 sec/batch\n",
      "Epoch 3/10 Iteration: 124000 Avg. Training loss: 3.9916 Avg. Reg. loss: 0.9121 0.0227 sec/batch\n",
      "Epoch 3/10 Iteration: 125000 Avg. Training loss: 3.9815 Avg. Reg. loss: 0.9082 0.0233 sec/batch\n",
      "Epoch 3/10 Iteration: 126000 Avg. Training loss: 3.9850 Avg. Reg. loss: 0.9035 0.0232 sec/batch\n",
      "Epoch 3/10 Iteration: 127000 Avg. Training loss: 3.9946 Avg. Reg. loss: 0.9015 0.0232 sec/batch\n",
      "Epoch 3/10 Iteration: 128000 Avg. Training loss: 3.9932 Avg. Reg. loss: 0.9031 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 129000 Avg. Training loss: 3.9925 Avg. Reg. loss: 0.8985 0.0216 sec/batch\n",
      "Epoch 3/10 Iteration: 130000 Avg. Training loss: 3.9750 Avg. Reg. loss: 0.8985 0.0230 sec/batch\n",
      "Epoch 3/10 Iteration: 131000 Avg. Training loss: 3.9878 Avg. Reg. loss: 0.8985 0.0224 sec/batch\n",
      "Epoch 3/10 Iteration: 132000 Avg. Training loss: 3.9831 Avg. Reg. loss: 0.8929 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 133000 Avg. Training loss: 3.9869 Avg. Reg. loss: 0.8926 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 134000 Avg. Training loss: 3.9818 Avg. Reg. loss: 0.8870 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 135000 Avg. Training loss: 3.9889 Avg. Reg. loss: 0.8856 0.0220 sec/batch\n",
      "Epoch 3/10 Iteration: 136000 Avg. Training loss: 3.9786 Avg. Reg. loss: 0.8803 0.0240 sec/batch\n",
      "Epoch 3/10 Iteration: 137000 Avg. Training loss: 3.9809 Avg. Reg. loss: 0.8873 0.0230 sec/batch\n",
      "Epoch 3/10 Iteration: 138000 Avg. Training loss: 3.9708 Avg. Reg. loss: 0.8809 0.0213 sec/batch\n",
      "Epoch 4/10 Iteration: 139000 Avg. Training loss: 3.9745 Avg. Reg. loss: 0.8691 0.0231 sec/batch\n",
      "Epoch 4/10 Iteration: 140000 Avg. Training loss: 3.9615 Avg. Reg. loss: 0.8771 0.0230 sec/batch\n",
      "Epoch 4/10 Iteration: 141000 Avg. Training loss: 3.9763 Avg. Reg. loss: 0.8785 0.0230 sec/batch\n",
      "Epoch 4/10 Iteration: 142000 Avg. Training loss: 3.9656 Avg. Reg. loss: 0.8793 0.1149 sec/batch\n",
      "Epoch 4/10 Iteration: 143000 Avg. Training loss: 3.9789 Avg. Reg. loss: 0.8749 0.0242 sec/batch\n",
      "Epoch 4/10 Iteration: 144000 Avg. Training loss: 3.9689 Avg. Reg. loss: 0.8645 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 145000 Avg. Training loss: 3.9601 Avg. Reg. loss: 0.8658 0.0241 sec/batch\n",
      "Epoch 4/10 Iteration: 146000 Avg. Training loss: 3.9622 Avg. Reg. loss: 0.8665 0.0240 sec/batch\n",
      "Epoch 4/10 Iteration: 147000 Avg. Training loss: 3.9647 Avg. Reg. loss: 0.8653 0.0235 sec/batch\n",
      "Epoch 4/10 Iteration: 148000 Avg. Training loss: 3.9689 Avg. Reg. loss: 0.8552 0.0234 sec/batch\n",
      "Epoch 4/10 Iteration: 149000 Avg. Training loss: 3.9554 Avg. Reg. loss: 0.8604 0.0230 sec/batch\n",
      "Epoch 4/10 Iteration: 150000 Avg. Training loss: 3.9681 Avg. Reg. loss: 0.8588 0.0224 sec/batch\n",
      "Epoch 4/10 Iteration: 151000 Avg. Training loss: 3.9528 Avg. Reg. loss: 0.8541 0.0231 sec/batch\n",
      "Epoch 4/10 Iteration: 152000 Avg. Training loss: 3.9612 Avg. Reg. loss: 0.8593 0.0227 sec/batch\n",
      "Epoch 4/10 Iteration: 153000 Avg. Training loss: 3.9617 Avg. Reg. loss: 0.8515 0.0227 sec/batch\n",
      "Epoch 4/10 Iteration: 154000 Avg. Training loss: 3.9686 Avg. Reg. loss: 0.8535 0.0232 sec/batch\n",
      "Epoch 4/10 Iteration: 155000 Avg. Training loss: 3.9595 Avg. Reg. loss: 0.8494 0.0225 sec/batch\n",
      "Epoch 4/10 Iteration: 156000 Avg. Training loss: 3.9516 Avg. Reg. loss: 0.8474 0.0232 sec/batch\n",
      "Epoch 4/10 Iteration: 157000 Avg. Training loss: 3.9566 Avg. Reg. loss: 0.8507 0.0218 sec/batch\n",
      "Epoch 4/10 Iteration: 158000 Avg. Training loss: 3.9627 Avg. Reg. loss: 0.8484 0.0229 sec/batch\n",
      "Epoch 4/10 Iteration: 159000 Avg. Training loss: 3.9630 Avg. Reg. loss: 0.8412 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 160000 Avg. Training loss: 3.9484 Avg. Reg. loss: 0.8460 0.0232 sec/batch\n",
      "Epoch 4/10 Iteration: 161000 Avg. Training loss: 3.9512 Avg. Reg. loss: 0.8436 0.0229 sec/batch\n",
      "Epoch 4/10 Iteration: 162000 Avg. Training loss: 3.9567 Avg. Reg. loss: 0.8429 0.0235 sec/batch\n",
      "Epoch 4/10 Iteration: 163000 Avg. Training loss: 3.9540 Avg. Reg. loss: 0.8392 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 164000 Avg. Training loss: 3.9522 Avg. Reg. loss: 0.8390 0.0238 sec/batch\n",
      "Epoch 4/10 Iteration: 165000 Avg. Training loss: 3.9442 Avg. Reg. loss: 0.8341 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 166000 Avg. Training loss: 3.9520 Avg. Reg. loss: 0.8339 0.0229 sec/batch\n",
      "Epoch 4/10 Iteration: 167000 Avg. Training loss: 3.9498 Avg. Reg. loss: 0.8360 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 168000 Avg. Training loss: 3.9334 Avg. Reg. loss: 0.8347 0.0235 sec/batch\n",
      "Epoch 4/10 Iteration: 169000 Avg. Training loss: 3.9513 Avg. Reg. loss: 0.8358 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 170000 Avg. Training loss: 3.9508 Avg. Reg. loss: 0.8303 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 171000 Avg. Training loss: 3.9453 Avg. Reg. loss: 0.8291 0.0232 sec/batch\n",
      "Epoch 4/10 Iteration: 172000 Avg. Training loss: 3.9559 Avg. Reg. loss: 0.8235 0.0236 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Iteration: 173000 Avg. Training loss: 3.9585 Avg. Reg. loss: 0.8234 0.0218 sec/batch\n",
      "Epoch 4/10 Iteration: 174000 Avg. Training loss: 3.9557 Avg. Reg. loss: 0.8248 0.0237 sec/batch\n",
      "Epoch 4/10 Iteration: 175000 Avg. Training loss: 3.9457 Avg. Reg. loss: 0.8245 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 176000 Avg. Training loss: 3.9497 Avg. Reg. loss: 0.8242 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 177000 Avg. Training loss: 3.9497 Avg. Reg. loss: 0.8269 0.0233 sec/batch\n",
      "Epoch 4/10 Iteration: 178000 Avg. Training loss: 3.9494 Avg. Reg. loss: 0.8221 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 179000 Avg. Training loss: 3.9560 Avg. Reg. loss: 0.8225 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 180000 Avg. Training loss: 3.9514 Avg. Reg. loss: 0.8190 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 181000 Avg. Training loss: 3.9469 Avg. Reg. loss: 0.8191 0.0237 sec/batch\n",
      "Epoch 4/10 Iteration: 182000 Avg. Training loss: 3.9407 Avg. Reg. loss: 0.8145 0.0237 sec/batch\n",
      "Epoch 4/10 Iteration: 183000 Avg. Training loss: 3.9481 Avg. Reg. loss: 0.8229 0.0236 sec/batch\n",
      "Epoch 4/10 Iteration: 184000 Avg. Training loss: 3.9353 Avg. Reg. loss: 0.8190 0.0232 sec/batch\n",
      "Epoch 5/10 Iteration: 185000 Avg. Training loss: 3.9437 Avg. Reg. loss: 0.8080 0.0237 sec/batch\n",
      "Epoch 5/10 Iteration: 186000 Avg. Training loss: 3.9292 Avg. Reg. loss: 0.8161 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 187000 Avg. Training loss: 3.9451 Avg. Reg. loss: 0.8186 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 188000 Avg. Training loss: 3.9388 Avg. Reg. loss: 0.8185 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 189000 Avg. Training loss: 3.9473 Avg. Reg. loss: 0.8167 0.0234 sec/batch\n",
      "Epoch 5/10 Iteration: 190000 Avg. Training loss: 3.9450 Avg. Reg. loss: 0.8083 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 191000 Avg. Training loss: 3.9271 Avg. Reg. loss: 0.8102 0.0233 sec/batch\n",
      "Epoch 5/10 Iteration: 192000 Avg. Training loss: 3.9356 Avg. Reg. loss: 0.8119 0.0223 sec/batch\n",
      "Epoch 5/10 Iteration: 193000 Avg. Training loss: 3.9370 Avg. Reg. loss: 0.8116 0.0233 sec/batch\n",
      "Epoch 5/10 Iteration: 194000 Avg. Training loss: 3.9340 Avg. Reg. loss: 0.8028 0.0234 sec/batch\n",
      "Epoch 5/10 Iteration: 195000 Avg. Training loss: 3.9253 Avg. Reg. loss: 0.8078 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 196000 Avg. Training loss: 3.9424 Avg. Reg. loss: 0.8092 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 197000 Avg. Training loss: 3.9269 Avg. Reg. loss: 0.8051 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 198000 Avg. Training loss: 3.9368 Avg. Reg. loss: 0.8108 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 199000 Avg. Training loss: 3.9250 Avg. Reg. loss: 0.8034 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 200000 Avg. Training loss: 3.9348 Avg. Reg. loss: 0.8060 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 201000 Avg. Training loss: 3.9343 Avg. Reg. loss: 0.8034 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 202000 Avg. Training loss: 3.9493 Avg. Reg. loss: 0.8025 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 203000 Avg. Training loss: 3.9396 Avg. Reg. loss: 0.8055 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 204000 Avg. Training loss: 3.9435 Avg. Reg. loss: 0.8045 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 205000 Avg. Training loss: 3.9425 Avg. Reg. loss: 0.7979 0.0232 sec/batch\n",
      "Epoch 5/10 Iteration: 206000 Avg. Training loss: 3.9269 Avg. Reg. loss: 0.8044 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 207000 Avg. Training loss: 3.9431 Avg. Reg. loss: 0.8017 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 208000 Avg. Training loss: 3.9321 Avg. Reg. loss: 0.8008 0.0237 sec/batch\n",
      "Epoch 5/10 Iteration: 209000 Avg. Training loss: 3.9320 Avg. Reg. loss: 0.7987 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 210000 Avg. Training loss: 3.9367 Avg. Reg. loss: 0.7995 0.0234 sec/batch\n",
      "Epoch 5/10 Iteration: 211000 Avg. Training loss: 3.9212 Avg. Reg. loss: 0.7951 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 212000 Avg. Training loss: 3.9278 Avg. Reg. loss: 0.7968 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 213000 Avg. Training loss: 3.9239 Avg. Reg. loss: 0.7981 0.0233 sec/batch\n",
      "Epoch 5/10 Iteration: 214000 Avg. Training loss: 3.9130 Avg. Reg. loss: 0.7978 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 215000 Avg. Training loss: 3.9301 Avg. Reg. loss: 0.8001 0.0218 sec/batch\n",
      "Epoch 5/10 Iteration: 216000 Avg. Training loss: 3.9302 Avg. Reg. loss: 0.7969 0.0224 sec/batch\n",
      "Epoch 5/10 Iteration: 217000 Avg. Training loss: 3.9226 Avg. Reg. loss: 0.7950 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 218000 Avg. Training loss: 3.9261 Avg. Reg. loss: 0.7901 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 219000 Avg. Training loss: 3.9358 Avg. Reg. loss: 0.7920 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 220000 Avg. Training loss: 3.9323 Avg. Reg. loss: 0.7937 0.0230 sec/batch\n",
      "Epoch 5/10 Iteration: 221000 Avg. Training loss: 3.9275 Avg. Reg. loss: 0.7945 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 222000 Avg. Training loss: 3.9221 Avg. Reg. loss: 0.7940 0.0225 sec/batch\n",
      "Epoch 5/10 Iteration: 223000 Avg. Training loss: 3.9359 Avg. Reg. loss: 0.7975 0.0233 sec/batch\n",
      "Epoch 5/10 Iteration: 224000 Avg. Training loss: 3.9274 Avg. Reg. loss: 0.7934 0.0233 sec/batch\n",
      "Epoch 5/10 Iteration: 225000 Avg. Training loss: 3.9336 Avg. Reg. loss: 0.7957 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 226000 Avg. Training loss: 3.9339 Avg. Reg. loss: 0.7908 0.0231 sec/batch\n",
      "Epoch 5/10 Iteration: 227000 Avg. Training loss: 3.9285 Avg. Reg. loss: 0.7922 0.0225 sec/batch\n",
      "Epoch 5/10 Iteration: 228000 Avg. Training loss: 3.9205 Avg. Reg. loss: 0.7870 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 229000 Avg. Training loss: 3.9364 Avg. Reg. loss: 0.7971 0.0236 sec/batch\n",
      "Epoch 5/10 Iteration: 230000 Avg. Training loss: 3.9198 Avg. Reg. loss: 0.7928 0.0235 sec/batch\n",
      "Epoch 6/10 Iteration: 231000 Avg. Training loss: 3.9279 Avg. Reg. loss: 0.7833 0.0233 sec/batch\n",
      "Epoch 6/10 Iteration: 232000 Avg. Training loss: 3.9134 Avg. Reg. loss: 0.7920 0.0235 sec/batch\n",
      "Epoch 6/10 Iteration: 233000 Avg. Training loss: 3.9331 Avg. Reg. loss: 0.7946 0.0237 sec/batch\n",
      "Epoch 6/10 Iteration: 234000 Avg. Training loss: 3.9184 Avg. Reg. loss: 0.7961 0.0234 sec/batch\n",
      "Epoch 6/10 Iteration: 235000 Avg. Training loss: 3.9247 Avg. Reg. loss: 0.7947 0.0231 sec/batch\n",
      "Epoch 6/10 Iteration: 236000 Avg. Training loss: 3.9265 Avg. Reg. loss: 0.7859 0.0234 sec/batch\n",
      "Epoch 6/10 Iteration: 237000 Avg. Training loss: 3.9105 Avg. Reg. loss: 0.7884 0.0236 sec/batch\n",
      "Epoch 6/10 Iteration: 238000 Avg. Training loss: 3.9256 Avg. Reg. loss: 0.7900 0.0233 sec/batch\n",
      "Epoch 6/10 Iteration: 239000 Avg. Training loss: 3.9232 Avg. Reg. loss: 0.7902 0.0235 sec/batch\n",
      "Epoch 6/10 Iteration: 240000 Avg. Training loss: 3.9242 Avg. Reg. loss: 0.7826 0.0235 sec/batch\n",
      "Epoch 6/10 Iteration: 241000 Avg. Training loss: 3.9142 Avg. Reg. loss: 0.7876 0.0236 sec/batch\n",
      "Epoch 6/10 Iteration: 242000 Avg. Training loss: 3.9274 Avg. Reg. loss: 0.7878 0.0273 sec/batch\n",
      "Epoch 6/10 Iteration: 243000 Avg. Training loss: 3.9114 Avg. Reg. loss: 0.7842 0.0309 sec/batch\n",
      "Epoch 6/10 Iteration: 244000 Avg. Training loss: 3.9286 Avg. Reg. loss: 0.7928 0.0264 sec/batch\n",
      "Epoch 6/10 Iteration: 245000 Avg. Training loss: 3.9101 Avg. Reg. loss: 0.7850 0.0258 sec/batch\n",
      "Epoch 6/10 Iteration: 246000 Avg. Training loss: 3.9285 Avg. Reg. loss: 0.7880 0.0270 sec/batch\n",
      "Epoch 6/10 Iteration: 247000 Avg. Training loss: 3.9200 Avg. Reg. loss: 0.7868 0.0269 sec/batch\n",
      "Epoch 6/10 Iteration: 248000 Avg. Training loss: 3.9127 Avg. Reg. loss: 0.7853 0.0265 sec/batch\n",
      "Epoch 6/10 Iteration: 249000 Avg. Training loss: 3.9182 Avg. Reg. loss: 0.7901 0.0260 sec/batch\n",
      "Epoch 6/10 Iteration: 250000 Avg. Training loss: 3.9313 Avg. Reg. loss: 0.7881 0.0248 sec/batch\n",
      "Epoch 6/10 Iteration: 251000 Avg. Training loss: 3.9233 Avg. Reg. loss: 0.7835 0.0263 sec/batch\n",
      "Epoch 6/10 Iteration: 252000 Avg. Training loss: 3.9118 Avg. Reg. loss: 0.7898 0.0246 sec/batch\n",
      "Epoch 6/10 Iteration: 253000 Avg. Training loss: 3.9170 Avg. Reg. loss: 0.7881 0.0244 sec/batch\n",
      "Epoch 6/10 Iteration: 254000 Avg. Training loss: 3.9209 Avg. Reg. loss: 0.7871 0.0267 sec/batch\n",
      "Epoch 6/10 Iteration: 255000 Avg. Training loss: 3.9096 Avg. Reg. loss: 0.7855 0.0255 sec/batch\n",
      "Epoch 6/10 Iteration: 256000 Avg. Training loss: 3.9195 Avg. Reg. loss: 0.7876 0.0253 sec/batch\n",
      "Epoch 6/10 Iteration: 257000 Avg. Training loss: 3.9154 Avg. Reg. loss: 0.7821 0.0252 sec/batch\n",
      "Epoch 6/10 Iteration: 258000 Avg. Training loss: 3.9204 Avg. Reg. loss: 0.7844 0.0254 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Iteration: 259000 Avg. Training loss: 3.9150 Avg. Reg. loss: 0.7851 0.0255 sec/batch\n",
      "Epoch 6/10 Iteration: 260000 Avg. Training loss: 3.9005 Avg. Reg. loss: 0.7849 0.0246 sec/batch\n",
      "Epoch 6/10 Iteration: 261000 Avg. Training loss: 3.9181 Avg. Reg. loss: 0.7879 0.0252 sec/batch\n",
      "Epoch 6/10 Iteration: 262000 Avg. Training loss: 3.9124 Avg. Reg. loss: 0.7858 0.0264 sec/batch\n",
      "Epoch 6/10 Iteration: 263000 Avg. Training loss: 3.9114 Avg. Reg. loss: 0.7848 0.0236 sec/batch\n",
      "Epoch 6/10 Iteration: 264000 Avg. Training loss: 3.9268 Avg. Reg. loss: 0.7802 0.0246 sec/batch\n",
      "Epoch 6/10 Iteration: 265000 Avg. Training loss: 3.9322 Avg. Reg. loss: 0.7827 0.0252 sec/batch\n",
      "Epoch 6/10 Iteration: 266000 Avg. Training loss: 3.9234 Avg. Reg. loss: 0.7835 0.0251 sec/batch\n",
      "Epoch 6/10 Iteration: 267000 Avg. Training loss: 3.9128 Avg. Reg. loss: 0.7847 0.0254 sec/batch\n",
      "Epoch 6/10 Iteration: 268000 Avg. Training loss: 3.9062 Avg. Reg. loss: 0.7843 0.0254 sec/batch\n",
      "Epoch 6/10 Iteration: 269000 Avg. Training loss: 3.9307 Avg. Reg. loss: 0.7889 0.0255 sec/batch\n",
      "Epoch 6/10 Iteration: 270000 Avg. Training loss: 3.9182 Avg. Reg. loss: 0.7854 0.0252 sec/batch\n",
      "Epoch 6/10 Iteration: 271000 Avg. Training loss: 3.9238 Avg. Reg. loss: 0.7882 0.0249 sec/batch\n",
      "Epoch 6/10 Iteration: 272000 Avg. Training loss: 3.9279 Avg. Reg. loss: 0.7850 0.0253 sec/batch\n",
      "Epoch 6/10 Iteration: 273000 Avg. Training loss: 3.9180 Avg. Reg. loss: 0.7847 0.0251 sec/batch\n",
      "Epoch 6/10 Iteration: 274000 Avg. Training loss: 3.9117 Avg. Reg. loss: 0.7811 0.0252 sec/batch\n",
      "Epoch 6/10 Iteration: 275000 Avg. Training loss: 3.9189 Avg. Reg. loss: 0.7908 0.0255 sec/batch\n",
      "Epoch 6/10 Iteration: 276000 Avg. Training loss: 3.9102 Avg. Reg. loss: 0.7873 0.0235 sec/batch\n",
      "Epoch 7/10 Iteration: 277000 Avg. Training loss: 3.9249 Avg. Reg. loss: 0.7782 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 278000 Avg. Training loss: 3.9144 Avg. Reg. loss: 0.7880 0.0251 sec/batch\n",
      "Epoch 7/10 Iteration: 279000 Avg. Training loss: 3.9176 Avg. Reg. loss: 0.7897 0.0252 sec/batch\n",
      "Epoch 7/10 Iteration: 280000 Avg. Training loss: 3.9104 Avg. Reg. loss: 0.7897 0.0252 sec/batch\n",
      "Epoch 7/10 Iteration: 281000 Avg. Training loss: 3.9151 Avg. Reg. loss: 0.7902 0.0252 sec/batch\n",
      "Epoch 7/10 Iteration: 282000 Avg. Training loss: 3.9233 Avg. Reg. loss: 0.7832 0.0251 sec/batch\n",
      "Epoch 7/10 Iteration: 283000 Avg. Training loss: 3.9031 Avg. Reg. loss: 0.7859 0.0248 sec/batch\n",
      "Epoch 7/10 Iteration: 284000 Avg. Training loss: 3.9104 Avg. Reg. loss: 0.7886 0.0255 sec/batch\n",
      "Epoch 7/10 Iteration: 285000 Avg. Training loss: 3.9168 Avg. Reg. loss: 0.7881 0.0232 sec/batch\n",
      "Epoch 7/10 Iteration: 286000 Avg. Training loss: 3.9119 Avg. Reg. loss: 0.7809 0.0253 sec/batch\n",
      "Epoch 7/10 Iteration: 287000 Avg. Training loss: 3.9048 Avg. Reg. loss: 0.7860 0.0245 sec/batch\n",
      "Epoch 7/10 Iteration: 288000 Avg. Training loss: 3.9228 Avg. Reg. loss: 0.7872 0.0238 sec/batch\n",
      "Epoch 7/10 Iteration: 289000 Avg. Training loss: 3.9057 Avg. Reg. loss: 0.7846 0.0252 sec/batch\n",
      "Epoch 7/10 Iteration: 290000 Avg. Training loss: 3.9110 Avg. Reg. loss: 0.7925 0.0255 sec/batch\n",
      "Epoch 7/10 Iteration: 291000 Avg. Training loss: 3.9016 Avg. Reg. loss: 0.7853 0.0246 sec/batch\n",
      "Epoch 7/10 Iteration: 292000 Avg. Training loss: 3.9173 Avg. Reg. loss: 0.7876 0.0245 sec/batch\n",
      "Epoch 7/10 Iteration: 293000 Avg. Training loss: 3.9128 Avg. Reg. loss: 0.7856 0.0254 sec/batch\n",
      "Epoch 7/10 Iteration: 294000 Avg. Training loss: 3.9087 Avg. Reg. loss: 0.7865 0.0267 sec/batch\n",
      "Epoch 7/10 Iteration: 295000 Avg. Training loss: 3.9210 Avg. Reg. loss: 0.7917 0.0281 sec/batch\n",
      "Epoch 7/10 Iteration: 296000 Avg. Training loss: 3.9199 Avg. Reg. loss: 0.7889 0.0268 sec/batch\n",
      "Epoch 7/10 Iteration: 297000 Avg. Training loss: 3.9074 Avg. Reg. loss: 0.7841 0.0267 sec/batch\n",
      "Epoch 7/10 Iteration: 298000 Avg. Training loss: 3.9049 Avg. Reg. loss: 0.7916 0.0254 sec/batch\n",
      "Epoch 7/10 Iteration: 299000 Avg. Training loss: 3.9072 Avg. Reg. loss: 0.7901 0.0259 sec/batch\n",
      "Epoch 7/10 Iteration: 300000 Avg. Training loss: 3.9135 Avg. Reg. loss: 0.7892 0.0260 sec/batch\n",
      "Epoch 7/10 Iteration: 301000 Avg. Training loss: 3.9084 Avg. Reg. loss: 0.7877 0.0262 sec/batch\n",
      "Epoch 7/10 Iteration: 302000 Avg. Training loss: 3.9108 Avg. Reg. loss: 0.7899 0.0274 sec/batch\n",
      "Epoch 7/10 Iteration: 303000 Avg. Training loss: 3.9037 Avg. Reg. loss: 0.7842 0.0274 sec/batch\n",
      "Epoch 7/10 Iteration: 304000 Avg. Training loss: 3.9084 Avg. Reg. loss: 0.7864 0.0274 sec/batch\n",
      "Epoch 7/10 Iteration: 305000 Avg. Training loss: 3.9089 Avg. Reg. loss: 0.7890 0.0269 sec/batch\n",
      "Epoch 7/10 Iteration: 306000 Avg. Training loss: 3.8980 Avg. Reg. loss: 0.7903 0.0292 sec/batch\n",
      "Epoch 7/10 Iteration: 307000 Avg. Training loss: 3.9065 Avg. Reg. loss: 0.7917 0.0271 sec/batch\n",
      "Epoch 7/10 Iteration: 308000 Avg. Training loss: 3.9136 Avg. Reg. loss: 0.7903 0.0275 sec/batch\n",
      "Epoch 7/10 Iteration: 309000 Avg. Training loss: 3.9022 Avg. Reg. loss: 0.7882 0.0257 sec/batch\n",
      "Epoch 7/10 Iteration: 310000 Avg. Training loss: 3.9078 Avg. Reg. loss: 0.7840 0.0253 sec/batch\n",
      "Epoch 7/10 Iteration: 311000 Avg. Training loss: 3.9167 Avg. Reg. loss: 0.7863 0.0266 sec/batch\n",
      "Epoch 7/10 Iteration: 312000 Avg. Training loss: 3.9108 Avg. Reg. loss: 0.7881 0.0242 sec/batch\n",
      "Epoch 7/10 Iteration: 313000 Avg. Training loss: 3.9090 Avg. Reg. loss: 0.7900 0.0244 sec/batch\n",
      "Epoch 7/10 Iteration: 314000 Avg. Training loss: 3.9040 Avg. Reg. loss: 0.7889 0.0256 sec/batch\n",
      "Epoch 7/10 Iteration: 315000 Avg. Training loss: 3.9102 Avg. Reg. loss: 0.7953 0.0264 sec/batch\n",
      "Epoch 7/10 Iteration: 316000 Avg. Training loss: 3.9070 Avg. Reg. loss: 0.7916 0.0246 sec/batch\n",
      "Epoch 7/10 Iteration: 317000 Avg. Training loss: 3.9200 Avg. Reg. loss: 0.7943 0.0245 sec/batch\n",
      "Epoch 7/10 Iteration: 318000 Avg. Training loss: 3.9138 Avg. Reg. loss: 0.7911 0.0271 sec/batch\n",
      "Epoch 7/10 Iteration: 319000 Avg. Training loss: 3.9174 Avg. Reg. loss: 0.7923 0.0272 sec/batch\n",
      "Epoch 7/10 Iteration: 320000 Avg. Training loss: 3.9116 Avg. Reg. loss: 0.7878 0.0261 sec/batch\n",
      "Epoch 7/10 Iteration: 321000 Avg. Training loss: 3.9110 Avg. Reg. loss: 0.7971 0.0272 sec/batch\n",
      "Epoch 7/10 Iteration: 322000 Avg. Training loss: 3.9029 Avg. Reg. loss: 0.7946 0.0295 sec/batch\n",
      "Epoch 8/10 Iteration: 323000 Avg. Training loss: 3.9120 Avg. Reg. loss: 0.7867 0.0281 sec/batch\n",
      "Epoch 8/10 Iteration: 324000 Avg. Training loss: 3.8971 Avg. Reg. loss: 0.7933 0.0255 sec/batch\n",
      "Epoch 8/10 Iteration: 325000 Avg. Training loss: 3.9094 Avg. Reg. loss: 0.7980 0.0250 sec/batch\n",
      "Epoch 8/10 Iteration: 326000 Avg. Training loss: 3.9092 Avg. Reg. loss: 0.7981 0.0272 sec/batch\n",
      "Epoch 8/10 Iteration: 327000 Avg. Training loss: 3.9098 Avg. Reg. loss: 0.7996 0.0261 sec/batch\n",
      "Epoch 8/10 Iteration: 328000 Avg. Training loss: 3.9094 Avg. Reg. loss: 0.7906 0.0253 sec/batch\n",
      "Epoch 8/10 Iteration: 329000 Avg. Training loss: 3.8977 Avg. Reg. loss: 0.7931 0.0322 sec/batch\n",
      "Epoch 8/10 Iteration: 330000 Avg. Training loss: 3.9260 Avg. Reg. loss: 0.7983 0.0283 sec/batch\n",
      "Epoch 8/10 Iteration: 331000 Avg. Training loss: 3.9081 Avg. Reg. loss: 0.7965 0.0239 sec/batch\n",
      "Epoch 8/10 Iteration: 332000 Avg. Training loss: 3.9057 Avg. Reg. loss: 0.7893 0.0252 sec/batch\n",
      "Epoch 8/10 Iteration: 333000 Avg. Training loss: 3.8949 Avg. Reg. loss: 0.7939 0.0273 sec/batch\n",
      "Epoch 8/10 Iteration: 334000 Avg. Training loss: 3.9131 Avg. Reg. loss: 0.7963 0.0269 sec/batch\n",
      "Epoch 8/10 Iteration: 335000 Avg. Training loss: 3.9074 Avg. Reg. loss: 0.7942 0.0270 sec/batch\n",
      "Epoch 8/10 Iteration: 336000 Avg. Training loss: 3.9066 Avg. Reg. loss: 0.8026 0.0273 sec/batch\n",
      "Epoch 8/10 Iteration: 337000 Avg. Training loss: 3.8994 Avg. Reg. loss: 0.7946 0.0267 sec/batch\n",
      "Epoch 8/10 Iteration: 338000 Avg. Training loss: 3.9068 Avg. Reg. loss: 0.7968 0.0247 sec/batch\n",
      "Epoch 8/10 Iteration: 339000 Avg. Training loss: 3.9079 Avg. Reg. loss: 0.7951 0.0265 sec/batch\n",
      "Epoch 8/10 Iteration: 340000 Avg. Training loss: 3.9040 Avg. Reg. loss: 0.7960 0.0257 sec/batch\n",
      "Epoch 8/10 Iteration: 341000 Avg. Training loss: 3.9048 Avg. Reg. loss: 0.8017 0.0260 sec/batch\n",
      "Epoch 8/10 Iteration: 342000 Avg. Training loss: 3.9123 Avg. Reg. loss: 0.7998 0.0242 sec/batch\n",
      "Epoch 8/10 Iteration: 343000 Avg. Training loss: 3.9096 Avg. Reg. loss: 0.7954 0.0239 sec/batch\n",
      "Epoch 8/10 Iteration: 344000 Avg. Training loss: 3.9002 Avg. Reg. loss: 0.8029 0.0242 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Iteration: 345000 Avg. Training loss: 3.9111 Avg. Reg. loss: 0.8018 0.0235 sec/batch\n",
      "Epoch 8/10 Iteration: 346000 Avg. Training loss: 3.9089 Avg. Reg. loss: 0.8004 0.0234 sec/batch\n",
      "Epoch 8/10 Iteration: 347000 Avg. Training loss: 3.9046 Avg. Reg. loss: 0.7978 0.0236 sec/batch\n",
      "Epoch 8/10 Iteration: 348000 Avg. Training loss: 3.9054 Avg. Reg. loss: 0.8014 0.0235 sec/batch\n",
      "Epoch 8/10 Iteration: 349000 Avg. Training loss: 3.8943 Avg. Reg. loss: 0.7947 0.0232 sec/batch\n",
      "Epoch 8/10 Iteration: 350000 Avg. Training loss: 3.9057 Avg. Reg. loss: 0.7986 0.0239 sec/batch\n",
      "Epoch 8/10 Iteration: 351000 Avg. Training loss: 3.9042 Avg. Reg. loss: 0.8000 0.0233 sec/batch\n",
      "Epoch 8/10 Iteration: 352000 Avg. Training loss: 3.8926 Avg. Reg. loss: 0.8023 0.0229 sec/batch\n",
      "Epoch 8/10 Iteration: 353000 Avg. Training loss: 3.9110 Avg. Reg. loss: 0.8040 0.0236 sec/batch\n",
      "Epoch 8/10 Iteration: 354000 Avg. Training loss: 3.9005 Avg. Reg. loss: 0.8027 0.0239 sec/batch\n",
      "Epoch 8/10 Iteration: 355000 Avg. Training loss: 3.9027 Avg. Reg. loss: 0.8022 0.0237 sec/batch\n",
      "Epoch 8/10 Iteration: 356000 Avg. Training loss: 3.9115 Avg. Reg. loss: 0.7992 0.0239 sec/batch\n",
      "Epoch 8/10 Iteration: 357000 Avg. Training loss: 3.9174 Avg. Reg. loss: 0.8003 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 358000 Avg. Training loss: 3.9169 Avg. Reg. loss: 0.8022 0.0238 sec/batch\n",
      "Epoch 8/10 Iteration: 359000 Avg. Training loss: 3.9083 Avg. Reg. loss: 0.8056 0.0234 sec/batch\n",
      "Epoch 8/10 Iteration: 360000 Avg. Training loss: 3.8963 Avg. Reg. loss: 0.8024 0.0233 sec/batch\n",
      "Epoch 8/10 Iteration: 361000 Avg. Training loss: 3.9069 Avg. Reg. loss: 0.8081 0.0236 sec/batch\n",
      "Epoch 8/10 Iteration: 362000 Avg. Training loss: 3.9051 Avg. Reg. loss: 0.8063 0.0236 sec/batch\n",
      "Epoch 8/10 Iteration: 363000 Avg. Training loss: 3.9154 Avg. Reg. loss: 0.8082 0.0238 sec/batch\n",
      "Epoch 8/10 Iteration: 364000 Avg. Training loss: 3.9120 Avg. Reg. loss: 0.8053 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 365000 Avg. Training loss: 3.9245 Avg. Reg. loss: 0.8072 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 366000 Avg. Training loss: 3.9123 Avg. Reg. loss: 0.8019 0.0238 sec/batch\n",
      "Epoch 8/10 Iteration: 367000 Avg. Training loss: 3.9097 Avg. Reg. loss: 0.8111 0.0236 sec/batch\n",
      "Epoch 8/10 Iteration: 368000 Avg. Training loss: 3.9139 Avg. Reg. loss: 0.8088 0.0236 sec/batch\n",
      "Epoch 9/10 Iteration: 369000 Avg. Training loss: 3.9094 Avg. Reg. loss: 0.7994 0.0235 sec/batch\n",
      "Epoch 9/10 Iteration: 370000 Avg. Training loss: 3.8916 Avg. Reg. loss: 0.8079 0.0227 sec/batch\n",
      "Epoch 9/10 Iteration: 371000 Avg. Training loss: 3.9128 Avg. Reg. loss: 0.8136 0.0232 sec/batch\n",
      "Epoch 9/10 Iteration: 372000 Avg. Training loss: 3.9068 Avg. Reg. loss: 0.8135 0.0240 sec/batch\n",
      "Epoch 9/10 Iteration: 373000 Avg. Training loss: 3.9094 Avg. Reg. loss: 0.8138 0.0232 sec/batch\n",
      "Epoch 9/10 Iteration: 374000 Avg. Training loss: 3.9113 Avg. Reg. loss: 0.8060 0.0237 sec/batch\n",
      "Epoch 9/10 Iteration: 375000 Avg. Training loss: 3.8961 Avg. Reg. loss: 0.8085 0.0208 sec/batch\n",
      "Epoch 9/10 Iteration: 376000 Avg. Training loss: 3.9035 Avg. Reg. loss: 0.8128 0.0238 sec/batch\n",
      "Epoch 9/10 Iteration: 377000 Avg. Training loss: 3.9029 Avg. Reg. loss: 0.8114 0.0231 sec/batch\n",
      "Epoch 9/10 Iteration: 378000 Avg. Training loss: 3.9092 Avg. Reg. loss: 0.8059 0.0239 sec/batch\n",
      "Epoch 9/10 Iteration: 379000 Avg. Training loss: 3.8964 Avg. Reg. loss: 0.8109 0.0238 sec/batch\n",
      "Epoch 9/10 Iteration: 380000 Avg. Training loss: 3.9125 Avg. Reg. loss: 0.8118 0.0238 sec/batch\n",
      "Epoch 9/10 Iteration: 381000 Avg. Training loss: 3.8919 Avg. Reg. loss: 0.8089 0.0238 sec/batch\n",
      "Epoch 9/10 Iteration: 382000 Avg. Training loss: 3.9116 Avg. Reg. loss: 0.8184 0.0235 sec/batch\n",
      "Epoch 9/10 Iteration: 383000 Avg. Training loss: 3.8952 Avg. Reg. loss: 0.8102 0.0232 sec/batch\n",
      "Epoch 9/10 Iteration: 384000 Avg. Training loss: 3.9027 Avg. Reg. loss: 0.8121 0.0240 sec/batch\n",
      "Epoch 9/10 Iteration: 385000 Avg. Training loss: 3.9068 Avg. Reg. loss: 0.8110 0.0253 sec/batch\n",
      "Epoch 9/10 Iteration: 386000 Avg. Training loss: 3.8980 Avg. Reg. loss: 0.8124 0.0237 sec/batch\n",
      "Epoch 9/10 Iteration: 387000 Avg. Training loss: 3.9090 Avg. Reg. loss: 0.8182 0.0230 sec/batch\n",
      "Epoch 9/10 Iteration: 388000 Avg. Training loss: 3.9163 Avg. Reg. loss: 0.8162 0.0231 sec/batch\n",
      "Epoch 9/10 Iteration: 389000 Avg. Training loss: 3.9008 Avg. Reg. loss: 0.8116 0.0235 sec/batch\n",
      "Epoch 9/10 Iteration: 390000 Avg. Training loss: 3.8967 Avg. Reg. loss: 0.8191 0.0237 sec/batch\n",
      "Epoch 9/10 Iteration: 391000 Avg. Training loss: 3.8986 Avg. Reg. loss: 0.8185 0.0232 sec/batch\n",
      "Epoch 9/10 Iteration: 392000 Avg. Training loss: 3.9044 Avg. Reg. loss: 0.8165 0.0226 sec/batch\n",
      "Epoch 9/10 Iteration: 393000 Avg. Training loss: 3.8992 Avg. Reg. loss: 0.8154 0.0237 sec/batch\n",
      "Epoch 9/10 Iteration: 394000 Avg. Training loss: 3.9012 Avg. Reg. loss: 0.8192 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 395000 Avg. Training loss: 3.8951 Avg. Reg. loss: 0.8114 0.0272 sec/batch\n",
      "Epoch 9/10 Iteration: 396000 Avg. Training loss: 3.9087 Avg. Reg. loss: 0.8161 0.0242 sec/batch\n",
      "Epoch 9/10 Iteration: 397000 Avg. Training loss: 3.8965 Avg. Reg. loss: 0.8169 0.0247 sec/batch\n",
      "Epoch 9/10 Iteration: 398000 Avg. Training loss: 3.8894 Avg. Reg. loss: 0.8179 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 399000 Avg. Training loss: 3.9039 Avg. Reg. loss: 0.8207 0.0255 sec/batch\n",
      "Epoch 9/10 Iteration: 400000 Avg. Training loss: 3.9010 Avg. Reg. loss: 0.8197 0.0259 sec/batch\n",
      "Epoch 9/10 Iteration: 401000 Avg. Training loss: 3.9104 Avg. Reg. loss: 0.8213 0.0258 sec/batch\n",
      "Epoch 9/10 Iteration: 402000 Avg. Training loss: 3.9026 Avg. Reg. loss: 0.8171 0.0257 sec/batch\n",
      "Epoch 9/10 Iteration: 403000 Avg. Training loss: 3.9102 Avg. Reg. loss: 0.8184 0.0256 sec/batch\n",
      "Epoch 9/10 Iteration: 404000 Avg. Training loss: 3.9117 Avg. Reg. loss: 0.8191 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 405000 Avg. Training loss: 3.9103 Avg. Reg. loss: 0.8226 0.0258 sec/batch\n",
      "Epoch 9/10 Iteration: 406000 Avg. Training loss: 3.9030 Avg. Reg. loss: 0.8213 0.0265 sec/batch\n",
      "Epoch 9/10 Iteration: 407000 Avg. Training loss: 3.9067 Avg. Reg. loss: 0.8274 0.0261 sec/batch\n",
      "Epoch 9/10 Iteration: 408000 Avg. Training loss: 3.9035 Avg. Reg. loss: 0.8247 0.0259 sec/batch\n",
      "Epoch 9/10 Iteration: 409000 Avg. Training loss: 3.9103 Avg. Reg. loss: 0.8267 0.0262 sec/batch\n",
      "Epoch 9/10 Iteration: 410000 Avg. Training loss: 3.9098 Avg. Reg. loss: 0.8229 0.0239 sec/batch\n",
      "Epoch 9/10 Iteration: 411000 Avg. Training loss: 3.9079 Avg. Reg. loss: 0.8238 0.0262 sec/batch\n",
      "Epoch 9/10 Iteration: 412000 Avg. Training loss: 3.9006 Avg. Reg. loss: 0.8185 0.0258 sec/batch\n",
      "Epoch 9/10 Iteration: 413000 Avg. Training loss: 3.9063 Avg. Reg. loss: 0.8302 0.0273 sec/batch\n",
      "Epoch 9/10 Iteration: 414000 Avg. Training loss: 3.8989 Avg. Reg. loss: 0.8259 0.0277 sec/batch\n",
      "Epoch 10/10 Iteration: 415000 Avg. Training loss: 3.9098 Avg. Reg. loss: 0.8192 0.0250 sec/batch\n",
      "Epoch 10/10 Iteration: 416000 Avg. Training loss: 3.8915 Avg. Reg. loss: 0.8274 0.0260 sec/batch\n",
      "Epoch 10/10 Iteration: 417000 Avg. Training loss: 3.9056 Avg. Reg. loss: 0.8306 0.0248 sec/batch\n",
      "Epoch 10/10 Iteration: 418000 Avg. Training loss: 3.9080 Avg. Reg. loss: 0.8319 0.0270 sec/batch\n",
      "Epoch 10/10 Iteration: 419000 Avg. Training loss: 3.9018 Avg. Reg. loss: 0.8330 0.0253 sec/batch\n",
      "Epoch 10/10 Iteration: 420000 Avg. Training loss: 3.9124 Avg. Reg. loss: 0.8257 0.0251 sec/batch\n",
      "Epoch 10/10 Iteration: 421000 Avg. Training loss: 3.8909 Avg. Reg. loss: 0.8271 0.0251 sec/batch\n",
      "Epoch 10/10 Iteration: 422000 Avg. Training loss: 3.9002 Avg. Reg. loss: 0.8297 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 423000 Avg. Training loss: 3.9009 Avg. Reg. loss: 0.8293 0.0235 sec/batch\n",
      "Epoch 10/10 Iteration: 424000 Avg. Training loss: 3.9076 Avg. Reg. loss: 0.8241 0.0236 sec/batch\n",
      "Epoch 10/10 Iteration: 425000 Avg. Training loss: 3.8891 Avg. Reg. loss: 0.8289 0.0238 sec/batch\n",
      "Epoch 10/10 Iteration: 426000 Avg. Training loss: 3.9053 Avg. Reg. loss: 0.8304 0.0239 sec/batch\n",
      "Epoch 10/10 Iteration: 427000 Avg. Training loss: 3.8920 Avg. Reg. loss: 0.8286 0.0248 sec/batch\n",
      "Epoch 10/10 Iteration: 428000 Avg. Training loss: 3.9031 Avg. Reg. loss: 0.8376 0.0238 sec/batch\n",
      "Epoch 10/10 Iteration: 429000 Avg. Training loss: 3.8948 Avg. Reg. loss: 0.8291 0.0262 sec/batch\n",
      "Epoch 10/10 Iteration: 430000 Avg. Training loss: 3.9093 Avg. Reg. loss: 0.8320 0.0276 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Iteration: 431000 Avg. Training loss: 3.9050 Avg. Reg. loss: 0.8314 0.0243 sec/batch\n",
      "Epoch 10/10 Iteration: 432000 Avg. Training loss: 3.8975 Avg. Reg. loss: 0.8327 0.0241 sec/batch\n",
      "Epoch 10/10 Iteration: 433000 Avg. Training loss: 3.9051 Avg. Reg. loss: 0.8364 0.0250 sec/batch\n",
      "Epoch 10/10 Iteration: 434000 Avg. Training loss: 3.9076 Avg. Reg. loss: 0.8349 0.0253 sec/batch\n",
      "Epoch 10/10 Iteration: 435000 Avg. Training loss: 3.9018 Avg. Reg. loss: 0.8316 0.0251 sec/batch\n",
      "Epoch 10/10 Iteration: 436000 Avg. Training loss: 3.9010 Avg. Reg. loss: 0.8379 0.0269 sec/batch\n",
      "Epoch 10/10 Iteration: 437000 Avg. Training loss: 3.9072 Avg. Reg. loss: 0.8397 0.0294 sec/batch\n",
      "Epoch 10/10 Iteration: 438000 Avg. Training loss: 3.9119 Avg. Reg. loss: 0.8370 0.0313 sec/batch\n",
      "Epoch 10/10 Iteration: 439000 Avg. Training loss: 3.9037 Avg. Reg. loss: 0.8361 0.0321 sec/batch\n",
      "Epoch 10/10 Iteration: 440000 Avg. Training loss: 3.9001 Avg. Reg. loss: 0.8394 0.0248 sec/batch\n",
      "Epoch 10/10 Iteration: 441000 Avg. Training loss: 3.9018 Avg. Reg. loss: 0.8314 0.0253 sec/batch\n",
      "Epoch 10/10 Iteration: 442000 Avg. Training loss: 3.8968 Avg. Reg. loss: 0.8358 0.0251 sec/batch\n",
      "Epoch 10/10 Iteration: 443000 Avg. Training loss: 3.8978 Avg. Reg. loss: 0.8367 0.0239 sec/batch\n",
      "Epoch 10/10 Iteration: 444000 Avg. Training loss: 3.8909 Avg. Reg. loss: 0.8371 0.0235 sec/batch\n",
      "Epoch 10/10 Iteration: 445000 Avg. Training loss: 3.9066 Avg. Reg. loss: 0.8405 0.0240 sec/batch\n",
      "Epoch 10/10 Iteration: 446000 Avg. Training loss: 3.8990 Avg. Reg. loss: 0.8419 0.0272 sec/batch\n",
      "Epoch 10/10 Iteration: 447000 Avg. Training loss: 3.8991 Avg. Reg. loss: 0.8396 0.0264 sec/batch\n",
      "Epoch 10/10 Iteration: 448000 Avg. Training loss: 3.9002 Avg. Reg. loss: 0.8354 0.0271 sec/batch\n",
      "Epoch 10/10 Iteration: 449000 Avg. Training loss: 3.9124 Avg. Reg. loss: 0.8382 0.0269 sec/batch\n",
      "Epoch 10/10 Iteration: 450000 Avg. Training loss: 3.9056 Avg. Reg. loss: 0.8394 0.0259 sec/batch\n",
      "Epoch 10/10 Iteration: 451000 Avg. Training loss: 3.9094 Avg. Reg. loss: 0.8436 0.0226 sec/batch\n",
      "Epoch 10/10 Iteration: 452000 Avg. Training loss: 3.9050 Avg. Reg. loss: 0.8424 0.0251 sec/batch\n",
      "Epoch 10/10 Iteration: 453000 Avg. Training loss: 3.9027 Avg. Reg. loss: 0.8476 0.0242 sec/batch\n",
      "Epoch 10/10 Iteration: 454000 Avg. Training loss: 3.9013 Avg. Reg. loss: 0.8456 0.0254 sec/batch\n",
      "Epoch 10/10 Iteration: 455000 Avg. Training loss: 3.9105 Avg. Reg. loss: 0.8479 0.0249 sec/batch\n",
      "Epoch 10/10 Iteration: 456000 Avg. Training loss: 3.9171 Avg. Reg. loss: 0.8433 0.0263 sec/batch\n",
      "Epoch 10/10 Iteration: 457000 Avg. Training loss: 3.9151 Avg. Reg. loss: 0.8464 0.0250 sec/batch\n",
      "Epoch 10/10 Iteration: 458000 Avg. Training loss: 3.9118 Avg. Reg. loss: 0.8406 0.0254 sec/batch\n",
      "Epoch 10/10 Iteration: 459000 Avg. Training loss: 3.9044 Avg. Reg. loss: 0.8508 0.0250 sec/batch\n",
      "Epoch 10/10 Iteration: 460000 Avg. Training loss: 3.9044 Avg. Reg. loss: 0.8471 0.0254 sec/batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(quo_idx_pairs)):\n",
    "    current_tokens = quo_tokens_lst[i] * 1000000\n",
    "    \n",
    "    print(\"Tokens: \", current_tokens)\n",
    "    print(\"Starting training at \", datetime.datetime.now())\n",
    "    t0 = time.time()\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        iteration = 1\n",
    "        loss = 0\n",
    "        regular_loss = 0\n",
    "        loss_best = 100\n",
    "        loss_list = []\n",
    "        iteration_best = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            batches = get_batches(quo_idx_pairs[i], batch_size)\n",
    "            start = time.time()\n",
    "            for x, y in batches:\n",
    "                feed = {inputs: x,\n",
    "                        labels: np.array(y)[:, None]}\n",
    "                sess.run(update_embed_op)\n",
    "                train_loss, _, regu_loss = sess.run([total_cost, optimizer, reg_loss], feed_dict=feed)\n",
    "#                 train_loss, _ = sess.run([total_cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                loss += train_loss\n",
    "                regular_loss += regu_loss\n",
    "\n",
    "                if loss < loss_best:\n",
    "                    W = sess.run(embedding).tolist()\n",
    "                    iteration_best = iteration\n",
    "                    loss_best = loss\n",
    "\n",
    "                if iteration % 1000 == 0:\n",
    "                    end = time.time()\n",
    "                    loss_list.append(loss / 1000)\n",
    "                    print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(loss / 1000),\n",
    "                          \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                          \"{:.4f} sec/batch\".format((end - start) / 1000))\n",
    "\n",
    "\n",
    "                    loss = 0\n",
    "                    regular_loss = 0\n",
    "                    start = time.time()\n",
    "                iteration += 1\n",
    "                \n",
    "        np.save('w2v_pivotsfull_'+str(quo_idx_pairs[i])+'m.npy',np.array(W))        \n",
    "        print(\"Finish training at \", datetime.datetime.now()) \n",
    "        print(\"-------------------------------------------------------------------------\") \n",
    "        print(\"-------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.696653425693512, -0.4016351103782654, -0.15490229427814484, -0.153431236743927, 0.11273664981126785, -0.20632797479629517, -0.05852844938635826, 0.18393464386463165, 0.04037215933203697, -0.15603983402252197, -0.12679000198841095, 0.10461419820785522, -0.03136150911450386, -0.09917640686035156, -0.21953696012496948, -0.06557910144329071, -0.3572455048561096, -0.07304935902357101, 0.2829059362411499, 0.25940605998039246, 0.18046262860298157, -0.18454191088676453, -0.13335512578487396, -0.11446908116340637, -0.09217895567417145, -0.028645846992731094, 0.07994083315134048, -0.3566879630088806, -0.16788771748542786, -0.09856567531824112, -0.05210083723068237, -0.06661748886108398, 0.09986916929483414, 0.1596103459596634, -0.1205173209309578, -0.03440592437982559, 0.028155574575066566, -0.17301133275032043, -0.17946180701255798, -0.0042143226601183414, -0.18912769854068756, -0.17107552289962769, -0.14589069783687592, -0.08563197404146194, -0.043947286903858185, -0.053388938307762146, 0.11201006174087524, -0.2840996980667114, -0.21458014845848083, 0.03424297273159027]\n",
      "[-0.6714518666267395, -0.397854745388031, -0.22400067746639252, -0.11230164021253586, 0.03792639076709747, -0.23540274798870087, 0.010916607454419136, 0.14502541720867157, 0.14910753071308136, -0.1241375207901001, -0.06717413663864136, 0.10475543141365051, -0.06811563670635223, -0.17020389437675476, -0.24623776972293854, 0.026321668177843094, -0.3198947608470917, 0.016895517706871033, 0.12025859951972961, 0.23876900970935822, 0.273358553647995, -0.2074965089559555, -0.03477171063423157, -0.013472441583871841, -0.038002073764801025, -0.07621145248413086, 0.027983732521533966, -0.24950556457042694, -0.16780859231948853, -0.04019904509186745, -0.16331572830677032, 0.00899406336247921, 0.12188966572284698, 0.043372541666030884, -0.14826881885528564, -0.08657801151275635, 0.01910969987511635, -0.12576808035373688, -0.17746634781360626, -0.008646461181342602, -0.1491350531578064, -0.09115835279226303, -0.1268480122089386, -0.04936075955629349, 0.02550850622355938, -0.226204052567482, 0.061961252242326736, -0.2975235879421234, -0.18389356136322021, 0.07822413742542267]\n"
     ]
    }
   ],
   "source": [
    "print(pivots_dict[1])\n",
    "print(W[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
