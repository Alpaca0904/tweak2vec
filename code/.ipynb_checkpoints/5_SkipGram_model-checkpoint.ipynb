{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size, stop_size):\n",
    "    idx_pairs = []\n",
    "    tokens = 0\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            tokens += 1\n",
    "            if tokens >= stop_size:\n",
    "                return idx_pairs, tokens\n",
    "            else:\n",
    "                # for each context word within window\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = sentence[context_word_pos]\n",
    "                    idx_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "                    \n",
    "    return idx_pairs, tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 46007576 word pairs\n",
      "totally 6975371 tokens\n"
     ]
    }
   ],
   "source": [
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "#corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/pubmed_corpus_int5.npy\").tolist()\n",
    "\n",
    "corpus_shuffle = corpus[:]\n",
    "\n",
    "random.shuffle(corpus_shuffle)\n",
    "idx_pairs_SG_7m, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = 7000000)\n",
    "print('totally {0} word pairs'.format(len(idx_pairs_SG_7m)))\n",
    "print('totally {0} tokens'.format(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 46007576 word pairs\n",
      "totally 6975371 tokens\n",
      "totally 39582075 word pairs\n",
      "totally 6000000 tokens\n",
      "totally 32972469 word pairs\n",
      "totally 5000000 tokens\n",
      "totally 26378696 word pairs\n",
      "totally 4000000 tokens\n",
      "totally 19784785 word pairs\n",
      "totally 3000000 tokens\n",
      "totally 13201468 word pairs\n",
      "totally 2000000 tokens\n",
      "totally 6596705 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 3292688 word pairs\n",
      "totally 500000 tokens\n",
      "totally 661574 word pairs\n",
      "totally 100000 tokens\n",
      "totally 328753 word pairs\n",
      "totally 50000 tokens\n",
      "totally 66708 word pairs\n",
      "totally 10000 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [7,6,5,4,3,2,1,0.5,0.1,0.05,0.01]\n",
    "idx_pairs = []\n",
    "for i in tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "wordlist.append(['UNK',0])\n",
    "\n",
    "#wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_vocab5.npy').tolist()\n",
    "#wordlist.append('UNK')\n",
    "\n",
    "word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w[0] for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 10000 pivot words\n"
     ]
    }
   ],
   "source": [
    "f = open('/Users/zhang/MscProject_tweak2vec/corpus/quora_pivots_google_10000.txt','r')\n",
    "#f = open('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_pivots_google_5000.txt','r')\n",
    "a = f.read()\n",
    "pivots_dict = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words'.format(len(pivots_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "def get_pivots_slice(pivots_dict, size):\n",
    "    pivots = deepcopy(pivots_dict)\n",
    "    pivots_slice = dict_slice(pivots, 0, size)\n",
    "    pivots_idx = []\n",
    "    pivots_vec = []\n",
    "    for i in pivots_slice.keys():\n",
    "        pivots_idx.append(i)\n",
    "        pivots_vec.append(pivots_slice[i])\n",
    "    return pivots_idx, pivots_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pivots = 500\n",
    "pivots_idx, pivots_vec = get_pivots_slice(pivots_dict, n_pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_pretrain = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/w2v_google_50d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 0.0001\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1000 # number of samples each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    # embedding layer\n",
    "    init_width = 0.5 / n_embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "#     embedding = tf.Variable(google_pretrain)\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # add regularization term\n",
    "    embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "    update_embed_op = tf.scatter_update(embedding_copy,pivots_idx,pivots_vec)\n",
    "    embed_copy = tf.nn.embedding_lookup(embedding_copy, inputs)\n",
    "    reg_loss = reg_constant * tf.reduce_sum((embed-embed_copy)**2)\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "#     total_cost = cost \n",
    "    total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_shuffle = idx_pairs_SG[:]\n",
    "random.shuffle(idx_pairs_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_10m = idx_pairs_shuffle[:10000000]\n",
    "idx_pairs_5m = idx_pairs_shuffle[:5000000]\n",
    "idx_pairs_1m = idx_pairs_shuffle[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(idx_pairs)):\n",
    "    current_tokens = tokens_lst[i] * 1000000\n",
    "    \n",
    "    print(\"Tokens: \", current_tokens)\n",
    "    print(\"Starting training at \", datetime.datetime.now())\n",
    "    t0 = time.time()\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        iteration = 1\n",
    "        loss = 0\n",
    "        regular_loss = 0\n",
    "        loss_best = 100\n",
    "        loss_list = []\n",
    "        iteration_best = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            batches = get_batches(idx_pairs[i], batch_size)\n",
    "            start = time.time()\n",
    "            for x, y in batches:\n",
    "                feed = {inputs: x,\n",
    "                        labels: np.array(y)[:, None]}\n",
    "                sess.run(update_embed_op)\n",
    "                train_loss, _, regu_loss = sess.run([total_cost, optimizer, reg_loss], feed_dict=feed)\n",
    "#                 train_loss, _ = sess.run([total_cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                loss += train_loss\n",
    "                regular_loss += regu_loss\n",
    "\n",
    "                if loss < loss_best:\n",
    "                    W = sess.run(embedding).tolist()\n",
    "                    iteration_best = iteration\n",
    "                    loss_best = loss\n",
    "\n",
    "                if iteration % 1000 == 0:\n",
    "                    end = time.time()\n",
    "                    loss_list.append(loss / 1000)\n",
    "                    print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(loss / 1000),\n",
    "                          \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                          \"{:.4f} sec/batch\".format((end - start) / 1000))\n",
    "\n",
    "\n",
    "                    loss = 0\n",
    "                    regular_loss = 0\n",
    "                    start = time.time()\n",
    "                iteration += 1\n",
    "                \n",
    "        np.save('w2v_pivots500_'+str(tokens_lst[i])+'m.npy',np.array(W))        \n",
    "        print(\"Finish training at \", datetime.datetime.now()) \n",
    "        print(\"-------------------------------------------------------------------------\") \n",
    "        print(\"-------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.696653425693512, -0.4016351103782654, -0.15490229427814484, -0.153431236743927, 0.11273664981126785, -0.20632797479629517, -0.05852844938635826, 0.18393464386463165, 0.04037215933203697, -0.15603983402252197, -0.12679000198841095, 0.10461419820785522, -0.03136150911450386, -0.09917640686035156, -0.21953696012496948, -0.06557910144329071, -0.3572455048561096, -0.07304935902357101, 0.2829059362411499, 0.25940605998039246, 0.18046262860298157, -0.18454191088676453, -0.13335512578487396, -0.11446908116340637, -0.09217895567417145, -0.028645846992731094, 0.07994083315134048, -0.3566879630088806, -0.16788771748542786, -0.09856567531824112, -0.05210083723068237, -0.06661748886108398, 0.09986916929483414, 0.1596103459596634, -0.1205173209309578, -0.03440592437982559, 0.028155574575066566, -0.17301133275032043, -0.17946180701255798, -0.0042143226601183414, -0.18912769854068756, -0.17107552289962769, -0.14589069783687592, -0.08563197404146194, -0.043947286903858185, -0.053388938307762146, 0.11201006174087524, -0.2840996980667114, -0.21458014845848083, 0.03424297273159027]\n",
      "[0.008135233074426651, 0.009025746025145054, -0.001867753337137401, 0.003911293111741543, 0.003032144159078598, 0.0047667911276221275, -0.00896137859672308, 0.001198031473904848, 0.0018118303269147873, -0.006547606084495783, -0.0038154898211359978, 0.0026109935715794563, 0.008958626538515091, 0.0037896926514804363, 0.006002503912895918, -0.003691143821924925, 0.0013169076992198825, 0.0020849949214607477, -0.0011018638033419847, -0.0019998643547296524, -0.0029249703511595726, 0.010995338670909405, -0.007096978835761547, -0.0011141536524519324, 0.005530660971999168, -0.0038179196417331696, 0.008521145209670067, -0.001958186738193035, 0.0018000431591644883, -0.00149300298653543, 0.008803918026387691, -0.0021494259126484394, -0.004088232293725014, -0.00018706242553889751, 0.0071846432983875275, -0.004491210449486971, 0.00123301288112998, -0.008554494008421898, -0.002175036119297147, -0.008669860661029816, -0.006715609226375818, 0.0013174714986234903, -0.008055897429585457, -0.006076232064515352, -0.00031157350167632103, -0.008276816457509995, 0.0038090732414275408, -0.005568799562752247, 0.007867271080613136, 0.0007471530698239803]\n"
     ]
    }
   ],
   "source": [
    "print(pivots_dict[1])\n",
    "print(W[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
