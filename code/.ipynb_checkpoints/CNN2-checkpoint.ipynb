{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "x_data = np.load(\"data/doc2v_x_data.npy\")\n",
    "y_data = np.load(\"data/doc2v_y_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_data,y_data,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/miniconda3/envs/project/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "import pandas as pd\n",
    "data_df = pd.read_csv(\"test_data/reports_labels.csv\", delimiter='\\t')\n",
    "inputs_int=np.load(\"data/cleaned_data_int_padding.npy\")\n",
    "data_y = data_df.drop(['text','Energy Generation and Supply'], axis=1).as_matrix()\n",
    "data_x = inputs_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data_x,data_y,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    Args:\n",
    "        input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: {0}\".format(str(shape)))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: {0}\".format(str(shape)))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        W = tf.get_variable(\"W\", [output_size, input_size], dtype=input_.dtype)\n",
    "        b = tf.get_variable(\"b\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.nn.xw_plus_b(input_, tf.transpose(W), b)\n",
    "\n",
    "\n",
    "# def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "#     \"\"\"\n",
    "#     Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "#     t = sigmoid(Wy + b)\n",
    "#     z = t * g(Wy + b) + (1 - t) * y\n",
    "#     where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "#     \"\"\"\n",
    "\n",
    "#     with tf.variable_scope(scope):\n",
    "#         for idx in range(num_layers):\n",
    "#             g = f(linear(input_, size, scope=('highway_lin_{0}'.format(idx))))\n",
    "#             t = tf.sigmoid(linear(input_, size, scope=('highway_gate_{0}'.format(idx))) + bias)\n",
    "#             output = t * g + (1. - t) * input_\n",
    "#             input_ = output\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"A CNN for text classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes,  fc_hidden_size, embedding_size, vocab_size,\n",
    "            embedding_type, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        #pretrained_embedding=None):\n",
    "        \n",
    "        \n",
    "\n",
    "        # Placeholders for input, output, dropout_prob and training_tag\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        self.embedding = tf.placeholder(tf.float32, [vocab_size, embedding_dim], name='embedding')\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "\n",
    "        # Embedding Layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # Use random generated the word vector by default\n",
    "            # Can also be obtained through our own word vectors trained by our corpus\n",
    "#             if pretrained_embedding is None:\n",
    "#                 self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0,\n",
    "#                                                                dtype=tf.float32), trainable=True, name=\"embedding\")\n",
    "#             else:\n",
    "#                 if embedding_type == 0:\n",
    "#                     self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=\"embedding\")\n",
    "#                 if embedding_type == 1:\n",
    "#                     self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n",
    "#                                                  dtype=tf.float32, name=\"embedding\")\n",
    "            \n",
    "            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n",
    "            self.embedded_sentence_expanded = tf.expand_dims(self.embedded_sentence, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for filter_size in filter_sizes:\n",
    "            with tf.name_scope(\"conv-filter{0}\".format(filter_size)):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters], dtype=tf.float32), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_sentence_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                conv = tf.nn.bias_add(conv, b)\n",
    "\n",
    "                # Batch Normalization Layer\n",
    "                conv_bn = tf.layers.batch_normalization(conv, training=self.is_training)\n",
    "\n",
    "                # Apply nonlinearity\n",
    "                conv_out = tf.nn.relu(conv_bn, name=\"relu\")\n",
    "\n",
    "            with tf.name_scope(\"pool-filter{0}\".format(filter_size)):\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    conv_out,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"pool\")\n",
    "\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.pool = tf.concat(pooled_outputs, 3)\n",
    "        self.pool_flat = tf.reshape(self.pool, [-1, num_filters_total])\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        with tf.name_scope(\"fc\"):\n",
    "            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total, fc_hidden_size],\n",
    "                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[fc_hidden_size], dtype=tf.float32), name=\"b\")\n",
    "            self.fc = tf.nn.xw_plus_b(self.pool_flat, W, b)\n",
    "\n",
    "            # Batch Normalization Layer\n",
    "            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n",
    "\n",
    "            # Apply nonlinearity\n",
    "            self.fc_out = tf.nn.relu(self.fc_bn, name=\"relu\")\n",
    "\n",
    "        # Highway Layer\n",
    "        #self.highway = highway(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0, scope=\"Highway\")\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.fc_out, self.dropout_keep_prob)\n",
    "\n",
    "        # Final scores\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n",
    "                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes], dtype=tf.float32), name=\"b\")\n",
    "            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"logits\")\n",
    "            self.scores = tf.sigmoid(self.logits, name=\"scores\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss, L2 loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=\"sigmoid_losses\")\n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n",
    "                                 name=\"l2_losses\") * l2_reg_lambda\n",
    "            self.loss = tf.add(losses, l2_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_using_scores_by_threshold(scores, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Get the predicted labels based on the threshold.\n",
    "    If there is no predict value greater than threshold, then choose the label which has the max predict value.\n",
    "    Args:\n",
    "        scores: The all classes predicted scores provided by network\n",
    "        threshold: The threshold (default: 0.5)\n",
    "    Returns:\n",
    "        predicted_labels: The predicted labels\n",
    "        predicted_values: The predicted values\n",
    "    \"\"\"\n",
    "    predicted_labels = []\n",
    "    predicted_values = []\n",
    "    scores = np.ndarray.tolist(scores)\n",
    "    for score in scores:\n",
    "        count = 0\n",
    "        index_list = []\n",
    "        value_list = []\n",
    "        for index, predict_value in enumerate(score):\n",
    "            if predict_value > threshold:\n",
    "                index_list.append(index)\n",
    "                value_list.append(predict_value)\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            index_list.append(score.index(max(score)))\n",
    "            value_list.append(max(score))\n",
    "        predicted_labels.append(index_list)\n",
    "        predicted_values.append(value_list)\n",
    "    return predicted_labels, predicted_values\n",
    "\n",
    "def cal_metric(predicted_labels, labels):\n",
    "    \"\"\"\n",
    "    Calculate the metric(recall, accuracy).\n",
    "    Args:\n",
    "        predicted_labels: The predicted_labels\n",
    "        labels: The true labels\n",
    "    Returns:\n",
    "        The value of metric\n",
    "    \"\"\"\n",
    "    label_no_zero = []\n",
    "    for index, label in enumerate(labels):\n",
    "        if int(label) == 1:\n",
    "            label_no_zero.append(index)\n",
    "    count = 0\n",
    "    for predicted_label in predicted_labels:\n",
    "        if int(predicted_label) in label_no_zero:\n",
    "            count += 1\n",
    "    recall = count / len(label_no_zero)\n",
    "    precision = count / len(predicted_labels)\n",
    "    return recall, precision\n",
    "\n",
    "def cal_F(recall, precision):\n",
    "    \"\"\"\n",
    "    Calculate the metric F value.\n",
    "    Args:\n",
    "        recall: The recall value\n",
    "        precision: The precision value\n",
    "    Returns:\n",
    "        The F value\n",
    "    \"\"\"\n",
    "    F = 0.0\n",
    "    if (recall + precision) == 0:\n",
    "        F = 0.0\n",
    "    else:\n",
    "        F = (2 * recall * precision) / (recall + precision)\n",
    "    return F\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    含有 yield 说明不是一个普通函数，是一个 Generator.\n",
    "    函数效果：对 data，一共分成 num_epochs 个阶段（epoch），在每个 epoch 内，如果 shuffle=True，就将 data 重新洗牌，\n",
    "    批量生成 (yield) 一批一批的重洗过的 data，每批大小是 batch_size，一共生成 int(len(data)/batch_size)+1 批。\n",
    "    Args:\n",
    "        data: The data\n",
    "        batch_size: The size of the data batch\n",
    "        num_epochs: The number of epochs\n",
    "        shuffle: Shuffle or not (default: True)\n",
    "    Returns:\n",
    "        A batch iterator for data set\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = x_train, y_train\n",
    "x_validation, y_validation = x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_word2vec_matrix =np.load(\"data/wordVectors_300.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "learning_rate=0.001 #\"The learning rate (default: 0.001)\")\n",
    "pad_seq_len=5500 #Recommended padding Sequence length of data (depends on the data)\")\n",
    "embedding_dim=300# \"Dimensionality of character embedding (default: 128)\")\n",
    "embedding_type=1 #, \"The embedding type (default: 1)\")\n",
    "fc_hidden_size=1024 #, \"Hidden size for fully connected layer (default: 1024)\")\n",
    "filter_sizes=\"3,4,5\" #, \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "num_filters=128 #, \"Number of filters per filter size (default: 128)\")\n",
    "dropout_keep_prob =0.5#, \"Dropout keep probability (default: 0.5)\")\n",
    "l2_reg_lambda=0.0#, \"L2 regularization lambda (default: 0.0)\")\n",
    "num_classes=18 #, \"Number of labels (depends on the task)\")\n",
    "pretrained_word2vec_matrix =np.load(\"data/wordVectors_300.npy\")\n",
    "threshold=0.5 #, \"Threshold for prediction classes (default: 0.5)\")\n",
    "\n",
    "# Training Parameters\n",
    "batch_size =1024 #, \"Batch Size (default: 256)\")\n",
    "num_epochs = 10 #, \"Number of training epochs (default: 100)\")\n",
    "evaluate_every = 5000 #, \"Evaluate model on dev set after this many steps (default: 5000)\")\n",
    "norm_ratio=2 #, \"The ratio of the sum of gradients norms of trainable variable (default: 1.25)\")\n",
    "decay_steps= 5000 #, \"how many steps before decay learning rate. (default: 500)\")\n",
    "decay_rate=0.95 #, \"Rate of decay for learning rate. (default: 0.95)\")\n",
    "#tf.flags.DEFINE_integer(\"checkpoint_every\", 1000, \"Save model after this many steps (default: 1000)\")\n",
    "#tf.flags.DEFINE_integer(\"num_checkpoints\", 50, \"Number of checkpoints to store (default: 50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver(tf.global_variables())\n",
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "with tf.Graph().as_default():\n",
    "    #sess = tf.Session()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length= pad_seq_len,\n",
    "            num_classes= num_classes,\n",
    "            vocab_size = len(pretrained_word2vec_matrix),\n",
    "            fc_hidden_size=fc_hidden_size,\n",
    "            embedding_size=embedding_dim,\n",
    "            embedding_type=embedding_type,\n",
    "\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "            #pretrained_embedding=pretrained_word2vec_matrix)\n",
    "        \n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads, vars_ = zip(*optimizer.compute_gradients(cnn.loss))\n",
    "        train_op = optimizer.apply_gradients(zip(grads, vars_), global_step=cnn.global_step, name=\"train_op\")\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "         # Summaries for loss\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "\n",
    "        # Train summaries\n",
    "        out_dir = \"tensorboard/cnn2_300/\"\n",
    "        train_summary_op = tf.summary.merge([loss_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summaries\n",
    "        validation_summary_op = tf.summary.merge([loss_summary])\n",
    "        validation_summary_dir = os.path.join(out_dir, \"summaries\", \"validation\")\n",
    "        validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n",
    "\n",
    "        current_step = sess.run(cnn.global_step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches_train = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "\n",
    "        num_batches_per_epoch = int((len(x_train) - 1) / batch_size) + 1\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        for batch_train in batches_train:\n",
    "                x_batch_train, y_batch_train = zip(*batch_train)\n",
    "\n",
    "                # train_step\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch_train,\n",
    "                    cnn.input_y: y_batch_train,\n",
    "                    cnn.dropout_keep_prob: dropout_keep_prob,\n",
    "                    cnn.is_training: True,\n",
    "                    cnn.embedding: pretrained_word2vec_matrix\n",
    "                }\n",
    "                _, step, summaries, loss = sess.run(\n",
    "                    [train_op, cnn.global_step, train_summary_op, cnn.loss], feed_dict)\n",
    "                logger.info(\"step {0}: loss {1:g}\".format(step, loss))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "                current_step = tf.train.global_step(sess, cnn.global_step)\n",
    "\n",
    "                if current_step % evaluate_every == 0:\n",
    "                    logger.info(\"\\nEvaluation:\")\n",
    "        #                     eval_loss, eval_rec_ts, eval_pre_ts, eval_F_ts = \\\n",
    "        #                         validation_step(x_validation, y_validation, writer=validation_summary_writer)\n",
    "                    # validation_step\n",
    "                    batches_validation = batch_iter(\n",
    "                        list(zip(x_validation, y_validation)), batch_size, 1)\n",
    "\n",
    "                    # Predict classes by threshold or topk ('ts': threshold; 'tk': topk)\n",
    "                    eval_counter, eval_loss, eval_rec_ts, eval_pre_ts, eval_F_ts = 0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "                    for batch_validation in batches_validation:\n",
    "                        x_batch_validation, y_batch_validation = zip(*batch_validation)\n",
    "                        feed_dict = {\n",
    "                            cnn.input_x: x_batch_validation,\n",
    "                            cnn.input_y: y_batch_validation,\n",
    "                            cnn.dropout_keep_prob: 1.0,\n",
    "                            cnn.is_training: False,\n",
    "                            cnn.embedding: pretrained_word2vec_matrix\n",
    "                        }\n",
    "                        step, summaries, scores, cur_loss = sess.run(\n",
    "                            [cnn.global_step, validation_summary_op, cnn.scores, cnn.loss], feed_dict)\n",
    "\n",
    "                        # Predict by threshold\n",
    "                        predicted_labels_threshold, predicted_values_threshold = \\\n",
    "                            get_label_using_scores_by_threshold(scores=scores, threshold=FLAGS.threshold)\n",
    "\n",
    "                        cur_rec_ts, cur_pre_ts, cur_F_ts = 0.0, 0.0, 0.0\n",
    "\n",
    "                        for index, predicted_label_threshold in enumerate(predicted_labels_threshold):\n",
    "                            rec_inc_ts, pre_inc_ts = cal_metric(predicted_label_threshold, y_batch_validation[index])\n",
    "                            cur_rec_ts, cur_pre_ts = cur_rec_ts + rec_inc_ts, cur_pre_ts + pre_inc_ts\n",
    "\n",
    "                        cur_rec_ts = cur_rec_ts / len(y_batch_validation)\n",
    "                        cur_pre_ts = cur_pre_ts / len(y_batch_validation)\n",
    "\n",
    "                        cur_F_ts = cal_F(cur_rec_ts, cur_pre_ts)\n",
    "\n",
    "                        eval_rec_ts, eval_pre_ts = eval_rec_ts + cur_rec_ts, eval_pre_ts + cur_pre_ts\n",
    "\n",
    "\n",
    "                        eval_loss = eval_loss + cur_loss\n",
    "                        eval_counter = eval_counter + 1\n",
    "\n",
    "                        logger.info(\"✔︎ validation batch {0}: loss {1:g}\".format(eval_counter, cur_loss))\n",
    "                        logger.info(\"︎☛ Predict by threshold: recall {0:g}, precision {1:g}, F {2:g}\"\n",
    "                                    .format(cur_rec_ts, cur_pre_ts, cur_F_ts))\n",
    "\n",
    "\n",
    "                        if writer:\n",
    "                            writer.add_summary(summaries, step)\n",
    "\n",
    "                    eval_loss = float(eval_loss / eval_counter)\n",
    "                    eval_rec_ts = float(eval_rec_ts / eval_counter)\n",
    "                    eval_pre_ts = float(eval_pre_ts / eval_counter)\n",
    "                    eval_F_ts = cal_F(eval_rec_ts, eval_pre_ts)\n",
    "\n",
    "                    logger.info(\"All Validation set: Loss {0:g}\".format(eval_loss))\n",
    "\n",
    "                    # Predict by threshold\n",
    "                    logger.info(\"︎☛ Predict by threshold: Recall {0:g}, Precision {1:g}, F {2:g}\"\n",
    "                                .format(eval_rec_ts, eval_pre_ts, eval_F_ts))\n",
    "\n",
    "\n",
    "                if current_step % num_batches_per_epoch == 0:\n",
    "                    current_epoch = current_step // num_batches_per_epoch\n",
    "                    logger.info(\"✔︎ Epoch {0} has finished!\".format(current_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
