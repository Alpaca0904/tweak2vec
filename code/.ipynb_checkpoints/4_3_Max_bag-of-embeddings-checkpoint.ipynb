{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime, time, json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\")\n",
    "labels = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_labels.npy\")\n",
    "\n",
    "w2v_google_50d = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/w2v_google_50d.npy\")\n",
    "w2v_quora_50d = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/w2v_quora5_50d.npy\")\n",
    "w2v_pivots100_50d = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/w2v_pivots100_50d.npy\")\n",
    "\n",
    "# w2v_retrain = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/w2v_retrain5.npy\")\n",
    "\n",
    "\n",
    "#w2v_concat = concat_vec = np.concatenate([w2v_google, w2v_quora], axis=1)\n",
    "#w2v_avg = (w2v_google + w2v_quora)/2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding = w2v_pivots100_50d[:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "w2v_concat = pca.fit_transform(w2v_concat)\n",
    "w2v_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate question1 and question2\n",
    "question1 = []\n",
    "question2 = []\n",
    "for n in range(int(len(quora_corpus)/2)):\n",
    "    question1.append(quora_corpus[2*n])\n",
    "    question2.append(quora_corpus[2*n+1])\n",
    "    \n",
    "q1_data = pad_sequences(question1, maxlen=25)\n",
    "q2_data = pad_sequences(question2, maxlen=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length:190, average length:8.62675648734686\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "l_avg = []\n",
    "for i in range(len(quora_corpus)):\n",
    "    if len(quora_corpus[i]) > l:\n",
    "        l = len(quora_corpus[i])\n",
    "    l_avg.append(len(quora_corpus[i]))\n",
    "print('max length:{0}, average length:{1}'.format(l,np.mean(np.array(l_avg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter setup\n",
    "max_sentence_len = 25\n",
    "embed_dim = 50\n",
    "dropout_rate = 0.1\n",
    "vocab_size = len(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split cross validation set and test set\n",
    "questions = np.stack((q1_data, q2_data), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(questions, labels, test_size=0.1, random_state=2018)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = Input(shape=(max_sentence_len,))\n",
    "question2 = Input(shape=(max_sentence_len,))\n",
    "\n",
    "q1 = Embedding(  input_dim=vocab_size, \n",
    "                 output_dim=embed_dim, \n",
    "                 weights=[word_embedding], \n",
    "                 input_length=max_sentence_len, \n",
    "                 trainable=False)(question1)\n",
    "q1 = TimeDistributed(Dense(embed_dim, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embed_dim, ))(q1)\n",
    "\n",
    "q2 = Embedding(  input_dim=vocab_size, \n",
    "                 output_dim=embed_dim, \n",
    "                 weights=[word_embedding], \n",
    "                 input_length=max_sentence_len, \n",
    "                 trainable=False)(question2)\n",
    "q2 = TimeDistributed(Dense(embed_dim, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embed_dim, ))(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(dropout_rate)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(dropout_rate)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(dropout_rate)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(dropout_rate)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 25, 50)       1515000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 25, 50)       1515000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 25, 50)       2550        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 25, 50)       2550        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 50)           0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 50)           0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100)          0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          20200       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 200)          40200       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 200)          40200       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200)          800         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 200)          40200       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200)          800         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            201         batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,179,301\n",
      "Trainable params: 147,701\n",
      "Non-trainable params: 3,031,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "val_split = 0.1\n",
    "batch_size = 32\n",
    "MODEL_WEIGHTS_FILE = 'SN_weights/pivots100_50d_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2018-07-01 19:03:53.089696\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      " - 56s - loss: 0.5395 - acc: 0.7268 - val_loss: 0.4892 - val_acc: 0.7552\n",
      "Epoch 2/50\n",
      " - 59s - loss: 0.4947 - acc: 0.7538 - val_loss: 0.4647 - val_acc: 0.7724\n",
      "Epoch 3/50\n",
      " - 64s - loss: 0.4782 - acc: 0.7632 - val_loss: 0.4580 - val_acc: 0.7762\n",
      "Epoch 4/50\n",
      " - 61s - loss: 0.4682 - acc: 0.7704 - val_loss: 0.4500 - val_acc: 0.7834\n",
      "Epoch 5/50\n",
      " - 61s - loss: 0.4621 - acc: 0.7733 - val_loss: 0.4457 - val_acc: 0.7822\n",
      "Epoch 6/50\n",
      " - 64s - loss: 0.4556 - acc: 0.7770 - val_loss: 0.4450 - val_acc: 0.7855\n",
      "Epoch 7/50\n",
      " - 64s - loss: 0.4504 - acc: 0.7807 - val_loss: 0.4427 - val_acc: 0.7855\n",
      "Epoch 8/50\n",
      " - 65s - loss: 0.4472 - acc: 0.7839 - val_loss: 0.4348 - val_acc: 0.7903\n",
      "Epoch 9/50\n",
      " - 61s - loss: 0.4434 - acc: 0.7853 - val_loss: 0.4382 - val_acc: 0.7891\n",
      "Epoch 10/50\n",
      " - 63s - loss: 0.4409 - acc: 0.7867 - val_loss: 0.4333 - val_acc: 0.7905\n",
      "Epoch 11/50\n",
      " - 66s - loss: 0.4380 - acc: 0.7888 - val_loss: 0.4294 - val_acc: 0.7930\n",
      "Epoch 12/50\n",
      " - 65s - loss: 0.4357 - acc: 0.7900 - val_loss: 0.4290 - val_acc: 0.7939\n",
      "Epoch 13/50\n",
      " - 63s - loss: 0.4334 - acc: 0.7918 - val_loss: 0.4274 - val_acc: 0.7945\n",
      "Epoch 14/50\n",
      " - 62s - loss: 0.4311 - acc: 0.7935 - val_loss: 0.4247 - val_acc: 0.7987\n",
      "Epoch 15/50\n",
      " - 65s - loss: 0.4301 - acc: 0.7945 - val_loss: 0.4271 - val_acc: 0.7969\n",
      "Epoch 16/50\n",
      " - 67s - loss: 0.4286 - acc: 0.7952 - val_loss: 0.4208 - val_acc: 0.7985\n",
      "Epoch 17/50\n",
      " - 66s - loss: 0.4259 - acc: 0.7965 - val_loss: 0.4268 - val_acc: 0.7977\n",
      "Epoch 18/50\n",
      " - 61s - loss: 0.4244 - acc: 0.7973 - val_loss: 0.4225 - val_acc: 0.7979\n",
      "Epoch 19/50\n",
      " - 61s - loss: 0.4241 - acc: 0.7981 - val_loss: 0.4213 - val_acc: 0.7990\n",
      "Epoch 20/50\n",
      " - 63s - loss: 0.4224 - acc: 0.7989 - val_loss: 0.4242 - val_acc: 0.7991\n",
      "Epoch 21/50\n",
      " - 66s - loss: 0.4208 - acc: 0.7998 - val_loss: 0.4217 - val_acc: 0.7977\n",
      "Epoch 22/50\n",
      " - 63s - loss: 0.4213 - acc: 0.7996 - val_loss: 0.4253 - val_acc: 0.7954\n",
      "Epoch 23/50\n",
      " - 62s - loss: 0.4188 - acc: 0.8006 - val_loss: 0.4187 - val_acc: 0.7987\n",
      "Epoch 24/50\n",
      " - 63s - loss: 0.4189 - acc: 0.8012 - val_loss: 0.4210 - val_acc: 0.7977\n",
      "Epoch 25/50\n",
      " - 64s - loss: 0.4179 - acc: 0.8011 - val_loss: 0.4205 - val_acc: 0.8017\n",
      "Epoch 26/50\n",
      " - 65s - loss: 0.4164 - acc: 0.8022 - val_loss: 0.4196 - val_acc: 0.8029\n",
      "Epoch 27/50\n",
      " - 64s - loss: 0.4156 - acc: 0.8028 - val_loss: 0.4204 - val_acc: 0.8001\n",
      "Epoch 28/50\n",
      " - 68s - loss: 0.4145 - acc: 0.8031 - val_loss: 0.4201 - val_acc: 0.8026\n",
      "Epoch 29/50\n",
      " - 64s - loss: 0.4151 - acc: 0.8022 - val_loss: 0.4159 - val_acc: 0.8022\n",
      "Epoch 30/50\n",
      " - 70s - loss: 0.4139 - acc: 0.8040 - val_loss: 0.4171 - val_acc: 0.8012\n",
      "Epoch 31/50\n",
      " - 66s - loss: 0.4125 - acc: 0.8043 - val_loss: 0.4166 - val_acc: 0.8023\n",
      "Epoch 32/50\n",
      " - 61s - loss: 0.4129 - acc: 0.8051 - val_loss: 0.4204 - val_acc: 0.8004\n",
      "Epoch 33/50\n",
      " - 58s - loss: 0.4113 - acc: 0.8046 - val_loss: 0.4180 - val_acc: 0.8015\n",
      "Epoch 34/50\n",
      " - 58s - loss: 0.4118 - acc: 0.8052 - val_loss: 0.4204 - val_acc: 0.8001\n",
      "Epoch 35/50\n",
      " - 65s - loss: 0.4109 - acc: 0.8049 - val_loss: 0.4185 - val_acc: 0.7991\n",
      "Epoch 36/50\n",
      " - 62s - loss: 0.4093 - acc: 0.8066 - val_loss: 0.4206 - val_acc: 0.8026\n",
      "Epoch 37/50\n",
      " - 59s - loss: 0.4100 - acc: 0.8063 - val_loss: 0.4160 - val_acc: 0.8029\n",
      "Epoch 38/50\n",
      " - 60s - loss: 0.4092 - acc: 0.8064 - val_loss: 0.4168 - val_acc: 0.8032\n",
      "Epoch 39/50\n",
      " - 61s - loss: 0.4082 - acc: 0.8070 - val_loss: 0.4158 - val_acc: 0.8034\n",
      "Epoch 40/50\n",
      " - 59s - loss: 0.4085 - acc: 0.8062 - val_loss: 0.4142 - val_acc: 0.8036\n",
      "Epoch 41/50\n",
      " - 58s - loss: 0.4068 - acc: 0.8080 - val_loss: 0.4173 - val_acc: 0.8017\n",
      "Epoch 42/50\n",
      " - 58s - loss: 0.4059 - acc: 0.8082 - val_loss: 0.4170 - val_acc: 0.8033\n",
      "Epoch 43/50\n",
      " - 58s - loss: 0.4061 - acc: 0.8084 - val_loss: 0.4184 - val_acc: 0.8000\n",
      "Epoch 44/50\n",
      " - 58s - loss: 0.4069 - acc: 0.8071 - val_loss: 0.4151 - val_acc: 0.8023\n",
      "Epoch 45/50\n",
      " - 59s - loss: 0.4055 - acc: 0.8085 - val_loss: 0.4166 - val_acc: 0.8031\n",
      "Epoch 46/50\n",
      " - 58s - loss: 0.4054 - acc: 0.8082 - val_loss: 0.4188 - val_acc: 0.8021\n",
      "Epoch 47/50\n",
      " - 58s - loss: 0.4048 - acc: 0.8087 - val_loss: 0.4156 - val_acc: 0.8041\n",
      "Epoch 48/50\n",
      " - 58s - loss: 0.4028 - acc: 0.8097 - val_loss: 0.4157 - val_acc: 0.7998\n",
      "Epoch 49/50\n",
      " - 58s - loss: 0.4031 - acc: 0.8097 - val_loss: 0.4140 - val_acc: 0.8056\n",
      "Epoch 50/50\n",
      " - 58s - loss: 0.4023 - acc: 0.8106 - val_loss: 0.4142 - val_acc: 0.8044\n",
      "Training ended at 2018-07-01 19:55:33.673243\n",
      "Minutes elapsed: 51.676381\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_split=val_split,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                    'training': history.history['acc'],\n",
    "                    'validation': history.history['val_acc']})\n",
    "ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.0,1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with best validation accuracy on the test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4726, accuracy = 0.8038\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
