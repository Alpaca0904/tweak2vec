{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime, time, json\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, History\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\")\n",
    "labels = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_labels.npy\")\n",
    "\n",
    "w2v_embedding = {}\n",
    "\n",
    "\n",
    "# w2v_embedding['reg_5m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_5m.npy\")\n",
    "# w2v_embedding['reg_3m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_3m.npy\")\n",
    "# w2v_embedding['reg_1m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_1m.npy\")\n",
    "# w2v_embedding['reg_05m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_05m.npy\")\n",
    "# w2v_embedding['reg_01m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_01m.npy\")\n",
    "# w2v_embedding['reg_005m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_005m.npy\")\n",
    "# w2v_embedding['reg_001m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_pivotsfull_alpha10_001m.npy\")\n",
    "\n",
    "# w2v_embedding['cat_reg_5m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_5m.npy\")\n",
    "# w2v_embedding['cat_reg_3m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_3m.npy\")\n",
    "# w2v_embedding['cat_reg_1m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_1m.npy\")\n",
    "# w2v_embedding['cat_reg_05m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_05m.npy\")\n",
    "# w2v_embedding['cat_reg_01m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_01m.npy\")\n",
    "# w2v_embedding['cat_reg_005m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_005m.npy\")\n",
    "# w2v_embedding['cat_reg_001m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_cat_reg_001m.npy\")\n",
    "\n",
    "w2v_embedding['ret_5m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_5m.npy\")\n",
    "w2v_embedding['ret_3m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_3m.npy\")\n",
    "w2v_embedding['ret_1m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_1m.npy\")\n",
    "w2v_embedding['ret_05m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_05m.npy\")\n",
    "w2v_embedding['ret_01m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_01m.npy\")\n",
    "w2v_embedding['ret_005m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_005m.npy\")\n",
    "w2v_embedding['ret_001m'] = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_retrain_001m.npy\")\n",
    "\n",
    "\n",
    "#w2v_google_50d = np.load(\"/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_quoragoogle_50d.npy\")\n",
    "#w2v_concat = np.concatenate([w2v_google_50d, w2v_pivots100_50d], axis=1)\n",
    "#w2v_avg = (w2v_google_50d + w2v_quora_50d)/2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30300, 50)\n",
      "(30300, 50)\n",
      "(30300, 50)\n",
      "(30300, 50)\n",
      "(30300, 50)\n",
      "(30300, 50)\n",
      "(30300, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "w2v_cat_reg = {}\n",
    "for reg in w2v_embedding.keys():\n",
    "    name = 'cat_'+reg\n",
    "    w2v_concat = np.concatenate([w2v_google_50d, w2v_embedding[reg]], axis=1)\n",
    "    w2v_concat = pca.fit_transform(w2v_concat)\n",
    "    np.save('w2v_cat_'+reg+'.npy',w2v_concat)\n",
    "    print(w2v_concat.shape)\n",
    "    w2v_cat_reg[name] = w2v_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate question1 and question2\n",
    "question1 = []\n",
    "question2 = []\n",
    "for n in range(int(len(quora_corpus)/2)):\n",
    "    question1.append(quora_corpus[2*n])\n",
    "    question2.append(quora_corpus[2*n+1])\n",
    "    \n",
    "q1_data = pad_sequences(question1, maxlen=25)\n",
    "q2_data = pad_sequences(question2, maxlen=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length:190, average length:8.62675648734686\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "l_avg = []\n",
    "for i in range(len(quora_corpus)):\n",
    "    if len(quora_corpus[i]) > l:\n",
    "        l = len(quora_corpus[i])\n",
    "    l_avg.append(len(quora_corpus[i]))\n",
    "print('max length:{0}, average length:{1}'.format(l,np.mean(np.array(l_avg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setup\n",
    "max_sentence_len = 25\n",
    "embed_dim = 50\n",
    "dropout_rate = 0.1\n",
    "vocab_size = len(w2v_embedding['ret_5m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split cross validation set and test set\n",
    "questions = np.stack((q1_data, q2_data), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(questions, labels, test_size=0.1, random_state=2018)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Max_BoE(word_embedding):\n",
    "\n",
    "    question1 = Input(shape=(max_sentence_len,))\n",
    "    question2 = Input(shape=(max_sentence_len,))\n",
    "\n",
    "\n",
    "\n",
    "    q1 = Embedding(  input_dim=vocab_size, \n",
    "                     output_dim=embed_dim, \n",
    "                     weights=[word_embedding], \n",
    "                     input_length=max_sentence_len, \n",
    "                     trainable=False)(question1)\n",
    "    q1 = TimeDistributed(Dense(embed_dim, activation='relu'))(q1)\n",
    "    q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embed_dim, ))(q1)\n",
    "\n",
    "    q2 = Embedding(  input_dim=vocab_size, \n",
    "                     output_dim=embed_dim, \n",
    "                     weights=[word_embedding], \n",
    "                     input_length=max_sentence_len, \n",
    "                     trainable=False)(question2)\n",
    "    q2 = TimeDistributed(Dense(embed_dim, activation='relu'))(q2)\n",
    "    q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embed_dim, ))(q2)\n",
    "\n",
    "    merged = concatenate([q1,q2])\n",
    "    merged = Dense(200, activation='relu')(merged)\n",
    "    merged = Dropout(dropout_rate)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(200, activation='relu')(merged)\n",
    "    merged = Dropout(dropout_rate)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(200, activation='relu')(merged)\n",
    "    merged = Dropout(dropout_rate)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(200, activation='relu')(merged)\n",
    "    merged = Dropout(dropout_rate)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "val_split = 0.1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current embedding:  ret_5m\n",
      "Starting training at 2018-08-01 02:49:42.205811\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 88s 268us/step - loss: 0.5439 - acc: 0.7249 - val_loss: 0.4918 - val_acc: 0.7588\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4977 - acc: 0.7537 - val_loss: 0.4704 - val_acc: 0.7711\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4806 - acc: 0.7630 - val_loss: 0.4632 - val_acc: 0.7756\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4707 - acc: 0.7673 - val_loss: 0.4558 - val_acc: 0.7777\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4649 - acc: 0.7721 - val_loss: 0.4487 - val_acc: 0.7819\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 76s 231us/step - loss: 0.4599 - acc: 0.7743 - val_loss: 0.4468 - val_acc: 0.7845\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4536 - acc: 0.7787 - val_loss: 0.4445 - val_acc: 0.7843\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 75s 229us/step - loss: 0.4499 - acc: 0.7814 - val_loss: 0.4375 - val_acc: 0.7869\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4468 - acc: 0.7829 - val_loss: 0.4396 - val_acc: 0.7840\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 69s 211us/step - loss: 0.4439 - acc: 0.7849 - val_loss: 0.4366 - val_acc: 0.7877\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 70s 215us/step - loss: 0.4420 - acc: 0.7858 - val_loss: 0.4318 - val_acc: 0.7914\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 69s 212us/step - loss: 0.4379 - acc: 0.7887 - val_loss: 0.4370 - val_acc: 0.7877\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 70s 213us/step - loss: 0.4358 - acc: 0.7901 - val_loss: 0.4306 - val_acc: 0.7943\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4334 - acc: 0.7910 - val_loss: 0.4260 - val_acc: 0.7938\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4314 - acc: 0.7921 - val_loss: 0.4270 - val_acc: 0.7943\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4296 - acc: 0.7940 - val_loss: 0.4262 - val_acc: 0.7932\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4283 - acc: 0.7939 - val_loss: 0.4302 - val_acc: 0.7945\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4268 - acc: 0.7953 - val_loss: 0.4236 - val_acc: 0.7981\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4260 - acc: 0.7959 - val_loss: 0.4239 - val_acc: 0.7983\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4250 - acc: 0.7961 - val_loss: 0.4249 - val_acc: 0.7952\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4235 - acc: 0.7973 - val_loss: 0.4228 - val_acc: 0.7974\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4220 - acc: 0.7980 - val_loss: 0.4230 - val_acc: 0.7989\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4196 - acc: 0.7999 - val_loss: 0.4209 - val_acc: 0.7989\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4194 - acc: 0.8005 - val_loss: 0.4198 - val_acc: 0.8025\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4180 - acc: 0.8010 - val_loss: 0.4187 - val_acc: 0.7982\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 73s 223us/step - loss: 0.4169 - acc: 0.8016 - val_loss: 0.4175 - val_acc: 0.8025\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4157 - acc: 0.8024 - val_loss: 0.4204 - val_acc: 0.7995\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4144 - acc: 0.8025 - val_loss: 0.4201 - val_acc: 0.8003\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4131 - acc: 0.8035 - val_loss: 0.4182 - val_acc: 0.8021\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4118 - acc: 0.8046 - val_loss: 0.4231 - val_acc: 0.8014\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4125 - acc: 0.8041 - val_loss: 0.4133 - val_acc: 0.8065\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4119 - acc: 0.8039 - val_loss: 0.4161 - val_acc: 0.8007\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4099 - acc: 0.8058 - val_loss: 0.4138 - val_acc: 0.8037\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 73s 222us/step - loss: 0.4085 - acc: 0.8066 - val_loss: 0.4176 - val_acc: 0.8017\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4078 - acc: 0.8062 - val_loss: 0.4127 - val_acc: 0.8043\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4073 - acc: 0.8066 - val_loss: 0.4145 - val_acc: 0.8042\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4076 - acc: 0.8069 - val_loss: 0.4137 - val_acc: 0.8007\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4060 - acc: 0.8081 - val_loss: 0.4127 - val_acc: 0.8040\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4053 - acc: 0.8083 - val_loss: 0.4168 - val_acc: 0.8016\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4063 - acc: 0.8072 - val_loss: 0.4112 - val_acc: 0.8074\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4044 - acc: 0.8081 - val_loss: 0.4128 - val_acc: 0.8059\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4036 - acc: 0.8098 - val_loss: 0.4145 - val_acc: 0.8034\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4031 - acc: 0.8096 - val_loss: 0.4092 - val_acc: 0.8061\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4033 - acc: 0.8093 - val_loss: 0.4114 - val_acc: 0.8061\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4024 - acc: 0.8094 - val_loss: 0.4123 - val_acc: 0.8051\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4018 - acc: 0.8102 - val_loss: 0.4168 - val_acc: 0.7996\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4019 - acc: 0.8097 - val_loss: 0.4122 - val_acc: 0.8073\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4011 - acc: 0.8105 - val_loss: 0.4093 - val_acc: 0.8061\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 72s 221us/step - loss: 0.4009 - acc: 0.8105 - val_loss: 0.4093 - val_acc: 0.8064\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 73s 223us/step - loss: 0.4001 - acc: 0.8112 - val_loss: 0.4122 - val_acc: 0.8043\n",
      "Training ended at 2018-08-01 03:50:21.677149\n",
      "Minutes elapsed: 60.657855\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_3m\n",
      "Starting training at 2018-08-01 03:50:25.263709\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.5419 - acc: 0.7254 - val_loss: 0.4943 - val_acc: 0.7550\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4988 - acc: 0.7524 - val_loss: 0.4785 - val_acc: 0.7600\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 75s 228us/step - loss: 0.4846 - acc: 0.7600 - val_loss: 0.4659 - val_acc: 0.7715\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4752 - acc: 0.7660 - val_loss: 0.4600 - val_acc: 0.7766\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4677 - acc: 0.7703 - val_loss: 0.4507 - val_acc: 0.7810\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4623 - acc: 0.7740 - val_loss: 0.4473 - val_acc: 0.7823\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4571 - acc: 0.7770 - val_loss: 0.4455 - val_acc: 0.7792\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4537 - acc: 0.7791 - val_loss: 0.4446 - val_acc: 0.7827\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4498 - acc: 0.7817 - val_loss: 0.4485 - val_acc: 0.7837\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 70s 215us/step - loss: 0.4468 - acc: 0.7830 - val_loss: 0.4394 - val_acc: 0.7847\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 71s 218us/step - loss: 0.4443 - acc: 0.7848 - val_loss: 0.4397 - val_acc: 0.7857\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4416 - acc: 0.7860 - val_loss: 0.4355 - val_acc: 0.7888\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4402 - acc: 0.7873 - val_loss: 0.4387 - val_acc: 0.7893\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4375 - acc: 0.7893 - val_loss: 0.4316 - val_acc: 0.7916\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4351 - acc: 0.7904 - val_loss: 0.4340 - val_acc: 0.7921\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4327 - acc: 0.7922 - val_loss: 0.4335 - val_acc: 0.7894\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4321 - acc: 0.7930 - val_loss: 0.4320 - val_acc: 0.7907\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4338 - val_acc: 0.7930\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4281 - acc: 0.7946 - val_loss: 0.4252 - val_acc: 0.7968\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4264 - acc: 0.7959 - val_loss: 0.4283 - val_acc: 0.7941\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4259 - acc: 0.7967 - val_loss: 0.4276 - val_acc: 0.7937\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4237 - acc: 0.7972 - val_loss: 0.4284 - val_acc: 0.7920\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4223 - acc: 0.7983 - val_loss: 0.4285 - val_acc: 0.7933\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4225 - acc: 0.7987 - val_loss: 0.4265 - val_acc: 0.7941\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4209 - acc: 0.7990 - val_loss: 0.4248 - val_acc: 0.7945\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4192 - acc: 0.8005 - val_loss: 0.4298 - val_acc: 0.7940\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4186 - acc: 0.8007 - val_loss: 0.4265 - val_acc: 0.7933\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 70s 215us/step - loss: 0.4180 - acc: 0.8017 - val_loss: 0.4283 - val_acc: 0.7936\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 72s 219us/step - loss: 0.4176 - acc: 0.8016 - val_loss: 0.4252 - val_acc: 0.7953\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4175 - acc: 0.8014 - val_loss: 0.4267 - val_acc: 0.7962\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4177 - acc: 0.8018 - val_loss: 0.4255 - val_acc: 0.7961\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4159 - acc: 0.8026 - val_loss: 0.4268 - val_acc: 0.7937\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4165 - acc: 0.8027 - val_loss: 0.4248 - val_acc: 0.7951\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4161 - acc: 0.8033 - val_loss: 0.4229 - val_acc: 0.7963\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4139 - acc: 0.8040 - val_loss: 0.4270 - val_acc: 0.7940\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4158 - acc: 0.8033 - val_loss: 0.4248 - val_acc: 0.7973\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4131 - acc: 0.8046 - val_loss: 0.4221 - val_acc: 0.7968\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 71s 215us/step - loss: 0.4133 - acc: 0.8039 - val_loss: 0.4240 - val_acc: 0.7964\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4125 - acc: 0.8048 - val_loss: 0.4232 - val_acc: 0.7977\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 71s 218us/step - loss: 0.4122 - acc: 0.8046 - val_loss: 0.4266 - val_acc: 0.7960\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4111 - acc: 0.8049 - val_loss: 0.4254 - val_acc: 0.7944\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4112 - acc: 0.8047 - val_loss: 0.4238 - val_acc: 0.7975\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4108 - acc: 0.8051 - val_loss: 0.4237 - val_acc: 0.7959\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4093 - acc: 0.8060 - val_loss: 0.4248 - val_acc: 0.7963\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4093 - acc: 0.8064 - val_loss: 0.4230 - val_acc: 0.7979\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4089 - acc: 0.8066 - val_loss: 0.4276 - val_acc: 0.7970\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 71s 216us/step - loss: 0.4099 - acc: 0.8065 - val_loss: 0.4219 - val_acc: 0.7970\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4083 - acc: 0.8062 - val_loss: 0.4220 - val_acc: 0.7992\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 71s 217us/step - loss: 0.4081 - acc: 0.8065 - val_loss: 0.4215 - val_acc: 0.8009\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 72s 220us/step - loss: 0.4072 - acc: 0.8073 - val_loss: 0.4246 - val_acc: 0.7966\n",
      "Training ended at 2018-08-01 04:49:50.783215\n",
      "Minutes elapsed: 59.425324\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_1m\n",
      "Starting training at 2018-08-01 04:49:54.603577\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.5458 - acc: 0.7215 - val_loss: 0.5075 - val_acc: 0.7494\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.5058 - acc: 0.7477 - val_loss: 0.4831 - val_acc: 0.7610\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4918 - acc: 0.7558 - val_loss: 0.4736 - val_acc: 0.7661\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4827 - acc: 0.7605 - val_loss: 0.4685 - val_acc: 0.7669\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4751 - acc: 0.7649 - val_loss: 0.4653 - val_acc: 0.7734\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4710 - acc: 0.7678 - val_loss: 0.4584 - val_acc: 0.7772\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 76s 231us/step - loss: 0.4658 - acc: 0.7712 - val_loss: 0.4675 - val_acc: 0.7643\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 75s 230us/step - loss: 0.4619 - acc: 0.7737 - val_loss: 0.4545 - val_acc: 0.7791\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4590 - acc: 0.7757 - val_loss: 0.4512 - val_acc: 0.7812\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4558 - acc: 0.7774 - val_loss: 0.4488 - val_acc: 0.7816\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4528 - acc: 0.7797 - val_loss: 0.4449 - val_acc: 0.7841\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4492 - acc: 0.7810 - val_loss: 0.4463 - val_acc: 0.7835\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4478 - acc: 0.7820 - val_loss: 0.4438 - val_acc: 0.7842\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4462 - acc: 0.7827 - val_loss: 0.4436 - val_acc: 0.7821\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4443 - acc: 0.7842 - val_loss: 0.4406 - val_acc: 0.7881\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4427 - acc: 0.7856 - val_loss: 0.4471 - val_acc: 0.7835\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4421 - acc: 0.7854 - val_loss: 0.4458 - val_acc: 0.7845\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4406 - acc: 0.7859 - val_loss: 0.4402 - val_acc: 0.7881\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 75s 228us/step - loss: 0.4387 - acc: 0.7879 - val_loss: 0.4398 - val_acc: 0.7877\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4368 - acc: 0.7891 - val_loss: 0.4384 - val_acc: 0.7885\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4362 - acc: 0.7894 - val_loss: 0.4375 - val_acc: 0.7898\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4350 - acc: 0.7902 - val_loss: 0.4360 - val_acc: 0.7895\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4336 - acc: 0.7907 - val_loss: 0.4384 - val_acc: 0.7896\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4330 - acc: 0.7915 - val_loss: 0.4365 - val_acc: 0.7887\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4312 - acc: 0.7927 - val_loss: 0.4343 - val_acc: 0.7920\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4306 - acc: 0.7930 - val_loss: 0.4384 - val_acc: 0.7917\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4289 - acc: 0.7932 - val_loss: 0.4330 - val_acc: 0.7932\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4287 - acc: 0.7940 - val_loss: 0.4327 - val_acc: 0.7920\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4274 - acc: 0.7944 - val_loss: 0.4314 - val_acc: 0.7924\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4272 - acc: 0.7951 - val_loss: 0.4348 - val_acc: 0.7935\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4260 - acc: 0.7961 - val_loss: 0.4337 - val_acc: 0.7902\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 73s 223us/step - loss: 0.4258 - acc: 0.7963 - val_loss: 0.4342 - val_acc: 0.7904\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4252 - acc: 0.7970 - val_loss: 0.4348 - val_acc: 0.7922\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4249 - acc: 0.7968 - val_loss: 0.4337 - val_acc: 0.7909\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4233 - acc: 0.7972 - val_loss: 0.4303 - val_acc: 0.7924\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4231 - acc: 0.7967 - val_loss: 0.4348 - val_acc: 0.7922\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4223 - acc: 0.7980 - val_loss: 0.4304 - val_acc: 0.7927\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4219 - acc: 0.7975 - val_loss: 0.4307 - val_acc: 0.7920\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4205 - acc: 0.7993 - val_loss: 0.4314 - val_acc: 0.7919\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 74s 225us/step - loss: 0.4196 - acc: 0.7991 - val_loss: 0.4287 - val_acc: 0.7960\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 73s 224us/step - loss: 0.4190 - acc: 0.7994 - val_loss: 0.4289 - val_acc: 0.7943\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4184 - acc: 0.8006 - val_loss: 0.4298 - val_acc: 0.7939\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 75s 229us/step - loss: 0.4188 - acc: 0.8000 - val_loss: 0.4279 - val_acc: 0.7946\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4191 - acc: 0.7997 - val_loss: 0.4272 - val_acc: 0.7942\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4184 - acc: 0.8009 - val_loss: 0.4320 - val_acc: 0.7921\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4177 - acc: 0.8006 - val_loss: 0.4347 - val_acc: 0.7918\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 74s 227us/step - loss: 0.4178 - acc: 0.8004 - val_loss: 0.4290 - val_acc: 0.7949\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4170 - acc: 0.8013 - val_loss: 0.4292 - val_acc: 0.7947\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4167 - acc: 0.8017 - val_loss: 0.4307 - val_acc: 0.7932\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 74s 226us/step - loss: 0.4166 - acc: 0.8019 - val_loss: 0.4294 - val_acc: 0.7938\n",
      "Training ended at 2018-08-01 05:51:36.770108\n",
      "Minutes elapsed: 61.702775\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_05m\n",
      "Starting training at 2018-08-01 05:51:40.921403\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.5534 - acc: 0.7167 - val_loss: 0.5108 - val_acc: 0.7489\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 76s 234us/step - loss: 0.5125 - acc: 0.7419 - val_loss: 0.4893 - val_acc: 0.7556\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4974 - acc: 0.7504 - val_loss: 0.4776 - val_acc: 0.7610\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4884 - acc: 0.7560 - val_loss: 0.4718 - val_acc: 0.7673\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4816 - acc: 0.7610 - val_loss: 0.4702 - val_acc: 0.7684\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4762 - acc: 0.7641 - val_loss: 0.4665 - val_acc: 0.7681\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4727 - acc: 0.7661 - val_loss: 0.4615 - val_acc: 0.7710\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4688 - acc: 0.7684 - val_loss: 0.4661 - val_acc: 0.7686\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4646 - acc: 0.7703 - val_loss: 0.4596 - val_acc: 0.7736\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4628 - acc: 0.7725 - val_loss: 0.4584 - val_acc: 0.7754\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4603 - acc: 0.7737 - val_loss: 0.4572 - val_acc: 0.7758\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4589 - acc: 0.7744 - val_loss: 0.4537 - val_acc: 0.7756\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 76s 231us/step - loss: 0.4552 - acc: 0.7768 - val_loss: 0.4525 - val_acc: 0.7770\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4538 - acc: 0.7784 - val_loss: 0.4545 - val_acc: 0.7769\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 76s 231us/step - loss: 0.4531 - acc: 0.7787 - val_loss: 0.4515 - val_acc: 0.7796\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4500 - acc: 0.7804 - val_loss: 0.4495 - val_acc: 0.7810\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4487 - acc: 0.7812 - val_loss: 0.4479 - val_acc: 0.7807\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4479 - acc: 0.7815 - val_loss: 0.4459 - val_acc: 0.7826\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4464 - acc: 0.7822 - val_loss: 0.4496 - val_acc: 0.7795\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 76s 231us/step - loss: 0.4449 - acc: 0.7835 - val_loss: 0.4516 - val_acc: 0.7774\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4439 - acc: 0.7847 - val_loss: 0.4500 - val_acc: 0.7805\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4432 - acc: 0.7852 - val_loss: 0.4491 - val_acc: 0.7805\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4405 - acc: 0.7869 - val_loss: 0.4476 - val_acc: 0.7811\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4397 - acc: 0.7865 - val_loss: 0.4456 - val_acc: 0.7813\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4401 - acc: 0.7869 - val_loss: 0.4438 - val_acc: 0.7829\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4390 - acc: 0.7870 - val_loss: 0.4440 - val_acc: 0.7846\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4374 - acc: 0.7883 - val_loss: 0.4428 - val_acc: 0.7859\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4359 - acc: 0.7890 - val_loss: 0.4440 - val_acc: 0.7834\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4357 - acc: 0.7893 - val_loss: 0.4435 - val_acc: 0.7851\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4348 - acc: 0.7895 - val_loss: 0.4436 - val_acc: 0.7839\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4344 - acc: 0.7899 - val_loss: 0.4431 - val_acc: 0.7860\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4334 - acc: 0.7908 - val_loss: 0.4471 - val_acc: 0.7825\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4335 - acc: 0.7907 - val_loss: 0.4443 - val_acc: 0.7834\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4332 - acc: 0.7913 - val_loss: 0.4409 - val_acc: 0.7867\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4312 - acc: 0.7923 - val_loss: 0.4429 - val_acc: 0.7875\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4313 - acc: 0.7921 - val_loss: 0.4391 - val_acc: 0.7866\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4306 - acc: 0.7926 - val_loss: 0.4428 - val_acc: 0.7868\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4395 - val_acc: 0.7874\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 78s 238us/step - loss: 0.4287 - acc: 0.7941 - val_loss: 0.4379 - val_acc: 0.7860\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4417 - val_acc: 0.7871\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4285 - acc: 0.7947 - val_loss: 0.4391 - val_acc: 0.7885\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4283 - acc: 0.7939 - val_loss: 0.4401 - val_acc: 0.7861\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4275 - acc: 0.7940 - val_loss: 0.4423 - val_acc: 0.7876\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4273 - acc: 0.7951 - val_loss: 0.4413 - val_acc: 0.7863\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 77s 235us/step - loss: 0.4270 - acc: 0.7946 - val_loss: 0.4405 - val_acc: 0.7882\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4260 - acc: 0.7953 - val_loss: 0.4421 - val_acc: 0.7853\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4263 - acc: 0.7955 - val_loss: 0.4427 - val_acc: 0.7828\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 76s 232us/step - loss: 0.4246 - acc: 0.7964 - val_loss: 0.4386 - val_acc: 0.7863\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4258 - acc: 0.7960 - val_loss: 0.4410 - val_acc: 0.7873\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 76s 233us/step - loss: 0.4252 - acc: 0.7963 - val_loss: 0.4379 - val_acc: 0.7877\n",
      "Training ended at 2018-08-01 06:55:32.383050\n",
      "Minutes elapsed: 63.857693\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_01m\n",
      "Starting training at 2018-08-01 06:55:36.990685\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.5587 - acc: 0.7117 - val_loss: 0.5151 - val_acc: 0.7389\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.5209 - acc: 0.7364 - val_loss: 0.5026 - val_acc: 0.7442\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.5071 - acc: 0.7458 - val_loss: 0.4944 - val_acc: 0.7528\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4986 - acc: 0.7507 - val_loss: 0.4904 - val_acc: 0.7528\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4923 - acc: 0.7550 - val_loss: 0.4871 - val_acc: 0.7565\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4874 - acc: 0.7581 - val_loss: 0.4789 - val_acc: 0.7609\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4840 - acc: 0.7606 - val_loss: 0.4753 - val_acc: 0.7625\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4806 - acc: 0.7627 - val_loss: 0.4799 - val_acc: 0.7619\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4768 - acc: 0.7645 - val_loss: 0.4729 - val_acc: 0.7623\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 77s 234us/step - loss: 0.4753 - acc: 0.7654 - val_loss: 0.4718 - val_acc: 0.7645\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 79s 243us/step - loss: 0.4725 - acc: 0.7663 - val_loss: 0.4661 - val_acc: 0.7680\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4693 - acc: 0.7698 - val_loss: 0.4744 - val_acc: 0.7624\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4678 - acc: 0.7715 - val_loss: 0.4718 - val_acc: 0.7667\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 79s 242us/step - loss: 0.4654 - acc: 0.7712 - val_loss: 0.4595 - val_acc: 0.7732\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4650 - acc: 0.7722 - val_loss: 0.4647 - val_acc: 0.7700\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4628 - acc: 0.7742 - val_loss: 0.4585 - val_acc: 0.7756\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 78s 238us/step - loss: 0.4616 - acc: 0.7750 - val_loss: 0.4630 - val_acc: 0.7708\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4588 - acc: 0.7765 - val_loss: 0.4597 - val_acc: 0.7744\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4573 - acc: 0.7772 - val_loss: 0.4561 - val_acc: 0.7751\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4560 - acc: 0.7787 - val_loss: 0.4530 - val_acc: 0.7774\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4545 - acc: 0.7791 - val_loss: 0.4550 - val_acc: 0.7772\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 78s 240us/step - loss: 0.4539 - acc: 0.7793 - val_loss: 0.4578 - val_acc: 0.7765\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 78s 240us/step - loss: 0.4533 - acc: 0.7807 - val_loss: 0.4552 - val_acc: 0.7772\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4526 - acc: 0.7805 - val_loss: 0.4601 - val_acc: 0.7737\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4508 - acc: 0.7809 - val_loss: 0.4511 - val_acc: 0.7776\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4499 - acc: 0.7817 - val_loss: 0.4498 - val_acc: 0.7801\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4493 - acc: 0.7827 - val_loss: 0.4510 - val_acc: 0.7796\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 80s 244us/step - loss: 0.4482 - acc: 0.7831 - val_loss: 0.4515 - val_acc: 0.7807\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 78s 240us/step - loss: 0.4476 - acc: 0.7838 - val_loss: 0.4583 - val_acc: 0.7774\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4469 - acc: 0.7840 - val_loss: 0.4514 - val_acc: 0.7789\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4448 - acc: 0.7856 - val_loss: 0.4511 - val_acc: 0.7786\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 78s 240us/step - loss: 0.4461 - acc: 0.7843 - val_loss: 0.4496 - val_acc: 0.7820\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4435 - acc: 0.7866 - val_loss: 0.4545 - val_acc: 0.7772\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4444 - acc: 0.7861 - val_loss: 0.4500 - val_acc: 0.7790\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4448 - acc: 0.7852 - val_loss: 0.4483 - val_acc: 0.7789\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4425 - acc: 0.7869 - val_loss: 0.4446 - val_acc: 0.7821\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4416 - acc: 0.7868 - val_loss: 0.4477 - val_acc: 0.7824\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4407 - acc: 0.7873 - val_loss: 0.4540 - val_acc: 0.7780\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 79s 240us/step - loss: 0.4404 - acc: 0.7886 - val_loss: 0.4491 - val_acc: 0.7816\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4407 - acc: 0.7876 - val_loss: 0.4494 - val_acc: 0.7819\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4396 - acc: 0.7881 - val_loss: 0.4506 - val_acc: 0.7806\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 78s 238us/step - loss: 0.4382 - acc: 0.7896 - val_loss: 0.4457 - val_acc: 0.7840\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4386 - acc: 0.7894 - val_loss: 0.4478 - val_acc: 0.7833\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4374 - acc: 0.7893 - val_loss: 0.4536 - val_acc: 0.7789\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4379 - acc: 0.7888 - val_loss: 0.4499 - val_acc: 0.7833\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4365 - acc: 0.7903 - val_loss: 0.4450 - val_acc: 0.7850\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4369 - acc: 0.7910 - val_loss: 0.4451 - val_acc: 0.7830\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 78s 239us/step - loss: 0.4363 - acc: 0.7911 - val_loss: 0.4444 - val_acc: 0.7839\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4358 - acc: 0.7913 - val_loss: 0.4486 - val_acc: 0.7806\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 79s 241us/step - loss: 0.4352 - acc: 0.7913 - val_loss: 0.4462 - val_acc: 0.7838\n",
      "Training ended at 2018-08-01 08:01:09.569042\n",
      "Minutes elapsed: 65.542972\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_005m\n",
      "Starting training at 2018-08-01 08:01:14.568821\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 84s 255us/step - loss: 0.5587 - acc: 0.7092 - val_loss: 0.5163 - val_acc: 0.7384\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.5168 - acc: 0.7393 - val_loss: 0.4943 - val_acc: 0.7531\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.5015 - acc: 0.7493 - val_loss: 0.4910 - val_acc: 0.7538\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4923 - acc: 0.7548 - val_loss: 0.4814 - val_acc: 0.7633\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 82s 252us/step - loss: 0.4842 - acc: 0.7597 - val_loss: 0.4695 - val_acc: 0.7659\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4794 - acc: 0.7641 - val_loss: 0.4674 - val_acc: 0.7708\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4751 - acc: 0.7663 - val_loss: 0.4680 - val_acc: 0.7678\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4707 - acc: 0.7687 - val_loss: 0.4656 - val_acc: 0.7692\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4676 - acc: 0.7718 - val_loss: 0.4594 - val_acc: 0.7754\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4644 - acc: 0.7735 - val_loss: 0.4657 - val_acc: 0.7693\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4625 - acc: 0.7746 - val_loss: 0.4551 - val_acc: 0.7776\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4590 - acc: 0.7769 - val_loss: 0.4570 - val_acc: 0.7771\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4565 - acc: 0.7782 - val_loss: 0.4555 - val_acc: 0.7771\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4549 - acc: 0.7794 - val_loss: 0.4566 - val_acc: 0.7767\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4535 - acc: 0.7807 - val_loss: 0.4513 - val_acc: 0.7818\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4527 - acc: 0.7811 - val_loss: 0.4515 - val_acc: 0.7791\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4508 - acc: 0.7816 - val_loss: 0.4491 - val_acc: 0.7814\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4488 - acc: 0.7832 - val_loss: 0.4455 - val_acc: 0.7841\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4476 - acc: 0.7839 - val_loss: 0.4455 - val_acc: 0.7839\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4458 - acc: 0.7845 - val_loss: 0.4491 - val_acc: 0.7824\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4440 - acc: 0.7864 - val_loss: 0.4474 - val_acc: 0.7826\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4435 - acc: 0.7864 - val_loss: 0.4449 - val_acc: 0.7849\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4419 - acc: 0.7876 - val_loss: 0.4452 - val_acc: 0.7863\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4412 - acc: 0.7877 - val_loss: 0.4452 - val_acc: 0.7841\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4397 - acc: 0.7888 - val_loss: 0.4415 - val_acc: 0.7862\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4390 - acc: 0.7899 - val_loss: 0.4455 - val_acc: 0.7814\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4376 - acc: 0.7904 - val_loss: 0.4407 - val_acc: 0.7854\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4368 - acc: 0.7911 - val_loss: 0.4434 - val_acc: 0.7856\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4365 - acc: 0.7912 - val_loss: 0.4376 - val_acc: 0.7863\n",
      "Epoch 30/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4345 - acc: 0.7916 - val_loss: 0.4404 - val_acc: 0.7877\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4347 - acc: 0.7928 - val_loss: 0.4434 - val_acc: 0.7858\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4334 - acc: 0.7934 - val_loss: 0.4378 - val_acc: 0.7862\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4314 - acc: 0.7943 - val_loss: 0.4402 - val_acc: 0.7872\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4310 - acc: 0.7941 - val_loss: 0.4397 - val_acc: 0.7881\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4300 - acc: 0.7949 - val_loss: 0.4412 - val_acc: 0.7895\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4300 - acc: 0.7944 - val_loss: 0.4387 - val_acc: 0.7870\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4289 - acc: 0.7954 - val_loss: 0.4376 - val_acc: 0.7878\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4272 - acc: 0.7964 - val_loss: 0.4374 - val_acc: 0.7875\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4274 - acc: 0.7967 - val_loss: 0.4381 - val_acc: 0.7868\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4272 - acc: 0.7964 - val_loss: 0.4450 - val_acc: 0.7814\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4283 - acc: 0.7954 - val_loss: 0.4388 - val_acc: 0.7886\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 80s 246us/step - loss: 0.4262 - acc: 0.7968 - val_loss: 0.4416 - val_acc: 0.7873\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4265 - acc: 0.7962 - val_loss: 0.4329 - val_acc: 0.7910\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4236 - acc: 0.7980 - val_loss: 0.4398 - val_acc: 0.7865\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 80s 243us/step - loss: 0.4243 - acc: 0.7979 - val_loss: 0.4471 - val_acc: 0.7836\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 80s 246us/step - loss: 0.4236 - acc: 0.7987 - val_loss: 0.4338 - val_acc: 0.7904\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 80s 245us/step - loss: 0.4232 - acc: 0.7992 - val_loss: 0.4371 - val_acc: 0.7895\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 80s 246us/step - loss: 0.4217 - acc: 0.7995 - val_loss: 0.4346 - val_acc: 0.7917\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4216 - acc: 0.8000 - val_loss: 0.4346 - val_acc: 0.7906\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 81s 247us/step - loss: 0.4204 - acc: 0.8009 - val_loss: 0.4361 - val_acc: 0.7908\n",
      "Training ended at 2018-08-01 09:08:47.086174\n",
      "Minutes elapsed: 67.541955\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "current embedding:  ret_001m\n",
      "Starting training at 2018-08-01 09:08:52.507285\n",
      "Train on 327472 samples, validate on 36386 samples\n",
      "Epoch 1/50\n",
      "327472/327472 [==============================] - 85s 259us/step - loss: 0.5660 - acc: 0.7020 - val_loss: 0.5225 - val_acc: 0.7323\n",
      "Epoch 2/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.5205 - acc: 0.7359 - val_loss: 0.5007 - val_acc: 0.7491\n",
      "Epoch 3/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.5039 - acc: 0.7474 - val_loss: 0.4897 - val_acc: 0.7557\n",
      "Epoch 4/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4912 - acc: 0.7551 - val_loss: 0.4780 - val_acc: 0.7635\n",
      "Epoch 5/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4832 - acc: 0.7612 - val_loss: 0.4765 - val_acc: 0.7646\n",
      "Epoch 6/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4762 - acc: 0.7648 - val_loss: 0.4662 - val_acc: 0.7672\n",
      "Epoch 7/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4706 - acc: 0.7695 - val_loss: 0.4679 - val_acc: 0.7702\n",
      "Epoch 8/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4665 - acc: 0.7718 - val_loss: 0.4564 - val_acc: 0.7762\n",
      "Epoch 9/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4627 - acc: 0.7744 - val_loss: 0.4581 - val_acc: 0.7751\n",
      "Epoch 10/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4600 - acc: 0.7759 - val_loss: 0.4552 - val_acc: 0.7775\n",
      "Epoch 11/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4558 - acc: 0.7793 - val_loss: 0.4511 - val_acc: 0.7796\n",
      "Epoch 12/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4534 - acc: 0.7805 - val_loss: 0.4496 - val_acc: 0.7843\n",
      "Epoch 13/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4510 - acc: 0.7813 - val_loss: 0.4512 - val_acc: 0.7793\n",
      "Epoch 14/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4480 - acc: 0.7838 - val_loss: 0.4532 - val_acc: 0.7837\n",
      "Epoch 15/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4460 - acc: 0.7849 - val_loss: 0.4446 - val_acc: 0.7845\n",
      "Epoch 16/50\n",
      "327472/327472 [==============================] - 83s 253us/step - loss: 0.4450 - acc: 0.7854 - val_loss: 0.4481 - val_acc: 0.7798\n",
      "Epoch 17/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4427 - acc: 0.7867 - val_loss: 0.4466 - val_acc: 0.7831\n",
      "Epoch 18/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4411 - acc: 0.7879 - val_loss: 0.4418 - val_acc: 0.7877\n",
      "Epoch 19/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4398 - acc: 0.7891 - val_loss: 0.4407 - val_acc: 0.7859\n",
      "Epoch 20/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4390 - acc: 0.7885 - val_loss: 0.4460 - val_acc: 0.7829\n",
      "Epoch 21/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4378 - acc: 0.7898 - val_loss: 0.4414 - val_acc: 0.7869\n",
      "Epoch 22/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4348 - acc: 0.7914 - val_loss: 0.4415 - val_acc: 0.7874\n",
      "Epoch 23/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4355 - acc: 0.7913 - val_loss: 0.4406 - val_acc: 0.7889\n",
      "Epoch 24/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4337 - acc: 0.7918 - val_loss: 0.4377 - val_acc: 0.7902\n",
      "Epoch 25/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4331 - acc: 0.7924 - val_loss: 0.4391 - val_acc: 0.7901\n",
      "Epoch 26/50\n",
      "327472/327472 [==============================] - 81s 249us/step - loss: 0.4318 - acc: 0.7932 - val_loss: 0.4377 - val_acc: 0.7895\n",
      "Epoch 27/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4301 - acc: 0.7948 - val_loss: 0.4362 - val_acc: 0.7915\n",
      "Epoch 28/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4291 - acc: 0.7952 - val_loss: 0.4379 - val_acc: 0.7907\n",
      "Epoch 29/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4286 - acc: 0.7948 - val_loss: 0.4375 - val_acc: 0.7911\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4261 - acc: 0.7967 - val_loss: 0.4358 - val_acc: 0.7913\n",
      "Epoch 31/50\n",
      "327472/327472 [==============================] - 82s 252us/step - loss: 0.4250 - acc: 0.7970 - val_loss: 0.4342 - val_acc: 0.7912\n",
      "Epoch 32/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4256 - acc: 0.7969 - val_loss: 0.4340 - val_acc: 0.7917\n",
      "Epoch 33/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4251 - acc: 0.7972 - val_loss: 0.4354 - val_acc: 0.7912\n",
      "Epoch 34/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4239 - acc: 0.7985 - val_loss: 0.4362 - val_acc: 0.7899\n",
      "Epoch 35/50\n",
      "327472/327472 [==============================] - 82s 252us/step - loss: 0.4233 - acc: 0.7988 - val_loss: 0.4384 - val_acc: 0.7913\n",
      "Epoch 36/50\n",
      "327472/327472 [==============================] - 82s 249us/step - loss: 0.4228 - acc: 0.7994 - val_loss: 0.4360 - val_acc: 0.7937\n",
      "Epoch 37/50\n",
      "327472/327472 [==============================] - 83s 253us/step - loss: 0.4213 - acc: 0.8004 - val_loss: 0.4323 - val_acc: 0.7922\n",
      "Epoch 38/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4210 - acc: 0.8005 - val_loss: 0.4367 - val_acc: 0.7894\n",
      "Epoch 39/50\n",
      "327472/327472 [==============================] - 83s 252us/step - loss: 0.4205 - acc: 0.8001 - val_loss: 0.4352 - val_acc: 0.7920\n",
      "Epoch 40/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4193 - acc: 0.8005 - val_loss: 0.4325 - val_acc: 0.7918\n",
      "Epoch 41/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4199 - acc: 0.8005 - val_loss: 0.4315 - val_acc: 0.7940\n",
      "Epoch 42/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4180 - acc: 0.8016 - val_loss: 0.4314 - val_acc: 0.7935\n",
      "Epoch 43/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4184 - acc: 0.8019 - val_loss: 0.4328 - val_acc: 0.7930\n",
      "Epoch 44/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4184 - acc: 0.8014 - val_loss: 0.4327 - val_acc: 0.7935\n",
      "Epoch 45/50\n",
      "327472/327472 [==============================] - 82s 252us/step - loss: 0.4178 - acc: 0.8017 - val_loss: 0.4340 - val_acc: 0.7945\n",
      "Epoch 46/50\n",
      "327472/327472 [==============================] - 81s 246us/step - loss: 0.4164 - acc: 0.8027 - val_loss: 0.4316 - val_acc: 0.7944\n",
      "Epoch 47/50\n",
      "327472/327472 [==============================] - 81s 248us/step - loss: 0.4170 - acc: 0.8023 - val_loss: 0.4313 - val_acc: 0.7940\n",
      "Epoch 48/50\n",
      "327472/327472 [==============================] - 82s 251us/step - loss: 0.4159 - acc: 0.8026 - val_loss: 0.4314 - val_acc: 0.7955\n",
      "Epoch 49/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4168 - acc: 0.8026 - val_loss: 0.4312 - val_acc: 0.7940\n",
      "Epoch 50/50\n",
      "327472/327472 [==============================] - 82s 250us/step - loss: 0.4142 - acc: 0.8043 - val_loss: 0.4295 - val_acc: 0.7948\n",
      "Training ended at 2018-08-01 10:17:11.041367\n",
      "Minutes elapsed: 68.308900\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "save_data = {}\n",
    "for embed_name in w2v_embedding.keys():\n",
    "    current_embed = w2v_embedding[embed_name]\n",
    "    model = Max_BoE(current_embed)\n",
    "    MODEL_WEIGHTS_FILE = '/Users/zhang/MscProject_tweak2vec/Max_BOE_weights/'+embed_name+'_weights.h5'\n",
    "    print('current embedding: ',embed_name)\n",
    "    print(\"Starting training at\", datetime.datetime.now())\n",
    "    t0 = time.time()\n",
    "    callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "    model_history = model.fit([Q1_train, Q2_train],\n",
    "                        y_train,\n",
    "                        epochs=n_epoch,\n",
    "                        validation_split=val_split,\n",
    "                        verbose=1,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks)\n",
    "    save_data[embed_name] = model_history.history\n",
    "    t1 = time.time()\n",
    "    print(\"Training ended at\", datetime.datetime.now())\n",
    "    print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "    print(\"-------------------------------------------------------------------------\") \n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('retrain_data.txt','w')\n",
    "f.write(str(save_data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pivotsfull_alpha1_1m', 'pivotsfull_alpha10_1m', 'pivotsfull_alpha100_1m'])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora_data/pivotsfull_alpha_tune_data.txt','r')\n",
    "a = f.read()\n",
    "adata = eval(a)\n",
    "adata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pivotsfull_5m', 'pivotsfull_3m', 'pivotsfull_1m', 'pivotsfull_05m', 'pivotsfull_01m', 'pivotsfull_005m', 'pivotsfull_001m'])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora_data/pivotsfull_data.txt','r')\n",
    "a = f.read()\n",
    "fdata = eval(a)\n",
    "fdata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAF3CAYAAACbhOyeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiEgAl2iIiIiIiAfj/5xR+ETJ2IPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ad01f5ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.subplot(111)\n",
    "model_num = range(50)\n",
    "\n",
    "ax1.plot(model_num,adata['pivotsfull_alpha10_1m']['val_acc'], linestyle='-', label='α = 10·φ(w)')\n",
    "ax1.plot(model_num,fdata['pivotsfull_1m']['val_acc'], linestyle='-', label='α = 0.0001')\n",
    "ax1.legend(loc=0)\n",
    "ax1.set_xlabel('Epoch number')\n",
    "ax1.set_ylabel('Validation accuarcy')\n",
    "#plt.ylim(0.74,0.79)\n",
    "plt.show()\n",
    "fig.savefig('tune_lambda_in_alpha.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with best validation accuracy on the test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4198, accuracy = 0.7981\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_FILE = '/Users/zhang/MscProject_tweak2vec/Max_BOE_weights/quora_5m_weights.h5'\n",
    "\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "pred_y = model.predict([Q1_test, Q2_test], verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 25, 50)       1515000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 25, 50)       1515000     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 25, 50)       2550        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 25, 50)       2550        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 50)           0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 50)           0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100)          0           lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 200)          20200       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 200)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200)          800         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 200)          40200       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200)          800         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 200)          40200       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200)          800         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 200)          40200       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200)          800         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1)            201         batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,179,301\n",
      "Trainable params: 147,701\n",
      "Non-trainable params: 3,031,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
