{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zeros_padding(lst):\n",
    "    inner_mean_len = seq_length\n",
    "    result = np.zeros([len(lst), inner_mean_len])\n",
    "    for i, row in enumerate(lst):\n",
    "        for j, val in enumerate(row):\n",
    "            if j < inner_mean_len:\n",
    "                result[i][j] = val\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_to_lines(docs):\n",
    "    l = []\n",
    "    for doc in docs:\n",
    "        for line in doc:\n",
    "            l.append(line)\n",
    "    return l\n",
    "\n",
    "def to_one_of_k(int_targets, num_classes):\n",
    "    one_of_k_targets = np.zeros((np.array(int_targets).shape[0], num_classes))\n",
    "    one_of_k_targets[range(np.array(int_targets).shape[0]), int_targets] = 1\n",
    "    return one_of_k_targets\n",
    "\n",
    "        \n",
    "def get_batches(input_x, input_y, batch_size, isShuffle =  False):\n",
    "    n_batches = len(input_x) // batch_size\n",
    "    train_size = n_batches*batch_size\n",
    "    if isShuffle:\n",
    "        shuffle_idx = np.random.permutation(np.arange(len(input_x)))\n",
    "        train_x = input_x[shuffle_idx]\n",
    "        train_y = np.array(input_y)[shuffle_idx]\n",
    "    else: \n",
    "        train_x = input_x[:train_size]\n",
    "        train_y = np.array(input_y)[:train_size]\n",
    "    for idx in range(0, len(train_x), batch_size):\n",
    "        x = train_x[idx:idx+batch_size]\n",
    "        y = train_y[idx:idx+batch_size]\n",
    "        yy = to_one_of_k(y.astype(np.int32), num_classes)\n",
    "        yield x, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_google_50d = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_google_50d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_pubmed = {}\n",
    "# embedding_pubmed['retrain_2m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_2m.npy')\n",
    "# embedding_pubmed['retrain_1m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_1m.npy')\n",
    "# embedding_pubmed['retrain_05m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_05m.npy')\n",
    "# embedding_pubmed['retrain_01m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_01m.npy')\n",
    "# embedding_pubmed['retrain_005m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_005m.npy')\n",
    "# embedding_pubmed['retrain_001m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_001m.npy')\n",
    "\n",
    "embedding_pubmed['retrain_3m'] = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_retrain_3m.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_pivotsfull_3m (27188, 50)\n",
      "concat_pivotsfull_2m (27188, 50)\n",
      "concat_pivotsfull_1m (27188, 50)\n",
      "concat_pivotsfull_05m (27188, 50)\n",
      "concat_pivotsfull_01m (27188, 50)\n",
      "concat_pivotsfull_005m (27188, 50)\n",
      "concat_pivotsfull_001m (27188, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "embedding_pubmed_concat = {}\n",
    "for key in embedding_pubmed.keys():\n",
    "    key_str = 'concat_'+key\n",
    "    concat_vec = np.concatenate([w2v_google_50d, embedding_pubmed[key]], axis=1)\n",
    "    embedding_pubmed_concat[key_str]  = pca.fit_transform(concat_vec)\n",
    "    print(key_str,embedding_pubmed_concat[key_str].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "seq_length = 100 \n",
    "num_classes = 5  \n",
    "num_filters = 256  # number of kernels\n",
    "kernel_size = 5  \n",
    "vocab_size = len(embedding_pubmed['retrain_2m']) \n",
    "\n",
    "hidden_dim = 128  \n",
    "\n",
    "keep_prob_rate = 0.75\n",
    "learning_rate = 1e-3\n",
    "\n",
    "batch_size = 100  \n",
    "num_epoch = 8  \n",
    "\n",
    "print_per_batch = 100 \n",
    "save_per_batch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train_x = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_train_x.npy'\n",
    "file_train_y = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_train_y.npy'\n",
    "file_val_x = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_dev_x.npy'\n",
    "file_val_y = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_dev_y.npy'\n",
    "file_test_x = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_test_x.npy'\n",
    "file_test_y = '/Users/zhang/MscProject_tweak2vec/corpus/pubmed_test_y.npy'\n",
    "\n",
    "\n",
    "\n",
    "train_x = zeros_padding( docs_to_lines( np.load(file_train_x).tolist() ) )\n",
    "train_y = docs_to_lines( np.load(file_train_y).tolist() )\n",
    "val_x = zeros_padding( docs_to_lines( np.load(file_val_x).tolist() ) )\n",
    "val_y = to_one_of_k(docs_to_lines( np.load(file_val_y).tolist() ),5)\n",
    "test_x = zeros_padding( docs_to_lines( np.load(file_test_x).tolist() ) )\n",
    "test_y = to_one_of_k(docs_to_lines( np.load(file_test_y).tolist() ),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180037"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From <ipython-input-29-5a270814fff9>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n",
    "    input_y = tf.placeholder(tf.int32, [None, num_classes], name='input_y')\n",
    "    embedding = tf.placeholder(tf.float32, [vocab_size, embedding_dim], name='embedding')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        # embedding layer\n",
    "        embedding_inputs = tf.nn.embedding_lookup(embedding, input_x)\n",
    "    with tf.name_scope(\"CNN\"):\n",
    "        # CNN layer\n",
    "        conv1 = tf.layers.conv1d(inputs=embedding_inputs, filters=num_filters, \n",
    "                                 kernel_size=kernel_size, padding=\"VALID\", activation=tf.nn.relu,\n",
    "                                 activity_regularizer=tf.contrib.layers.l2_regularizer(0.001),)\n",
    "        # global maxpooling layer\n",
    "        pool1 = tf.reduce_max(conv1, reduction_indices=[1])\n",
    "        bn1 = tf.layers.batch_normalization(pool1)\n",
    "        \n",
    "        conv2 = tf.layers.conv1d(inputs=conv1, filters=num_filters, \n",
    "                                 kernel_size=kernel_size, padding=\"VALID\", activation=tf.nn.relu,\n",
    "                                 activity_regularizer=tf.contrib.layers.l2_regularizer(0.001),)\n",
    "        pool2 = tf.reduce_max(conv2, reduction_indices=[1])\n",
    "        bn2 = tf.layers.batch_normalization(pool2)    \n",
    "        \n",
    "    with tf.name_scope(\"classifier\"):\n",
    "        # fully connected layer\n",
    "        fc = tf.layers.dense(bn2, hidden_dim, name='fc1')\n",
    "        fc = tf.contrib.layers.dropout(fc, keep_prob)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        # classifier\n",
    "        logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "        y_pred_class = tf.argmax(tf.nn.softmax(logits), 1) \n",
    "    with tf.name_scope(\"optimize\"):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.argmax(input_y, 1), y_pred_class)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain_3m Starting training at 2018-08-02 11:09:11.393276\n",
      "Epoch 1/8 Iteration: 600 Avg. Training loss: 0.7569 Avg. Training acc: 0.7063 0.2056 sec/batch\n",
      "Epoch 1/8 Iteration: 1200 Avg. Training loss: 0.6325 Avg. Training acc: 0.7609 0.2005 sec/batch\n",
      "Epoch 1/8 Iteration: 1800 Avg. Training loss: 0.5985 Avg. Training acc: 0.7758 0.2004 sec/batch\n",
      "Epoch 1/8 Avg. Val. loss: 0.5711 Avg. Val. acc: 0.7853 30.3510 sec --------------------------------\n",
      "Epoch 2/8 Iteration: 2400 Avg. Training loss: 0.5529 Avg. Training acc: 0.7942 0.1963 sec/batch\n",
      "Epoch 2/8 Iteration: 3000 Avg. Training loss: 0.5504 Avg. Training acc: 0.7949 0.1960 sec/batch\n",
      "Epoch 2/8 Iteration: 3600 Avg. Training loss: 0.5517 Avg. Training acc: 0.7945 0.1956 sec/batch\n",
      "Epoch 2/8 Avg. Val. loss: 0.5532 Avg. Val. acc: 0.7934 24.0549 sec --------------------------------\n",
      "Epoch 3/8 Iteration: 4200 Avg. Training loss: 0.5004 Avg. Training acc: 0.8153 0.1955 sec/batch\n",
      "Epoch 3/8 Iteration: 4800 Avg. Training loss: 0.5045 Avg. Training acc: 0.8138 0.1959 sec/batch\n",
      "Epoch 3/8 Iteration: 5400 Avg. Training loss: 0.5029 Avg. Training acc: 0.8115 0.1952 sec/batch\n",
      "Epoch 3/8 Avg. Val. loss: 0.5312 Avg. Val. acc: 0.8020 24.7844 sec --------------------------------\n",
      "Epoch 4/8 Iteration: 6000 Avg. Training loss: 0.4451 Avg. Training acc: 0.8338 0.1942 sec/batch\n",
      "Epoch 4/8 Iteration: 6600 Avg. Training loss: 0.4544 Avg. Training acc: 0.8299 0.1946 sec/batch\n",
      "Epoch 4/8 Iteration: 7200 Avg. Training loss: 0.4661 Avg. Training acc: 0.8259 0.1952 sec/batch\n",
      "Epoch 4/8 Avg. Val. loss: 0.5406 Avg. Val. acc: 0.8031 25.1810 sec --------------------------------\n",
      "Epoch 5/8 Iteration: 7800 Avg. Training loss: 0.3913 Avg. Training acc: 0.8530 0.1946 sec/batch\n",
      "Epoch 5/8 Iteration: 8400 Avg. Training loss: 0.4084 Avg. Training acc: 0.8451 0.1950 sec/batch\n",
      "Epoch 5/8 Iteration: 9000 Avg. Training loss: 0.4169 Avg. Training acc: 0.8441 0.1948 sec/batch\n",
      "Epoch 5/8 Avg. Val. loss: 0.5570 Avg. Val. acc: 0.8010 25.3522 sec --------------------------------\n",
      "Epoch 6/8 Iteration: 9600 Avg. Training loss: 0.3345 Avg. Training acc: 0.8742 0.1940 sec/batch\n",
      "Epoch 6/8 Iteration: 10200 Avg. Training loss: 0.3572 Avg. Training acc: 0.8639 0.1946 sec/batch\n",
      "Epoch 6/8 Iteration: 10800 Avg. Training loss: 0.3767 Avg. Training acc: 0.8574 0.1941 sec/batch\n",
      "Epoch 6/8 Avg. Val. loss: 0.6085 Avg. Val. acc: 0.7935 23.8435 sec --------------------------------\n",
      "Epoch 7/8 Iteration: 11400 Avg. Training loss: 0.2874 Avg. Training acc: 0.8903 0.1931 sec/batch\n",
      "Epoch 7/8 Iteration: 12000 Avg. Training loss: 0.3146 Avg. Training acc: 0.8811 0.1950 sec/batch\n",
      "Epoch 7/8 Iteration: 12600 Avg. Training loss: 0.3290 Avg. Training acc: 0.8727 0.1997 sec/batch\n",
      "Epoch 7/8 Avg. Val. loss: 0.6554 Avg. Val. acc: 0.7874 29.5914 sec --------------------------------\n",
      "Epoch 8/8 Iteration: 13200 Avg. Training loss: 0.2438 Avg. Training acc: 0.9048 0.1981 sec/batch\n",
      "Epoch 8/8 Iteration: 13800 Avg. Training loss: 0.2689 Avg. Training acc: 0.8948 0.2057 sec/batch\n",
      "Epoch 8/8 Iteration: 14400 Avg. Training loss: 0.2951 Avg. Training acc: 0.8853 0.2056 sec/batch\n",
      "Epoch 8/8 Avg. Val. loss: 0.7910 Avg. Val. acc: 0.7836 27.2775 sec --------------------------------\n",
      "Finish at 2018-08-02 12:00:00.339659\n"
     ]
    }
   ],
   "source": [
    "# for embed_type in embedding_pubmed.keys():\n",
    "for embed_type in embedding_pubmed.keys():\n",
    "\n",
    "    print( embed_type + \" Starting training at\", datetime.datetime.now())\n",
    "    #sess = tf.InteractiveSession()\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        save_path = saver.save(sess,'/Users/zhang/MscProject_tweak2vec/Pubmed_save/'+embed_type+'_model.ckpt')\n",
    "         \n",
    "\n",
    "        t_loss = 0\n",
    "        t_acc = 0\n",
    "        v_best_acc = 0\n",
    "        iteration = 0\n",
    "        train_batches_size = len(train_x) // batch_size\n",
    "        val_batches_size = len(val_x) // batch_size\n",
    "\n",
    "        for e in range(1, num_epoch+1):\n",
    "            train_batches = get_batches(train_x, train_y, batch_size, isShuffle =  True)\n",
    "            val_batches = get_batches(val_x, val_y, batch_size)\n",
    "            \n",
    "\n",
    "            start = time.time()  \n",
    "            # training\n",
    "            for train_inputs, train_targets in train_batches:\n",
    "                iteration += 1\n",
    "                feed = {input_x: train_inputs,\n",
    "                        input_y: train_targets,\n",
    "                        embedding: embedding_pubmed[embed_type],\n",
    "                        keep_prob: keep_prob_rate}\n",
    "                train_loss, train_acc, _ = sess.run([loss, accuracy, optimizer], feed_dict=feed)\n",
    "                t_loss += train_loss\n",
    "                t_acc += train_acc\n",
    "                if iteration % 600 == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"Epoch {}/{}\".format(e, num_epoch),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(t_loss / 600),\n",
    "                          \"Avg. Training acc: {:.4f}\".format(t_acc / 600),\n",
    "                          \"{:.4f} sec/batch\".format((end - start) / 600))      \n",
    "                    t_loss = 0\n",
    "                    t_acc = 0\n",
    "                    start = time.time()\n",
    "            train_loss_lst.append(train_loss)\n",
    "            train_acc_lst.append(train_acc)\n",
    "\n",
    "\n",
    "            # validation\n",
    "            feed = {input_x: val_x,\n",
    "                    input_y: val_y,\n",
    "                    embedding: embedding_pubmed[embed_type],\n",
    "                    keep_prob: 1}\n",
    "            val_loss, val_acc, y_pred = sess.run([loss, accuracy, y_pred_class], feed_dict=feed)\n",
    "            if val_acc > v_best_acc:\n",
    "                v_best_acc = val_acc\n",
    "                y_predict = y_pred\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"Epoch {}/{}\".format(e, num_epoch),\n",
    "                  \"Avg. Val. loss: {:.4f}\".format(val_loss),\n",
    "                  \"Avg. Val. acc: {:.4f}\".format(val_acc),\n",
    "                  \"{:.4f} sec\".format(end - start),\n",
    "                  \"--------------------------------\")\n",
    "            val_loss_lst.append(val_loss)\n",
    "            val_acc_lst.append(val_acc)\n",
    "        y_label = docs_to_lines( np.load(file_val_y).tolist() )\n",
    "        confusion_mat = tf.confusion_matrix(y_label, y_predict, 5)\n",
    "        confusion = sess.run(confusion_mat)\n",
    "\n",
    "\n",
    "    print(\"Finish at\", datetime.datetime.now())\n",
    "\n",
    "    np.save(embed_type+'_train_acc.npy',np.array(train_acc_lst))\n",
    "    np.save(embed_type+'_train_loss.npy',np.array(train_loss_lst))\n",
    "    np.save(embed_type+'_val_acc.npy',np.array(val_acc_lst))\n",
    "    np.save(embed_type+'_val_loss.npy',np.array(val_loss_lst))\n",
    "    np.save(embed_type+'_confusion.npy',confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2191  159  274   95  730]\n",
      " [ 849 1017  189   39  282]\n",
      " [ 155   32 8872  720  185]\n",
      " [  81    0  696 8529  535]\n",
      " [ 424    4  230  514 3410]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    y_label = docs_to_lines( np.load(file_val_y).tolist() )\n",
    "    confusion_mat = tf.confusion_matrix(y_label, y_predict, 5)\n",
    "    confusion = sess.run(confusion_mat)\n",
    "    print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
