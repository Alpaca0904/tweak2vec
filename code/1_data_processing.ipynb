{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests_html import HTML\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_question = pd.read_csv('/Users/zhang/MscProject_tweak2vec/data_stacksample/Questions.csv',encoding='ISO-8859-1')\n",
    "all_tag = pd.read_csv('/Users/zhang/MscProject_tweak2vec/data_stacksample/Tags.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus r:50k   py:50k   js:50k   cpp:25k   php:25k\n",
    "\n",
    "cpp_tag = all_tag[all_tag.Tag=='c++']\n",
    "cpp_question = pd.merge(cpp_tag,all_question,on='Id')\n",
    "\n",
    "js_tag = all_tag[all_tag.Tag=='javascript']\n",
    "js_question = pd.merge(js_tag,all_question,on='Id')\n",
    "\n",
    "php_tag = all_tag[all_tag.Tag=='php']\n",
    "php_question = pd.merge(php_tag,all_question,on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_question = pd.read_csv('/Users/zhang/MscProject_tweak2vec/rquestions/Questions.csv', usecols=['Title','Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_question = pd.read_csv('/Users/zhang/MscProject_tweak2vec/pythonquestions/Questions.csv',encoding='ISO-8859-1',usecols=['Title','Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_question['Text'] = r_question['Title'] + r_question['Body']\n",
    "py_question['Text'] = py_question['Title'] + py_question['Body']\n",
    "js_question['Text'] = js_question['Title'] + js_question['Body']\n",
    "cpp_question['Text'] = cpp_question['Title'] + cpp_question['Body']\n",
    "php_question['Text'] = php_question['Title'] + php_question['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_question_list = np.array(r_question['Text'])\n",
    "py_question_list = np.array(py_question['Text'])\n",
    "js_question_list = np.array(js_question['Text'])\n",
    "cpp_question_list = np.array(cpp_question['Text'])\n",
    "php_question_list = np.array(php_question['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc2tokens(data):\n",
    "    doc = HTML(html=data).text #remove html label\n",
    "    doc = ''.join(' ' if c in punctuation else c for c in doc) #remove punctuations\n",
    "    tokens = doc.split()\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def tokens2doc(tokens, vocab_list):\n",
    "    tokens = ['UNK' if w not in vocab_list else w for w in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def update_vocab(tokens, vocab):\n",
    "    tokens = clean_doc2tokens(data)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "\n",
    "# create vocab (clean data, split tokens)\n",
    "# create clean corpus (clean data again, replace UNK)\n",
    "# train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(corpus):\n",
    "    corpus_nounk = list()\n",
    "    vocab = Counter()\n",
    "    loop = 1\n",
    "    for data in corpus:\n",
    "        if loop%10000 == 0:\n",
    "            print(loop)\n",
    "        tokens = clean_doc2tokens(data)\n",
    "        corpus_nounk.append(tokens)\n",
    "        vocab.update(tokens)\n",
    "        loop = loop + 1\n",
    "    return corpus_nounk,vocab\n",
    "\n",
    "def create_vocab_list(vocab, min_occurrence):\n",
    "    vocab_freq_list = [[k,c] for k,c in vocab.most_common() if c >= min_occurrence]\n",
    "    return vocab_freq_list\n",
    "\n",
    "def create_clean_corpus(corpus_nounk, vocab_list):\n",
    "    corpus_withunk = list()\n",
    "    loop = 1\n",
    "    for data in corpus_nounk:\n",
    "        if loop%10000 == 0:\n",
    "            print(loop)\n",
    "        loop = loop + 1\n",
    "        tokens = ['UNK' if w not in vocab_list else w for w in data]\n",
    "        corpus_withunk.append(tokens)\n",
    "    return corpus_withunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# corpus r:100k   py:100k   js:50k   cpp:25k   php:25k\n",
    "cop_r = r_question_list[0:50000]\n",
    "cop_py = py_question_list[0:50000]\n",
    "cop_js = js_question_list[0:50000]\n",
    "cop_cpp = cpp_question_list[0:25000]\n",
    "cop_php = php_question_list[0:25000]\n",
    "cop = (cop_r, cop_py, cop_js, cop_cpp, cop_php)\n",
    "\n",
    "corpus = np.concatenate(cop, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "corpus_nounk, vocab = create_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('stack_vocaball.npy', vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47614"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_occurrence = 10\n",
    "vocab_freq_list = create_vocab_list(vocab, min_occurrence)\n",
    "vocab_list = [w[0] for w in vocab_freq_list]\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('stack_vocab10.npy',np.array(vocab_freq_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "corpus_withunk = create_clean_corpus(corpus_nounk, vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('stack_corpus_withunk.npy',np.array(corpus_withunk))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
