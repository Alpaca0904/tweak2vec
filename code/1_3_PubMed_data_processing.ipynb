{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open('/Users/zhang/MscProject_tweak2vec/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt','r')\n",
    "dev_file = open('/Users/zhang/MscProject_tweak2vec/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt','r')\n",
    "test_file = open('/Users/zhang/MscProject_tweak2vec/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_docs(filename):\n",
    "    file = filename\n",
    "    lines = file.readlines()\n",
    "    start = 0\n",
    "    end = 0\n",
    "    docs = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i] == '\\n':\n",
    "            end = i\n",
    "            docs.append(lines[start:end])\n",
    "            start = i+1\n",
    "    return docs\n",
    "\n",
    "def docs_to_xy(docs, add_unk = False, vocab_list = []):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "    for doc in docs:\n",
    "        doc_x = []\n",
    "        doc_y = []\n",
    "        for sentence in doc:\n",
    "            sentence = re.sub(r\"\\n\", \" \", sentence)\n",
    "            sentence = re.sub(r\"\\t\", \" \", sentence)\n",
    "            #sentence = re.sub(r\"@\", \" @ \", sentence)\n",
    "            sentence = sentence.split()\n",
    "            sentence = [c for c in sentence if c not in punctuation]\n",
    "            sentence = [c for c in sentence if c not in stop_words]\n",
    "            sentence = [c.lower() for c in sentence]\n",
    "            if sentence[0][:3]!='###':\n",
    "                if add_unk:\n",
    "                    sentence = ['UNK' if c not in vocab_list else c for c in sentence]\n",
    "                if len(sentence)>1:\n",
    "                    doc_y.append(sentence[0])\n",
    "                    doc_x.append(' '.join(sentence[1:]))\n",
    "        docs_x.append(doc_x)\n",
    "        docs_y.append(doc_y)\n",
    "    return docs_x, docs_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs = file_to_docs(train_file)\n",
    "dev_docs = file_to_docs(dev_file)\n",
    "test_docs = file_to_docs(test_file)\n",
    "\n",
    "train_x, train_y = docs_to_xy(train_docs)\n",
    "dev_x, dev_y = docs_to_xy(dev_docs)\n",
    "test_x, test_y = docs_to_xy(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = np.concatenate([train_x,dev_x],axis=0).tolist()\n",
    "b = str(corpus)\n",
    "b = b.replace('[','')\n",
    "b = b.replace(']','')\n",
    "corpus = list(eval(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for i in corpus:\n",
    "    tokens = i.split()\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27187\n",
      "27188\n"
     ]
    }
   ],
   "source": [
    "vocab_list = [k for k,c in vocab.most_common() if c >= 5]\n",
    "vocab_freq_list = [[k,c] for k,c in vocab.most_common() if c >= 5]\n",
    "print(len(vocab_list))\n",
    "vocab_list.append('UNK')\n",
    "word2int = {word: ii for ii, word in enumerate(vocab_list)}\n",
    "lable2int = {'background':0,\n",
    "             'objective':1,\n",
    "             'methods':2,\n",
    "             'results':3,\n",
    "             'conclusions':4}\n",
    "print(len(vocab_list))\n",
    "np.save('pubmed_vocab5.npy',np.array(vocab_list))\n",
    "np.save('pubmed_vocab5_frq.npy',np.array(vocab_freq_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus with unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "train_x_unk, train_y_unk = docs_to_xy(train_docs,True,vocab_list)\n",
    "print('train')\n",
    "dev_x_unk, dev_y_unk = docs_to_xy(dev_docs,True,vocab_list)\n",
    "print('dev')\n",
    "test_x_unk, test_y_unk = docs_to_xy(test_docs,True,vocab_list)\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_unk = np.concatenate([train_x_unk,dev_x_unk],axis=0).tolist()\n",
    "b = str(corpus_unk)\n",
    "b = b.replace('[','')\n",
    "b = b.replace(']','')\n",
    "corpus_unk = list(eval(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus of int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_int = []\n",
    "for line in corpus_unk:\n",
    "    corpus_int.append([word2int[w] for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('pubmed_corpus_int5.npy',np.array(corpus_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x_to_int(docs, word2int):\n",
    "    ints_x = []\n",
    "    for doc in docs:\n",
    "        int_x = []\n",
    "        for sentence in doc:\n",
    "            int_x.append([word2int[word] for word in sentence.split()])\n",
    "        ints_x.append(int_x)   \n",
    "    return ints_x\n",
    "\n",
    "def y_to_int(docs, word2int):\n",
    "    ints_y = []\n",
    "    for doc in docs:\n",
    "        int_y = []\n",
    "        for lable in doc:\n",
    "            int_y.append(word2int[lable])\n",
    "        ints_y.append(int_y)   \n",
    "    return ints_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_int = x_to_int(train_x_unk, word2int)\n",
    "train_y_int = y_to_int(train_y_unk, lable2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_x_int = x_to_int(dev_x_unk, word2int)\n",
    "dev_y_int = y_to_int(dev_y_unk, lable2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_int = x_to_int(test_x_unk, word2int)\n",
    "test_y_int = y_to_int(test_y_unk, lable2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('pubmed_train_x',np.array(train_x_int))\n",
    "np.save('pubmed_train_y',np.array(train_y_int))\n",
    "np.save('pubmed_dev_x',np.array(dev_x_int))\n",
    "np.save('pubmed_dev_y',np.array(dev_y_int))\n",
    "np.save('pubmed_test_x',np.array(test_x_int))\n",
    "np.save('pubmed_test_y',np.array(test_y_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_model_full = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/zhang/MscProject_tweak2vec/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_wordVec = []\n",
    "for word in vocab_list:\n",
    "    if word in google_model_full:\n",
    "        google_wordVec.append(google_model_full[word])\n",
    "    if word not in google_model_full:\n",
    "        google_wordVec.append(google_model_full['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27188, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "google_wordVec_50 = pca.fit_transform(google_wordVec)\n",
    "google_wordVec_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('w2v_google_50d.npy',np.array(google_wordVec_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivots selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_model_40k = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/zhang/MscProject_tweak2vec/GoogleNews-vectors-negative300.bin',binary=True, limit=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8327"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivots = []\n",
    "for word in word2int.keys():\n",
    "    if word in google_model_40k:\n",
    "        pivots.append(word)\n",
    "len(pivots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_at_pivots = pivots[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivots_vec = {}\n",
    "pivots_size = 5000\n",
    "for i in range(pivots_size):\n",
    "    pivots_vec[ word2int[remove_at_pivots[i]] ] = google_wordVec_50[word2int[remove_at_pivots[i]]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('pubmed_pivots_google_500.txt','w')  \n",
    "f.write(str(pivots_vec))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
