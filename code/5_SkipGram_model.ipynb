{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size, stop_size):\n",
    "    idx_pairs = []\n",
    "    tokens = 0\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            tokens += 1\n",
    "            if tokens >= stop_size:\n",
    "                return idx_pairs, tokens\n",
    "            else:\n",
    "                # for each context word within window\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = sentence[context_word_pos]\n",
    "                    idx_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "                    \n",
    "    return idx_pairs, tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 26324522 word pairs\n",
      "totally 3258438 tokens\n"
     ]
    }
   ],
   "source": [
    "# corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "\n",
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/pubmed_corpus_int5.npy\").tolist()\n",
    "\n",
    "corpus_shuffle = corpus[:]\n",
    "\n",
    "random.shuffle(corpus_shuffle)\n",
    "quora_idx_pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size=7000000)\n",
    "print('totally {0} word pairs'.format(len(quora_idx_pairs)))\n",
    "print('totally {0} tokens'.format(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split Quora corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 66209 word pairs\n",
      "totally 10000 tokens\n",
      "totally 328653 word pairs\n",
      "totally 50000 tokens\n",
      "totally 659424 word pairs\n",
      "totally 100000 tokens\n",
      "totally 3301805 word pairs\n",
      "totally 500000 tokens\n",
      "totally 6593213 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 19783540 word pairs\n",
      "totally 3000000 tokens\n",
      "totally 32976829 word pairs\n",
      "totally 5000000 tokens\n"
     ]
    }
   ],
   "source": [
    "quo_tokens_lst = [0.01, 0.05, 0.1, 0.5, 1, 3, 5]\n",
    "quo_idx_pairs = []\n",
    "for i in quo_tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    quo_idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split PubMed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 80440 word pairs\n",
      "totally 10000 tokens\n",
      "totally 403227 word pairs\n",
      "totally 50000 tokens\n",
      "totally 806176 word pairs\n",
      "totally 100000 tokens\n",
      "totally 4042017 word pairs\n",
      "totally 500000 tokens\n",
      "totally 8074776 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 16161575 word pairs\n",
      "totally 2000000 tokens\n",
      "totally 24239608 word pairs\n",
      "totally 3000000 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [0.01,0.05,0.1,0.5,1,2,3]\n",
    "idx_pairs = []\n",
    "for i in tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "# wordlist.append(['UNK',0])\n",
    "# word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "# idx2word = {wordlist.index(w): w[0] for w in wordlist }\n",
    "\n",
    "\n",
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_vocab5.npy').tolist()\n",
    "wordlist.append('UNK')\n",
    "word2idx = {w: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivots slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 485 pivot words\n",
      "load 5075 pivot words\n",
      "load 9343 pivot words\n"
     ]
    }
   ],
   "source": [
    "q_pivots_dict = {}\n",
    "for i in [1,10,20]:\n",
    "    f = open('/Users/zhang/MscProject_tweak2vec/corpus/quora_pivot_'+str(i)+'.txt','r')\n",
    "    a = f.read()\n",
    "    q_pivots_dict[i] = eval(a)\n",
    "    f.close()\n",
    "    print('load {0} pivot words'.format(len(q_pivots_dict[i].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_pivots_idx = {}\n",
    "q_pivots_vec = {}\n",
    "for i in [1,10,20]:\n",
    "    q_pivots_idx[i] = []\n",
    "    q_pivots_vec[i] = []\n",
    "    for p in q_pivots_dict[i].keys():\n",
    "        q_pivots_idx[i].append(p)\n",
    "        q_pivots_vec[i].append(q_pivots_dict[i][p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9343, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cur_pivots_vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_pivots_idx = q_pivots_idx[20]\n",
    "cur_pivots_vec = q_pivots_vec[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 16867 pivot words from quora corpus\n"
     ]
    }
   ],
   "source": [
    "f = open('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_pivots_google_full.txt','r')\n",
    "a = f.read()\n",
    "full_pivots_dict = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words from quora corpus'.format(len(full_pivots_dict.keys())))\n",
    "\n",
    "full_pivots_idx = []\n",
    "full_pivots_vec = []\n",
    "for w in full_pivots_dict.keys():\n",
    "    full_pivots_idx.append(w)\n",
    "    full_pivots_vec.append(full_pivots_dict[w])\n",
    "cur_pivots_idx = full_pivots_idx\n",
    "cur_pivots_vec = full_pivots_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27188,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = np.load('pubmed_alpha.npy')\n",
    "alpha = alpha.reshape(len(alpha),1)\n",
    "alpha = alpha * 1000000\n",
    "alpha = alpha.reshape(len(alpha))\n",
    "alpha.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------ closed --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "def get_pivots_slice(pivots_dict, size):\n",
    "    pivots = deepcopy(pivots_dict)\n",
    "    pivots_slice = dict_slice(pivots, 0, size)\n",
    "    pivots_idx = []\n",
    "    pivots_vec = []\n",
    "    for i in pivots_slice.keys():\n",
    "        pivots_idx.append(i)\n",
    "        pivots_vec.append(pivots_slice[i])\n",
    "    return pivots_idx, pivots_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pivots = 10000\n",
    "pivots_idx, pivots_vec = get_pivots_slice(pivots_dict, n_pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------ closed --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google_pretrain = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/tokens/w2v_quoragoogle_50d.npy')\n",
    "google_pretrain = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/pubmed/w2v_pubmedgoogle_50d.npy')\n",
    "google_pretrain = np.float32(google_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quora(30300),pubmed(27188)\n",
      "current vocab word: 27188\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 0.0001\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1000 # number of samples each iteration\n",
    "print('quora(30300),pubmed(27188)')\n",
    "print('current vocab word:',n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "#     alpha_table = tf.placeholder(tf.float32, [n_vocab], name='alpha')\n",
    "\n",
    "    \n",
    "    # embedding layer\n",
    "#     init_width = 0.5 / n_embedding\n",
    "#     embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "    embedding = tf.get_variable(initializer=google_pretrain, name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "#     # add regularization term\n",
    "#     embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "#     update_embed_op = tf.scatter_update(embedding_copy, cur_pivots_idx, cur_pivots_vec)\n",
    "#     embed_copy = tf.nn.embedding_lookup(embedding_copy, inputs)\n",
    "#     alpha_subet = tf.nn.embedding_lookup(alpha_table, inputs)\n",
    "#     alpha_subet = tf.cast(alpha_subet, tf.float32)\n",
    "# #     reg_loss = reg_constant * tf.reduce_sum((embed-embed_copy)**2)\n",
    "#     w_diff = tf.reduce_sum((embed-embed_copy)**2,1)\n",
    "#     alpha_diff = tf.multiply(alpha_subet, w_diff)\n",
    "#     reg_loss = tf.reduce_sum( alpha_diff )\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    total_cost = cost \n",
    "#     total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  10000.0\n",
      "Starting training at  2018-08-01 21:06:30.806721\n",
      "Finish training at  2018-08-01 21:06:46.200018\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  50000.0\n",
      "Starting training at  2018-08-01 21:06:46.200382\n",
      "Epoch 3/10 Iteration: 1000 Avg. Training loss: 7.6915 0.0036 sec/batch\n",
      "Epoch 5/10 Iteration: 2000 Avg. Training loss: 6.1672 0.0072 sec/batch\n",
      "Epoch 8/10 Iteration: 3000 Avg. Training loss: 5.2405 0.0032 sec/batch\n",
      "Epoch 10/10 Iteration: 4000 Avg. Training loss: 4.7014 0.0066 sec/batch\n",
      "Finish training at  2018-08-01 21:08:44.908590\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  100000.0\n",
      "Starting training at  2018-08-01 21:08:44.915422\n",
      "Epoch 2/10 Iteration: 1000 Avg. Training loss: 7.6544 0.0036 sec/batch\n",
      "Epoch 3/10 Iteration: 2000 Avg. Training loss: 6.2636 0.0070 sec/batch\n",
      "Epoch 4/10 Iteration: 3000 Avg. Training loss: 5.4510 0.0106 sec/batch\n",
      "Epoch 5/10 Iteration: 4000 Avg. Training loss: 4.9672 0.0139 sec/batch\n",
      "Epoch 7/10 Iteration: 5000 Avg. Training loss: 4.6638 0.0027 sec/batch\n",
      "Epoch 8/10 Iteration: 6000 Avg. Training loss: 4.4514 0.0065 sec/batch\n",
      "Epoch 9/10 Iteration: 7000 Avg. Training loss: 4.2759 0.0101 sec/batch\n",
      "Epoch 10/10 Iteration: 8000 Avg. Training loss: 4.1505 0.0126 sec/batch\n",
      "Finish training at  2018-08-01 21:12:48.143921\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  500000.0\n",
      "Starting training at  2018-08-01 21:12:48.153136\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 7.6844 0.0669 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 6.3648 0.0182 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.6235 0.0184 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 5.1953 0.0181 sec/batch\n",
      "Epoch 2/10 Iteration: 5000 Avg. Training loss: 4.9268 0.0177 sec/batch\n",
      "Epoch 2/10 Iteration: 6000 Avg. Training loss: 4.7353 0.0680 sec/batch\n",
      "Epoch 2/10 Iteration: 7000 Avg. Training loss: 4.6243 0.0190 sec/batch\n",
      "Epoch 2/10 Iteration: 8000 Avg. Training loss: 4.5273 0.0186 sec/batch\n",
      "Epoch 3/10 Iteration: 9000 Avg. Training loss: 4.4337 0.0166 sec/batch\n",
      "Epoch 3/10 Iteration: 10000 Avg. Training loss: 4.3574 0.0186 sec/batch\n",
      "Epoch 3/10 Iteration: 11000 Avg. Training loss: 4.3221 0.0175 sec/batch\n",
      "Epoch 3/10 Iteration: 12000 Avg. Training loss: 4.2682 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 13000 Avg. Training loss: 4.2218 0.0158 sec/batch\n",
      "Epoch 4/10 Iteration: 14000 Avg. Training loss: 4.1706 0.0692 sec/batch\n",
      "Epoch 4/10 Iteration: 15000 Avg. Training loss: 4.1536 0.0180 sec/batch\n",
      "Epoch 4/10 Iteration: 16000 Avg. Training loss: 4.1256 0.0179 sec/batch\n",
      "Epoch 5/10 Iteration: 17000 Avg. Training loss: 4.0975 0.0150 sec/batch\n",
      "Epoch 5/10 Iteration: 18000 Avg. Training loss: 4.0639 0.0181 sec/batch\n",
      "Epoch 5/10 Iteration: 19000 Avg. Training loss: 4.0528 0.0200 sec/batch\n",
      "Epoch 5/10 Iteration: 20000 Avg. Training loss: 4.0373 0.0185 sec/batch\n",
      "Epoch 6/10 Iteration: 21000 Avg. Training loss: 4.0154 0.0141 sec/batch\n",
      "Epoch 6/10 Iteration: 22000 Avg. Training loss: 3.9929 0.0193 sec/batch\n",
      "Epoch 6/10 Iteration: 23000 Avg. Training loss: 3.9814 0.0210 sec/batch\n",
      "Epoch 6/10 Iteration: 24000 Avg. Training loss: 3.9753 0.0202 sec/batch\n",
      "Epoch 7/10 Iteration: 25000 Avg. Training loss: 3.9581 0.0160 sec/batch\n",
      "Epoch 7/10 Iteration: 26000 Avg. Training loss: 3.9433 0.0232 sec/batch\n",
      "Epoch 7/10 Iteration: 27000 Avg. Training loss: 3.9335 0.0214 sec/batch\n",
      "Epoch 7/10 Iteration: 28000 Avg. Training loss: 3.9338 0.0210 sec/batch\n",
      "Epoch 8/10 Iteration: 29000 Avg. Training loss: 3.9083 0.0164 sec/batch\n",
      "Epoch 8/10 Iteration: 30000 Avg. Training loss: 3.8915 0.0243 sec/batch\n",
      "Epoch 8/10 Iteration: 31000 Avg. Training loss: 3.8831 0.0265 sec/batch\n",
      "Epoch 8/10 Iteration: 32000 Avg. Training loss: 3.8847 0.0195 sec/batch\n",
      "Epoch 9/10 Iteration: 33000 Avg. Training loss: 3.8791 0.0143 sec/batch\n",
      "Epoch 9/10 Iteration: 34000 Avg. Training loss: 3.8595 0.0210 sec/batch\n",
      "Epoch 9/10 Iteration: 35000 Avg. Training loss: 3.8474 0.0235 sec/batch\n",
      "Epoch 9/10 Iteration: 36000 Avg. Training loss: 3.8654 0.0226 sec/batch\n",
      "Epoch 10/10 Iteration: 37000 Avg. Training loss: 3.8427 0.0138 sec/batch\n",
      "Epoch 10/10 Iteration: 38000 Avg. Training loss: 3.8227 0.0214 sec/batch\n",
      "Epoch 10/10 Iteration: 39000 Avg. Training loss: 3.8183 0.0215 sec/batch\n",
      "Epoch 10/10 Iteration: 40000 Avg. Training loss: 3.8231 0.0200 sec/batch\n",
      "Finish training at  2018-08-01 21:28:46.649316\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  1000000\n",
      "Starting training at  2018-08-01 21:28:46.656789\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 7.8481 0.0838 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 6.3885 0.0226 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.6188 0.0206 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 5.1973 0.0201 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.9744 0.0219 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.8058 0.0779 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.6733 0.0221 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.5769 0.0213 sec/batch\n",
      "Epoch 2/10 Iteration: 9000 Avg. Training loss: 4.4773 0.0172 sec/batch\n",
      "Epoch 2/10 Iteration: 10000 Avg. Training loss: 4.4035 0.0190 sec/batch\n",
      "Epoch 2/10 Iteration: 11000 Avg. Training loss: 4.3585 0.0180 sec/batch\n",
      "Epoch 2/10 Iteration: 12000 Avg. Training loss: 4.3066 0.0698 sec/batch\n",
      "Epoch 2/10 Iteration: 13000 Avg. Training loss: 4.2800 0.0188 sec/batch\n",
      "Epoch 2/10 Iteration: 14000 Avg. Training loss: 4.2628 0.0186 sec/batch\n",
      "Epoch 2/10 Iteration: 15000 Avg. Training loss: 4.2321 0.0185 sec/batch\n",
      "Epoch 2/10 Iteration: 16000 Avg. Training loss: 4.1996 0.0191 sec/batch\n",
      "Epoch 3/10 Iteration: 17000 Avg. Training loss: 4.1614 0.0158 sec/batch\n",
      "Epoch 3/10 Iteration: 18000 Avg. Training loss: 4.1440 0.0183 sec/batch\n",
      "Epoch 3/10 Iteration: 19000 Avg. Training loss: 4.1248 0.0182 sec/batch\n",
      "Epoch 3/10 Iteration: 20000 Avg. Training loss: 4.0993 0.0181 sec/batch\n",
      "Epoch 3/10 Iteration: 21000 Avg. Training loss: 4.0985 0.0186 sec/batch\n",
      "Epoch 3/10 Iteration: 22000 Avg. Training loss: 4.0814 0.0192 sec/batch\n",
      "Epoch 3/10 Iteration: 23000 Avg. Training loss: 4.0699 0.0187 sec/batch\n",
      "Epoch 3/10 Iteration: 24000 Avg. Training loss: 4.0492 0.0187 sec/batch\n",
      "Epoch 4/10 Iteration: 25000 Avg. Training loss: 4.0439 0.0147 sec/batch\n",
      "Epoch 4/10 Iteration: 26000 Avg. Training loss: 4.0243 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 27000 Avg. Training loss: 4.0118 0.0185 sec/batch\n",
      "Epoch 4/10 Iteration: 28000 Avg. Training loss: 3.9927 0.0181 sec/batch\n",
      "Epoch 4/10 Iteration: 29000 Avg. Training loss: 4.0027 0.0187 sec/batch\n",
      "Epoch 4/10 Iteration: 30000 Avg. Training loss: 3.9992 0.0187 sec/batch\n",
      "Epoch 4/10 Iteration: 31000 Avg. Training loss: 3.9831 0.0189 sec/batch\n",
      "Epoch 4/10 Iteration: 32000 Avg. Training loss: 3.9756 0.0190 sec/batch\n",
      "Epoch 5/10 Iteration: 33000 Avg. Training loss: 3.9641 0.0133 sec/batch\n",
      "Epoch 5/10 Iteration: 34000 Avg. Training loss: 3.9540 0.0186 sec/batch\n",
      "Epoch 5/10 Iteration: 35000 Avg. Training loss: 3.9425 0.0187 sec/batch\n",
      "Epoch 5/10 Iteration: 36000 Avg. Training loss: 3.9355 0.0188 sec/batch\n",
      "Epoch 5/10 Iteration: 37000 Avg. Training loss: 3.9388 0.0186 sec/batch\n",
      "Epoch 5/10 Iteration: 38000 Avg. Training loss: 3.9435 0.0188 sec/batch\n",
      "Epoch 5/10 Iteration: 39000 Avg. Training loss: 3.9362 0.0183 sec/batch\n",
      "Epoch 5/10 Iteration: 40000 Avg. Training loss: 3.9198 0.0182 sec/batch\n",
      "Epoch 6/10 Iteration: 41000 Avg. Training loss: 3.9173 0.0115 sec/batch\n",
      "Epoch 6/10 Iteration: 42000 Avg. Training loss: 3.9074 0.0173 sec/batch\n",
      "Epoch 6/10 Iteration: 43000 Avg. Training loss: 3.9040 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 44000 Avg. Training loss: 3.8914 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 45000 Avg. Training loss: 3.8964 0.0185 sec/batch\n",
      "Epoch 6/10 Iteration: 46000 Avg. Training loss: 3.8998 0.0188 sec/batch\n",
      "Epoch 6/10 Iteration: 47000 Avg. Training loss: 3.8903 0.0183 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Iteration: 48000 Avg. Training loss: 3.8829 0.0186 sec/batch\n",
      "Epoch 7/10 Iteration: 49000 Avg. Training loss: 3.8783 0.0105 sec/batch\n",
      "Epoch 7/10 Iteration: 50000 Avg. Training loss: 3.8736 0.0185 sec/batch\n",
      "Epoch 7/10 Iteration: 51000 Avg. Training loss: 3.8696 0.0185 sec/batch\n",
      "Epoch 7/10 Iteration: 52000 Avg. Training loss: 3.8579 0.0188 sec/batch\n",
      "Epoch 7/10 Iteration: 53000 Avg. Training loss: 3.8631 0.0187 sec/batch\n",
      "Epoch 7/10 Iteration: 54000 Avg. Training loss: 3.8554 0.0183 sec/batch\n",
      "Epoch 7/10 Iteration: 55000 Avg. Training loss: 3.8574 0.0183 sec/batch\n",
      "Epoch 7/10 Iteration: 56000 Avg. Training loss: 3.8465 0.0180 sec/batch\n",
      "Epoch 8/10 Iteration: 57000 Avg. Training loss: 3.8608 0.0090 sec/batch\n",
      "Epoch 8/10 Iteration: 58000 Avg. Training loss: 3.8411 0.0714 sec/batch\n",
      "Epoch 8/10 Iteration: 59000 Avg. Training loss: 3.8367 0.0190 sec/batch\n",
      "Epoch 8/10 Iteration: 60000 Avg. Training loss: 3.8316 0.0186 sec/batch\n",
      "Epoch 8/10 Iteration: 61000 Avg. Training loss: 3.8445 0.0189 sec/batch\n",
      "Epoch 8/10 Iteration: 62000 Avg. Training loss: 3.8306 0.0189 sec/batch\n",
      "Epoch 8/10 Iteration: 63000 Avg. Training loss: 3.8337 0.0187 sec/batch\n",
      "Epoch 8/10 Iteration: 64000 Avg. Training loss: 3.8315 0.0185 sec/batch\n",
      "Epoch 9/10 Iteration: 65000 Avg. Training loss: 3.8253 0.0068 sec/batch\n",
      "Epoch 9/10 Iteration: 66000 Avg. Training loss: 3.8147 0.0190 sec/batch\n",
      "Epoch 9/10 Iteration: 67000 Avg. Training loss: 3.8179 0.0186 sec/batch\n",
      "Epoch 9/10 Iteration: 68000 Avg. Training loss: 3.8100 0.0190 sec/batch\n",
      "Epoch 9/10 Iteration: 69000 Avg. Training loss: 3.8104 0.0190 sec/batch\n",
      "Epoch 9/10 Iteration: 70000 Avg. Training loss: 3.8178 0.0190 sec/batch\n",
      "Epoch 9/10 Iteration: 71000 Avg. Training loss: 3.8140 0.0185 sec/batch\n",
      "Epoch 9/10 Iteration: 72000 Avg. Training loss: 3.8083 0.0206 sec/batch\n",
      "Epoch 10/10 Iteration: 73000 Avg. Training loss: 3.8098 0.0072 sec/batch\n",
      "Epoch 10/10 Iteration: 74000 Avg. Training loss: 3.7986 0.0227 sec/batch\n",
      "Epoch 10/10 Iteration: 75000 Avg. Training loss: 3.8037 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 76000 Avg. Training loss: 3.7962 0.0214 sec/batch\n",
      "Epoch 10/10 Iteration: 77000 Avg. Training loss: 3.7844 0.0203 sec/batch\n",
      "Epoch 10/10 Iteration: 78000 Avg. Training loss: 3.7973 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 79000 Avg. Training loss: 3.7840 0.0198 sec/batch\n",
      "Epoch 10/10 Iteration: 80000 Avg. Training loss: 3.7890 0.0195 sec/batch\n",
      "Finish training at  2018-08-01 21:58:12.874216\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  2000000\n",
      "Starting training at  2018-08-01 21:58:12.878867\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 7.6878 0.0211 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 6.3642 0.0222 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.6273 0.0226 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 5.2074 0.0209 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.9529 0.0226 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.8095 0.0827 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.6774 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.5943 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.5232 0.0725 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.4564 0.0219 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.4095 0.0230 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.3678 0.0224 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.3296 0.0215 sec/batch\n",
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.3163 0.0209 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.2637 0.0208 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.2421 0.0768 sec/batch\n",
      "Epoch 2/10 Iteration: 17000 Avg. Training loss: 4.1941 0.0174 sec/batch\n",
      "Epoch 2/10 Iteration: 18000 Avg. Training loss: 4.1807 0.0204 sec/batch\n",
      "Epoch 2/10 Iteration: 19000 Avg. Training loss: 4.1758 0.0212 sec/batch\n",
      "Epoch 2/10 Iteration: 20000 Avg. Training loss: 4.1405 0.0195 sec/batch\n",
      "Epoch 2/10 Iteration: 21000 Avg. Training loss: 4.1336 0.0211 sec/batch\n",
      "Epoch 2/10 Iteration: 22000 Avg. Training loss: 4.1225 0.0235 sec/batch\n",
      "Epoch 2/10 Iteration: 23000 Avg. Training loss: 4.1153 0.0208 sec/batch\n",
      "Epoch 2/10 Iteration: 24000 Avg. Training loss: 4.0961 0.0210 sec/batch\n",
      "Epoch 2/10 Iteration: 25000 Avg. Training loss: 4.0853 0.0243 sec/batch\n",
      "Epoch 2/10 Iteration: 26000 Avg. Training loss: 4.0796 0.0228 sec/batch\n",
      "Epoch 2/10 Iteration: 27000 Avg. Training loss: 4.0651 0.0208 sec/batch\n",
      "Epoch 2/10 Iteration: 28000 Avg. Training loss: 4.0633 0.0218 sec/batch\n",
      "Epoch 2/10 Iteration: 29000 Avg. Training loss: 4.0483 0.0228 sec/batch\n",
      "Epoch 2/10 Iteration: 30000 Avg. Training loss: 4.0562 0.0233 sec/batch\n",
      "Epoch 2/10 Iteration: 31000 Avg. Training loss: 4.0544 0.0220 sec/batch\n",
      "Epoch 2/10 Iteration: 32000 Avg. Training loss: 4.0336 0.0213 sec/batch\n",
      "Epoch 3/10 Iteration: 33000 Avg. Training loss: 4.0063 0.0152 sec/batch\n",
      "Epoch 3/10 Iteration: 34000 Avg. Training loss: 4.0064 0.0241 sec/batch\n",
      "Epoch 3/10 Iteration: 35000 Avg. Training loss: 4.0083 0.0209 sec/batch\n",
      "Epoch 3/10 Iteration: 36000 Avg. Training loss: 3.9985 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 37000 Avg. Training loss: 3.9842 0.0220 sec/batch\n",
      "Epoch 3/10 Iteration: 38000 Avg. Training loss: 3.9944 0.0234 sec/batch\n",
      "Epoch 3/10 Iteration: 39000 Avg. Training loss: 3.9911 0.0239 sec/batch\n",
      "Epoch 3/10 Iteration: 40000 Avg. Training loss: 3.9784 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 41000 Avg. Training loss: 3.9710 0.0231 sec/batch\n",
      "Epoch 3/10 Iteration: 42000 Avg. Training loss: 3.9678 0.0237 sec/batch\n",
      "Epoch 3/10 Iteration: 43000 Avg. Training loss: 3.9634 0.0226 sec/batch\n",
      "Epoch 3/10 Iteration: 44000 Avg. Training loss: 3.9661 0.0844 sec/batch\n",
      "Epoch 3/10 Iteration: 45000 Avg. Training loss: 3.9591 0.0226 sec/batch\n",
      "Epoch 3/10 Iteration: 46000 Avg. Training loss: 3.9641 0.0225 sec/batch\n",
      "Epoch 3/10 Iteration: 47000 Avg. Training loss: 3.9620 0.0206 sec/batch\n",
      "Epoch 3/10 Iteration: 48000 Avg. Training loss: 3.9446 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 49000 Avg. Training loss: 3.9345 0.0106 sec/batch\n",
      "Epoch 4/10 Iteration: 50000 Avg. Training loss: 3.9313 0.0212 sec/batch\n",
      "Epoch 4/10 Iteration: 51000 Avg. Training loss: 3.9423 0.0223 sec/batch\n",
      "Epoch 4/10 Iteration: 52000 Avg. Training loss: 3.9241 0.0210 sec/batch\n",
      "Epoch 4/10 Iteration: 53000 Avg. Training loss: 3.9243 0.0224 sec/batch\n",
      "Epoch 4/10 Iteration: 54000 Avg. Training loss: 3.9241 0.0209 sec/batch\n",
      "Epoch 4/10 Iteration: 55000 Avg. Training loss: 3.9193 0.0219 sec/batch\n",
      "Epoch 4/10 Iteration: 56000 Avg. Training loss: 3.9218 0.0197 sec/batch\n",
      "Epoch 4/10 Iteration: 57000 Avg. Training loss: 3.9124 0.0218 sec/batch\n",
      "Epoch 4/10 Iteration: 58000 Avg. Training loss: 3.9082 0.0212 sec/batch\n",
      "Epoch 4/10 Iteration: 59000 Avg. Training loss: 3.9069 0.0212 sec/batch\n",
      "Epoch 4/10 Iteration: 60000 Avg. Training loss: 3.9159 0.0222 sec/batch\n",
      "Epoch 4/10 Iteration: 61000 Avg. Training loss: 3.9032 0.0212 sec/batch\n",
      "Epoch 4/10 Iteration: 62000 Avg. Training loss: 3.9029 0.0223 sec/batch\n",
      "Epoch 4/10 Iteration: 63000 Avg. Training loss: 3.9060 0.0219 sec/batch\n",
      "Epoch 4/10 Iteration: 64000 Avg. Training loss: 3.9012 0.0223 sec/batch\n",
      "Epoch 5/10 Iteration: 65000 Avg. Training loss: 3.8887 0.0088 sec/batch\n",
      "Epoch 5/10 Iteration: 66000 Avg. Training loss: 3.8760 0.0216 sec/batch\n",
      "Epoch 5/10 Iteration: 67000 Avg. Training loss: 3.8884 0.0221 sec/batch\n",
      "Epoch 5/10 Iteration: 68000 Avg. Training loss: 3.8862 0.0197 sec/batch\n",
      "Epoch 5/10 Iteration: 69000 Avg. Training loss: 3.8834 0.0205 sec/batch\n",
      "Epoch 5/10 Iteration: 70000 Avg. Training loss: 3.8755 0.0216 sec/batch\n",
      "Epoch 5/10 Iteration: 71000 Avg. Training loss: 3.8784 0.0223 sec/batch\n",
      "Epoch 5/10 Iteration: 72000 Avg. Training loss: 3.8801 0.0214 sec/batch\n",
      "Epoch 5/10 Iteration: 73000 Avg. Training loss: 3.8704 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 74000 Avg. Training loss: 3.8751 0.0185 sec/batch\n",
      "Epoch 5/10 Iteration: 75000 Avg. Training loss: 3.8757 0.0191 sec/batch\n",
      "Epoch 5/10 Iteration: 76000 Avg. Training loss: 3.8763 0.0222 sec/batch\n",
      "Epoch 5/10 Iteration: 77000 Avg. Training loss: 3.8681 0.0210 sec/batch\n",
      "Epoch 5/10 Iteration: 78000 Avg. Training loss: 3.8768 0.0200 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Iteration: 79000 Avg. Training loss: 3.8725 0.0194 sec/batch\n",
      "Epoch 5/10 Iteration: 80000 Avg. Training loss: 3.8601 0.0215 sec/batch\n",
      "Epoch 6/10 Iteration: 81000 Avg. Training loss: 3.8572 0.0050 sec/batch\n",
      "Epoch 6/10 Iteration: 82000 Avg. Training loss: 3.8460 0.0239 sec/batch\n",
      "Epoch 6/10 Iteration: 83000 Avg. Training loss: 3.8583 0.0180 sec/batch\n",
      "Epoch 6/10 Iteration: 84000 Avg. Training loss: 3.8477 0.0181 sec/batch\n",
      "Epoch 6/10 Iteration: 85000 Avg. Training loss: 3.8391 0.0174 sec/batch\n",
      "Epoch 6/10 Iteration: 86000 Avg. Training loss: 3.8466 0.0179 sec/batch\n",
      "Epoch 6/10 Iteration: 87000 Avg. Training loss: 3.8444 0.0173 sec/batch\n",
      "Epoch 6/10 Iteration: 88000 Avg. Training loss: 3.8515 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 89000 Avg. Training loss: 3.8389 0.0183 sec/batch\n",
      "Epoch 6/10 Iteration: 90000 Avg. Training loss: 3.8444 0.0174 sec/batch\n",
      "Epoch 6/10 Iteration: 91000 Avg. Training loss: 3.8427 0.0183 sec/batch\n",
      "Epoch 6/10 Iteration: 92000 Avg. Training loss: 3.8429 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 93000 Avg. Training loss: 3.8432 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 94000 Avg. Training loss: 3.8399 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 95000 Avg. Training loss: 3.8399 0.0184 sec/batch\n",
      "Epoch 6/10 Iteration: 96000 Avg. Training loss: 3.8357 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 97000 Avg. Training loss: 3.8286 0.0008 sec/batch\n",
      "Epoch 7/10 Iteration: 98000 Avg. Training loss: 3.8147 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 99000 Avg. Training loss: 3.8346 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 100000 Avg. Training loss: 3.8303 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 101000 Avg. Training loss: 3.8112 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 102000 Avg. Training loss: 3.8218 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 103000 Avg. Training loss: 3.8222 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 104000 Avg. Training loss: 3.8255 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 105000 Avg. Training loss: 3.8173 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 106000 Avg. Training loss: 3.8288 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 107000 Avg. Training loss: 3.8190 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 108000 Avg. Training loss: 3.8168 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 109000 Avg. Training loss: 3.8198 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 110000 Avg. Training loss: 3.8257 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 111000 Avg. Training loss: 3.8214 0.0184 sec/batch\n",
      "Epoch 7/10 Iteration: 112000 Avg. Training loss: 3.8141 0.0180 sec/batch\n",
      "Epoch 7/10 Iteration: 113000 Avg. Training loss: 3.8171 0.0184 sec/batch\n",
      "Epoch 8/10 Iteration: 114000 Avg. Training loss: 3.7952 0.0162 sec/batch\n",
      "Epoch 8/10 Iteration: 115000 Avg. Training loss: 3.8087 0.0184 sec/batch\n",
      "Epoch 8/10 Iteration: 116000 Avg. Training loss: 3.8120 0.0184 sec/batch\n",
      "Epoch 8/10 Iteration: 117000 Avg. Training loss: 3.7873 0.0184 sec/batch\n",
      "Epoch 8/10 Iteration: 118000 Avg. Training loss: 3.8039 0.0184 sec/batch\n",
      "Epoch 8/10 Iteration: 119000 Avg. Training loss: 3.8015 0.0189 sec/batch\n",
      "Epoch 8/10 Iteration: 120000 Avg. Training loss: 3.8405 0.0212 sec/batch\n",
      "Epoch 8/10 Iteration: 121000 Avg. Training loss: 3.7993 0.0223 sec/batch\n",
      "Epoch 8/10 Iteration: 122000 Avg. Training loss: 3.7927 0.0227 sec/batch\n",
      "Epoch 8/10 Iteration: 123000 Avg. Training loss: 3.8053 0.0210 sec/batch\n",
      "Epoch 8/10 Iteration: 124000 Avg. Training loss: 3.8033 0.0204 sec/batch\n",
      "Epoch 8/10 Iteration: 125000 Avg. Training loss: 3.8029 0.0204 sec/batch\n",
      "Epoch 8/10 Iteration: 126000 Avg. Training loss: 3.8054 0.0218 sec/batch\n",
      "Epoch 8/10 Iteration: 127000 Avg. Training loss: 3.8085 0.0214 sec/batch\n",
      "Epoch 8/10 Iteration: 128000 Avg. Training loss: 3.8072 0.0218 sec/batch\n",
      "Epoch 8/10 Iteration: 129000 Avg. Training loss: 3.7876 0.0230 sec/batch\n",
      "Epoch 9/10 Iteration: 130000 Avg. Training loss: 3.7785 0.0154 sec/batch\n",
      "Epoch 9/10 Iteration: 131000 Avg. Training loss: 3.7908 0.0222 sec/batch\n",
      "Epoch 9/10 Iteration: 132000 Avg. Training loss: 3.7884 0.0222 sec/batch\n",
      "Epoch 9/10 Iteration: 133000 Avg. Training loss: 3.7885 0.0206 sec/batch\n",
      "Epoch 9/10 Iteration: 134000 Avg. Training loss: 3.7728 0.0214 sec/batch\n",
      "Epoch 9/10 Iteration: 135000 Avg. Training loss: 3.7936 0.0212 sec/batch\n",
      "Epoch 9/10 Iteration: 136000 Avg. Training loss: 3.7985 0.0217 sec/batch\n",
      "Epoch 9/10 Iteration: 137000 Avg. Training loss: 3.7962 0.0224 sec/batch\n",
      "Epoch 9/10 Iteration: 138000 Avg. Training loss: 3.7850 0.0226 sec/batch\n",
      "Epoch 9/10 Iteration: 139000 Avg. Training loss: 3.7787 0.0220 sec/batch\n",
      "Epoch 9/10 Iteration: 140000 Avg. Training loss: 3.7910 0.0208 sec/batch\n",
      "Epoch 9/10 Iteration: 141000 Avg. Training loss: 3.7873 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 142000 Avg. Training loss: 3.7836 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 143000 Avg. Training loss: 3.7837 0.0223 sec/batch\n",
      "Epoch 9/10 Iteration: 144000 Avg. Training loss: 3.7868 0.0215 sec/batch\n",
      "Epoch 9/10 Iteration: 145000 Avg. Training loss: 3.7754 0.0833 sec/batch\n",
      "Epoch 10/10 Iteration: 146000 Avg. Training loss: 3.7734 0.0113 sec/batch\n",
      "Epoch 10/10 Iteration: 147000 Avg. Training loss: 3.7767 0.0213 sec/batch\n",
      "Epoch 10/10 Iteration: 148000 Avg. Training loss: 3.7939 0.0214 sec/batch\n",
      "Epoch 10/10 Iteration: 149000 Avg. Training loss: 3.7714 0.0231 sec/batch\n",
      "Epoch 10/10 Iteration: 150000 Avg. Training loss: 3.7628 0.0224 sec/batch\n",
      "Epoch 10/10 Iteration: 151000 Avg. Training loss: 3.7795 0.0221 sec/batch\n",
      "Epoch 10/10 Iteration: 152000 Avg. Training loss: 3.7821 0.0224 sec/batch\n",
      "Epoch 10/10 Iteration: 153000 Avg. Training loss: 3.7743 0.0233 sec/batch\n",
      "Epoch 10/10 Iteration: 154000 Avg. Training loss: 3.7640 0.0213 sec/batch\n",
      "Epoch 10/10 Iteration: 155000 Avg. Training loss: 3.7622 0.0232 sec/batch\n",
      "Epoch 10/10 Iteration: 156000 Avg. Training loss: 3.7715 0.0220 sec/batch\n",
      "Epoch 10/10 Iteration: 157000 Avg. Training loss: 3.7756 0.0211 sec/batch\n",
      "Epoch 10/10 Iteration: 158000 Avg. Training loss: 3.7660 0.0228 sec/batch\n",
      "Epoch 10/10 Iteration: 159000 Avg. Training loss: 3.7732 0.0228 sec/batch\n",
      "Epoch 10/10 Iteration: 160000 Avg. Training loss: 3.7774 0.0224 sec/batch\n",
      "Epoch 10/10 Iteration: 161000 Avg. Training loss: 3.7724 0.0214 sec/batch\n",
      "Finish training at  2018-08-01 22:59:22.248028\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  3000000\n",
      "Starting training at  2018-08-01 22:59:22.258562\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 7.7355 0.0213 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 6.3451 0.0201 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 5.5945 0.0198 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 5.1905 0.0197 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.9447 0.0190 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.7909 0.0194 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.6694 0.0752 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.5692 0.0236 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.5153 0.0213 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.4412 0.0222 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.4074 0.0190 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.3481 0.0785 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.3255 0.0246 sec/batch\n",
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.3001 0.0232 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.2636 0.0217 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.2477 0.0218 sec/batch\n",
      "Epoch 1/10 Iteration: 17000 Avg. Training loss: 4.2196 0.0215 sec/batch\n",
      "Epoch 1/10 Iteration: 18000 Avg. Training loss: 4.1961 0.0221 sec/batch\n",
      "Epoch 1/10 Iteration: 19000 Avg. Training loss: 4.1783 0.0218 sec/batch\n",
      "Epoch 1/10 Iteration: 20000 Avg. Training loss: 4.1644 0.0216 sec/batch\n",
      "Epoch 1/10 Iteration: 21000 Avg. Training loss: 4.1557 0.0218 sec/batch\n",
      "Epoch 1/10 Iteration: 22000 Avg. Training loss: 4.1356 0.0218 sec/batch\n",
      "Epoch 1/10 Iteration: 23000 Avg. Training loss: 4.1244 0.0225 sec/batch\n",
      "Epoch 1/10 Iteration: 24000 Avg. Training loss: 4.1107 0.0210 sec/batch\n",
      "Epoch 2/10 Iteration: 25000 Avg. Training loss: 4.1055 0.0165 sec/batch\n",
      "Epoch 2/10 Iteration: 26000 Avg. Training loss: 4.0755 0.0218 sec/batch\n",
      "Epoch 2/10 Iteration: 27000 Avg. Training loss: 4.0596 0.0222 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Iteration: 28000 Avg. Training loss: 4.0427 0.0230 sec/batch\n",
      "Epoch 2/10 Iteration: 29000 Avg. Training loss: 4.0382 0.0215 sec/batch\n",
      "Epoch 2/10 Iteration: 30000 Avg. Training loss: 4.0477 0.0194 sec/batch\n",
      "Epoch 2/10 Iteration: 31000 Avg. Training loss: 4.0386 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 32000 Avg. Training loss: 4.0222 0.0219 sec/batch\n",
      "Epoch 2/10 Iteration: 33000 Avg. Training loss: 4.0239 0.0217 sec/batch\n",
      "Epoch 2/10 Iteration: 34000 Avg. Training loss: 4.0181 0.0227 sec/batch\n",
      "Epoch 2/10 Iteration: 35000 Avg. Training loss: 4.0149 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 36000 Avg. Training loss: 4.0087 0.0216 sec/batch\n",
      "Epoch 2/10 Iteration: 37000 Avg. Training loss: 4.0003 0.0214 sec/batch\n",
      "Epoch 2/10 Iteration: 38000 Avg. Training loss: 3.9945 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 39000 Avg. Training loss: 3.9953 0.0211 sec/batch\n",
      "Epoch 2/10 Iteration: 40000 Avg. Training loss: 3.9850 0.0187 sec/batch\n",
      "Epoch 2/10 Iteration: 41000 Avg. Training loss: 3.9905 0.0224 sec/batch\n",
      "Epoch 2/10 Iteration: 42000 Avg. Training loss: 3.9804 0.0202 sec/batch\n",
      "Epoch 2/10 Iteration: 43000 Avg. Training loss: 3.9728 0.0212 sec/batch\n",
      "Epoch 2/10 Iteration: 44000 Avg. Training loss: 3.9667 0.0215 sec/batch\n",
      "Epoch 2/10 Iteration: 45000 Avg. Training loss: 3.9713 0.0198 sec/batch\n",
      "Epoch 2/10 Iteration: 46000 Avg. Training loss: 3.9669 0.0212 sec/batch\n",
      "Epoch 2/10 Iteration: 47000 Avg. Training loss: 3.9566 0.0210 sec/batch\n",
      "Epoch 2/10 Iteration: 48000 Avg. Training loss: 3.9654 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 49000 Avg. Training loss: 3.9567 0.0115 sec/batch\n",
      "Epoch 3/10 Iteration: 50000 Avg. Training loss: 3.9472 0.0216 sec/batch\n",
      "Epoch 3/10 Iteration: 51000 Avg. Training loss: 3.9340 0.0218 sec/batch\n",
      "Epoch 3/10 Iteration: 52000 Avg. Training loss: 3.9239 0.0187 sec/batch\n",
      "Epoch 3/10 Iteration: 53000 Avg. Training loss: 3.9335 0.0214 sec/batch\n",
      "Epoch 3/10 Iteration: 54000 Avg. Training loss: 3.9389 0.0246 sec/batch\n",
      "Epoch 3/10 Iteration: 55000 Avg. Training loss: 3.9332 0.0328 sec/batch\n",
      "Epoch 3/10 Iteration: 56000 Avg. Training loss: 3.9225 0.0240 sec/batch\n",
      "Epoch 3/10 Iteration: 57000 Avg. Training loss: 3.9332 0.0210 sec/batch\n",
      "Epoch 3/10 Iteration: 58000 Avg. Training loss: 3.9195 0.0202 sec/batch\n",
      "Epoch 3/10 Iteration: 59000 Avg. Training loss: 3.9245 0.0193 sec/batch\n",
      "Epoch 3/10 Iteration: 60000 Avg. Training loss: 3.9195 0.0214 sec/batch\n",
      "Epoch 3/10 Iteration: 61000 Avg. Training loss: 3.9126 0.0238 sec/batch\n",
      "Epoch 3/10 Iteration: 62000 Avg. Training loss: 3.9167 0.0230 sec/batch\n",
      "Epoch 3/10 Iteration: 63000 Avg. Training loss: 3.9198 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 64000 Avg. Training loss: 3.9087 0.0240 sec/batch\n",
      "Epoch 3/10 Iteration: 65000 Avg. Training loss: 3.9073 0.0224 sec/batch\n",
      "Epoch 3/10 Iteration: 66000 Avg. Training loss: 3.8975 0.0197 sec/batch\n",
      "Epoch 3/10 Iteration: 67000 Avg. Training loss: 3.9087 0.0183 sec/batch\n",
      "Epoch 3/10 Iteration: 68000 Avg. Training loss: 3.8972 0.0180 sec/batch\n",
      "Epoch 3/10 Iteration: 69000 Avg. Training loss: 3.8966 0.0183 sec/batch\n",
      "Epoch 3/10 Iteration: 70000 Avg. Training loss: 3.9094 0.0183 sec/batch\n",
      "Epoch 3/10 Iteration: 71000 Avg. Training loss: 3.8950 0.0177 sec/batch\n",
      "Epoch 3/10 Iteration: 72000 Avg. Training loss: 3.8974 0.0182 sec/batch\n",
      "Epoch 4/10 Iteration: 73000 Avg. Training loss: 3.8940 0.0054 sec/batch\n",
      "Epoch 4/10 Iteration: 74000 Avg. Training loss: 3.8882 0.0180 sec/batch\n",
      "Epoch 4/10 Iteration: 75000 Avg. Training loss: 3.8788 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 76000 Avg. Training loss: 3.8677 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 77000 Avg. Training loss: 3.8792 0.0182 sec/batch\n",
      "Epoch 4/10 Iteration: 78000 Avg. Training loss: 3.8871 0.0180 sec/batch\n",
      "Epoch 4/10 Iteration: 79000 Avg. Training loss: 3.8825 0.0179 sec/batch\n",
      "Epoch 4/10 Iteration: 80000 Avg. Training loss: 3.8726 0.0213 sec/batch\n",
      "Epoch 4/10 Iteration: 81000 Avg. Training loss: 3.8822 0.0209 sec/batch\n",
      "Epoch 4/10 Iteration: 82000 Avg. Training loss: 3.8669 0.0199 sec/batch\n",
      "Epoch 4/10 Iteration: 83000 Avg. Training loss: 3.8793 0.0185 sec/batch\n",
      "Epoch 4/10 Iteration: 84000 Avg. Training loss: 3.8791 0.0169 sec/batch\n",
      "Epoch 4/10 Iteration: 85000 Avg. Training loss: 3.8653 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 86000 Avg. Training loss: 3.8627 0.0182 sec/batch\n",
      "Epoch 4/10 Iteration: 87000 Avg. Training loss: 3.8826 0.0179 sec/batch\n",
      "Epoch 4/10 Iteration: 88000 Avg. Training loss: 3.8531 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 89000 Avg. Training loss: 3.8614 0.0175 sec/batch\n",
      "Epoch 4/10 Iteration: 90000 Avg. Training loss: 3.8624 0.0184 sec/batch\n",
      "Epoch 4/10 Iteration: 91000 Avg. Training loss: 3.8561 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 92000 Avg. Training loss: 3.8552 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 93000 Avg. Training loss: 3.8694 0.0184 sec/batch\n",
      "Epoch 4/10 Iteration: 94000 Avg. Training loss: 3.8596 0.0167 sec/batch\n",
      "Epoch 4/10 Iteration: 95000 Avg. Training loss: 3.8563 0.0183 sec/batch\n",
      "Epoch 4/10 Iteration: 96000 Avg. Training loss: 3.8558 0.0183 sec/batch\n",
      "Epoch 5/10 Iteration: 97000 Avg. Training loss: 3.8529 0.0011 sec/batch\n",
      "Epoch 5/10 Iteration: 98000 Avg. Training loss: 3.8456 0.0183 sec/batch\n",
      "Epoch 5/10 Iteration: 99000 Avg. Training loss: 3.8449 0.0183 sec/batch\n",
      "Epoch 5/10 Iteration: 100000 Avg. Training loss: 3.8381 0.0184 sec/batch\n",
      "Epoch 5/10 Iteration: 101000 Avg. Training loss: 3.8484 0.0216 sec/batch\n",
      "Epoch 5/10 Iteration: 102000 Avg. Training loss: 3.8376 0.0202 sec/batch\n",
      "Epoch 5/10 Iteration: 103000 Avg. Training loss: 3.8452 0.0207 sec/batch\n",
      "Epoch 5/10 Iteration: 104000 Avg. Training loss: 3.8485 0.0196 sec/batch\n",
      "Epoch 5/10 Iteration: 105000 Avg. Training loss: 3.8474 0.0193 sec/batch\n",
      "Epoch 5/10 Iteration: 106000 Avg. Training loss: 3.8455 0.0197 sec/batch\n",
      "Epoch 5/10 Iteration: 107000 Avg. Training loss: 3.8468 0.0211 sec/batch\n",
      "Epoch 5/10 Iteration: 108000 Avg. Training loss: 3.8449 0.0228 sec/batch\n",
      "Epoch 5/10 Iteration: 109000 Avg. Training loss: 3.8392 0.0235 sec/batch\n",
      "Epoch 5/10 Iteration: 110000 Avg. Training loss: 3.8431 0.0188 sec/batch\n",
      "Epoch 5/10 Iteration: 111000 Avg. Training loss: 3.8361 0.0187 sec/batch\n",
      "Epoch 5/10 Iteration: 112000 Avg. Training loss: 3.8256 0.0194 sec/batch\n",
      "Epoch 5/10 Iteration: 113000 Avg. Training loss: 3.8413 0.0188 sec/batch\n",
      "Epoch 5/10 Iteration: 114000 Avg. Training loss: 3.8295 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 115000 Avg. Training loss: 3.8255 0.0218 sec/batch\n",
      "Epoch 5/10 Iteration: 116000 Avg. Training loss: 3.8269 0.0243 sec/batch\n",
      "Epoch 5/10 Iteration: 117000 Avg. Training loss: 3.8311 0.0237 sec/batch\n",
      "Epoch 5/10 Iteration: 118000 Avg. Training loss: 3.8327 0.0221 sec/batch\n",
      "Epoch 5/10 Iteration: 119000 Avg. Training loss: 3.8302 0.0224 sec/batch\n",
      "Epoch 5/10 Iteration: 120000 Avg. Training loss: 3.8250 0.0215 sec/batch\n",
      "Epoch 5/10 Iteration: 121000 Avg. Training loss: 3.8276 0.0228 sec/batch\n",
      "Epoch 6/10 Iteration: 122000 Avg. Training loss: 3.8257 0.0172 sec/batch\n",
      "Epoch 6/10 Iteration: 123000 Avg. Training loss: 3.8178 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 124000 Avg. Training loss: 3.8148 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 125000 Avg. Training loss: 3.8085 0.0210 sec/batch\n",
      "Epoch 6/10 Iteration: 126000 Avg. Training loss: 3.8107 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 127000 Avg. Training loss: 3.8251 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 128000 Avg. Training loss: 3.8248 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 129000 Avg. Training loss: 3.8079 0.0206 sec/batch\n",
      "Epoch 6/10 Iteration: 130000 Avg. Training loss: 3.8197 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 131000 Avg. Training loss: 3.8190 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 132000 Avg. Training loss: 3.8181 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 133000 Avg. Training loss: 3.8156 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 134000 Avg. Training loss: 3.8129 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 135000 Avg. Training loss: 3.8136 0.0221 sec/batch\n",
      "Epoch 6/10 Iteration: 136000 Avg. Training loss: 3.8070 0.0239 sec/batch\n",
      "Epoch 6/10 Iteration: 137000 Avg. Training loss: 3.8179 0.0220 sec/batch\n",
      "Epoch 6/10 Iteration: 138000 Avg. Training loss: 3.8164 0.0204 sec/batch\n",
      "Epoch 6/10 Iteration: 139000 Avg. Training loss: 3.7993 0.0234 sec/batch\n",
      "Epoch 6/10 Iteration: 140000 Avg. Training loss: 3.8088 0.0218 sec/batch\n",
      "Epoch 6/10 Iteration: 141000 Avg. Training loss: 3.8116 0.0210 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Iteration: 142000 Avg. Training loss: 3.8070 0.0196 sec/batch\n",
      "Epoch 6/10 Iteration: 143000 Avg. Training loss: 3.8021 0.0201 sec/batch\n",
      "Epoch 6/10 Iteration: 144000 Avg. Training loss: 3.8032 0.0216 sec/batch\n",
      "Epoch 6/10 Iteration: 145000 Avg. Training loss: 3.8040 0.0213 sec/batch\n",
      "Epoch 7/10 Iteration: 146000 Avg. Training loss: 3.8068 0.0117 sec/batch\n",
      "Epoch 7/10 Iteration: 147000 Avg. Training loss: 3.7909 0.0182 sec/batch\n",
      "Epoch 7/10 Iteration: 148000 Avg. Training loss: 3.7941 0.0210 sec/batch\n",
      "Epoch 7/10 Iteration: 149000 Avg. Training loss: 3.7898 0.0234 sec/batch\n",
      "Epoch 7/10 Iteration: 150000 Avg. Training loss: 3.7981 0.0216 sec/batch\n",
      "Epoch 7/10 Iteration: 151000 Avg. Training loss: 3.8132 0.0266 sec/batch\n",
      "Epoch 7/10 Iteration: 152000 Avg. Training loss: 3.8052 0.0245 sec/batch\n",
      "Epoch 7/10 Iteration: 153000 Avg. Training loss: 3.7999 0.0241 sec/batch\n",
      "Epoch 7/10 Iteration: 154000 Avg. Training loss: 3.8045 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 155000 Avg. Training loss: 3.7996 0.0239 sec/batch\n",
      "Epoch 7/10 Iteration: 156000 Avg. Training loss: 3.7991 0.0242 sec/batch\n",
      "Epoch 7/10 Iteration: 157000 Avg. Training loss: 3.8101 0.0241 sec/batch\n",
      "Epoch 7/10 Iteration: 158000 Avg. Training loss: 3.8070 0.0239 sec/batch\n",
      "Epoch 7/10 Iteration: 159000 Avg. Training loss: 3.7893 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 160000 Avg. Training loss: 3.8078 0.0242 sec/batch\n",
      "Epoch 7/10 Iteration: 161000 Avg. Training loss: 3.7872 0.0241 sec/batch\n",
      "Epoch 7/10 Iteration: 162000 Avg. Training loss: 3.7926 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 163000 Avg. Training loss: 3.7877 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 164000 Avg. Training loss: 3.7965 0.0239 sec/batch\n",
      "Epoch 7/10 Iteration: 165000 Avg. Training loss: 3.7870 0.0238 sec/batch\n",
      "Epoch 7/10 Iteration: 166000 Avg. Training loss: 3.7900 0.0239 sec/batch\n",
      "Epoch 7/10 Iteration: 167000 Avg. Training loss: 3.7896 0.0239 sec/batch\n",
      "Epoch 7/10 Iteration: 168000 Avg. Training loss: 3.7979 0.0240 sec/batch\n",
      "Epoch 7/10 Iteration: 169000 Avg. Training loss: 3.7901 0.0238 sec/batch\n",
      "Epoch 8/10 Iteration: 170000 Avg. Training loss: 3.7860 0.0083 sec/batch\n",
      "Epoch 8/10 Iteration: 171000 Avg. Training loss: 3.7777 0.0240 sec/batch\n",
      "Epoch 8/10 Iteration: 172000 Avg. Training loss: 3.7811 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 173000 Avg. Training loss: 3.7709 0.0240 sec/batch\n",
      "Epoch 8/10 Iteration: 174000 Avg. Training loss: 3.7901 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 175000 Avg. Training loss: 3.7929 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 176000 Avg. Training loss: 3.7824 0.0240 sec/batch\n",
      "Epoch 8/10 Iteration: 177000 Avg. Training loss: 3.7810 0.0240 sec/batch\n",
      "Epoch 8/10 Iteration: 178000 Avg. Training loss: 3.7897 0.0242 sec/batch\n",
      "Epoch 8/10 Iteration: 179000 Avg. Training loss: 3.7821 0.0242 sec/batch\n",
      "Epoch 8/10 Iteration: 180000 Avg. Training loss: 3.7886 0.0241 sec/batch\n",
      "Epoch 8/10 Iteration: 181000 Avg. Training loss: 3.7840 0.0242 sec/batch\n",
      "Epoch 8/10 Iteration: 182000 Avg. Training loss: 3.7796 0.0245 sec/batch\n",
      "Epoch 8/10 Iteration: 183000 Avg. Training loss: 3.7755 0.0263 sec/batch\n",
      "Epoch 8/10 Iteration: 184000 Avg. Training loss: 3.7842 0.0238 sec/batch\n",
      "Epoch 8/10 Iteration: 185000 Avg. Training loss: 3.7700 0.0244 sec/batch\n",
      "Epoch 8/10 Iteration: 186000 Avg. Training loss: 3.7745 0.0248 sec/batch\n",
      "Epoch 8/10 Iteration: 187000 Avg. Training loss: 3.7734 0.0248 sec/batch\n",
      "Epoch 8/10 Iteration: 188000 Avg. Training loss: 3.7756 0.0248 sec/batch\n",
      "Epoch 8/10 Iteration: 189000 Avg. Training loss: 3.7705 0.0251 sec/batch\n",
      "Epoch 8/10 Iteration: 190000 Avg. Training loss: 3.7770 0.0251 sec/batch\n",
      "Epoch 8/10 Iteration: 191000 Avg. Training loss: 3.7911 0.0250 sec/batch\n",
      "Epoch 8/10 Iteration: 192000 Avg. Training loss: 3.7708 0.0251 sec/batch\n",
      "Epoch 8/10 Iteration: 193000 Avg. Training loss: 3.7832 0.0252 sec/batch\n",
      "Epoch 9/10 Iteration: 194000 Avg. Training loss: 3.7739 0.0029 sec/batch\n",
      "Epoch 9/10 Iteration: 195000 Avg. Training loss: 3.7760 0.0224 sec/batch\n",
      "Epoch 9/10 Iteration: 196000 Avg. Training loss: 3.7660 0.0223 sec/batch\n",
      "Epoch 9/10 Iteration: 197000 Avg. Training loss: 3.7643 0.0238 sec/batch\n",
      "Epoch 9/10 Iteration: 198000 Avg. Training loss: 3.7756 0.0231 sec/batch\n",
      "Epoch 9/10 Iteration: 199000 Avg. Training loss: 3.7759 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 200000 Avg. Training loss: 3.7727 0.0246 sec/batch\n",
      "Epoch 9/10 Iteration: 201000 Avg. Training loss: 3.7723 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 202000 Avg. Training loss: 3.7699 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 203000 Avg. Training loss: 3.7703 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 204000 Avg. Training loss: 3.7737 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 205000 Avg. Training loss: 3.7715 0.0246 sec/batch\n",
      "Epoch 9/10 Iteration: 206000 Avg. Training loss: 3.7642 0.0246 sec/batch\n",
      "Epoch 9/10 Iteration: 207000 Avg. Training loss: 3.7786 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 208000 Avg. Training loss: 3.7682 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 209000 Avg. Training loss: 3.7645 0.0247 sec/batch\n",
      "Epoch 9/10 Iteration: 210000 Avg. Training loss: 3.7705 0.0245 sec/batch\n",
      "Epoch 9/10 Iteration: 211000 Avg. Training loss: 3.7571 0.0243 sec/batch\n",
      "Epoch 9/10 Iteration: 212000 Avg. Training loss: 3.7689 0.0240 sec/batch\n",
      "Epoch 9/10 Iteration: 213000 Avg. Training loss: 3.7608 0.0243 sec/batch\n",
      "Epoch 9/10 Iteration: 214000 Avg. Training loss: 3.7686 0.0244 sec/batch\n",
      "Epoch 9/10 Iteration: 215000 Avg. Training loss: 3.7612 0.0226 sec/batch\n",
      "Epoch 9/10 Iteration: 216000 Avg. Training loss: 3.7595 0.0231 sec/batch\n",
      "Epoch 9/10 Iteration: 217000 Avg. Training loss: 3.7704 0.0250 sec/batch\n",
      "Epoch 9/10 Iteration: 218000 Avg. Training loss: 3.7619 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 219000 Avg. Training loss: 3.7583 0.0217 sec/batch\n",
      "Epoch 10/10 Iteration: 220000 Avg. Training loss: 3.7535 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 221000 Avg. Training loss: 3.7673 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 222000 Avg. Training loss: 3.7546 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 223000 Avg. Training loss: 3.7551 0.0244 sec/batch\n",
      "Epoch 10/10 Iteration: 224000 Avg. Training loss: 3.7606 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 225000 Avg. Training loss: 3.7693 0.0245 sec/batch\n",
      "Epoch 10/10 Iteration: 226000 Avg. Training loss: 3.7519 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 227000 Avg. Training loss: 3.7715 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 228000 Avg. Training loss: 3.7598 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 229000 Avg. Training loss: 3.7654 0.0245 sec/batch\n",
      "Epoch 10/10 Iteration: 230000 Avg. Training loss: 3.7520 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 231000 Avg. Training loss: 3.7552 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 232000 Avg. Training loss: 3.7565 0.0248 sec/batch\n",
      "Epoch 10/10 Iteration: 233000 Avg. Training loss: 3.7488 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 234000 Avg. Training loss: 3.7569 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 235000 Avg. Training loss: 3.7535 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 236000 Avg. Training loss: 3.7545 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 237000 Avg. Training loss: 3.7627 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 238000 Avg. Training loss: 3.7494 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 239000 Avg. Training loss: 3.7517 0.0246 sec/batch\n",
      "Epoch 10/10 Iteration: 240000 Avg. Training loss: 3.7489 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 241000 Avg. Training loss: 3.7548 0.0247 sec/batch\n",
      "Epoch 10/10 Iteration: 242000 Avg. Training loss: 3.7558 0.0246 sec/batch\n",
      "Finish training at  2018-08-02 00:30:39.102635\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "\n",
    "    current_tokens = tokens_lst[i] * 1000000\n",
    "#     current_pivots_idx = np.array(q_pivots_idx[p])\n",
    "#     current_pivots_vec = np.array(q_pivots_vec[p])\n",
    "\n",
    "    print(\"Tokens: \", current_tokens)\n",
    "    #print(\"Pivots: \", len(cur_pivots_idx))\n",
    "    print(\"Starting training at \", datetime.datetime.now())\n",
    "    t0 = time.time()\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        iteration = 1\n",
    "        loss = 0\n",
    "        regular_loss = 0\n",
    "        loss_best = 100\n",
    "        loss_list = []\n",
    "        iteration_best = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            batches = get_batches(idx_pairs[i], batch_size)\n",
    "            start = time.time()\n",
    "            for x, y in batches:\n",
    "                feed = {inputs: x,\n",
    "                        labels: np.array(y)[:, None]\n",
    "#                       alpha_table: alpha\n",
    "                       }\n",
    "#                 sess.run(update_embed_op)\n",
    "#                 train_loss, _, regu_loss = sess.run([total_cost, optimizer, reg_loss], feed_dict=feed)\n",
    "                \n",
    "                train_loss, _ = sess.run([total_cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                loss += train_loss\n",
    "#                 regular_loss += regu_loss\n",
    "\n",
    "                if loss < loss_best:\n",
    "                    W = sess.run(embedding).tolist()\n",
    "                    iteration_best = iteration\n",
    "                    loss_best = loss\n",
    "\n",
    "                if iteration % 1000 == 0:\n",
    "                    end = time.time()\n",
    "                    loss_list.append(loss / 1000)\n",
    "                    print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(loss / 1000),\n",
    "#                           \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                          \"{:.4f} sec/batch\".format((end - start) / 1000))\n",
    "\n",
    "\n",
    "                    loss = 0\n",
    "                    regular_loss = 0\n",
    "                    start = time.time()\n",
    "                iteration += 1\n",
    "\n",
    "        np.save('w2v_retrain_'+str(tokens_lst[i])+'m.npy',np.array(W))        \n",
    "        print(\"Finish training at \", datetime.datetime.now()) \n",
    "        print(\"-------------------------------------------------------------------------\") \n",
    "        print(\"-------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
