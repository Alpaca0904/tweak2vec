{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhang/anaconda/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size):\n",
    "    idx_pairs = []\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            # for each context word within window\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = sentence[context_word_pos]\n",
    "                idx_pairs.append((center_word_idx, context_word_idx))\n",
    "                    \n",
    "    return idx_pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 46007576 word pairs\n"
     ]
    }
   ],
   "source": [
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "idx_pairs_SG = create_word_pairs(corpus, window_size = 5)\n",
    "print('totally {0} word pairs'.format(len(idx_pairs_SG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "wordlist.append(['UNK','0'])\n",
    "word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w[0] for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 10000 pivot words\n"
     ]
    }
   ],
   "source": [
    "f = open('pivots_google_10000.txt','r')\n",
    "a = f.read()\n",
    "pivots_vec = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words'.format(len(list(pivots_vec.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "pivots_100 = dict_slice(pivots_vec, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivots_idx = []\n",
    "pivots_vec = []\n",
    "for i in pivots_100.keys():\n",
    "    pivots_idx.append(i)\n",
    "    pivots_vec.append(pivots_100[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_shuffle = corpus[:]\n",
    "random.shuffle(corpus_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_shuffle = idx_pairs_SG[:]\n",
    "random.shuffle(idx_pairs_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs_10m = idx_pairs_shuffle[:10000000]\n",
    "idx_pairs_5m = idx_pairs_shuffle[:5000000]\n",
    "idx_pairs_1m = idx_pairs_shuffle[:1000000]\n",
    "idx_pairs_500k = idx_pairs_shuffle[:500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 0.1\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 1000 # number of samples each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    # embedding layer\n",
    "    init_width = 0.5 / n_embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # add regularization term\n",
    "    embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "    update_embed_op = tf.scatter_update(embedding_copy,pivots_idx,pivots_vec)\n",
    "    embed_copy = tf.nn.embedding_lookup(embedding_copy, inputs)\n",
    "    \n",
    "    reg_loss = reg_constant * tf.reduce_sum((embed-embed_copy)**2)\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2018-07-02 11:40:31.932924\n",
      "Epoch 1/5 Iteration: 100 Avg. Training loss: 75.4638 Avg. Reg. loss: 68.0146 0.0422 sec/batch\n",
      "Epoch 1/5 Iteration: 200 Avg. Training loss: 52.6324 Avg. Reg. loss: 45.2920 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 300 Avg. Training loss: 40.9645 Avg. Reg. loss: 33.6569 0.0242 sec/batch\n",
      "Epoch 1/5 Iteration: 400 Avg. Training loss: 32.3355 Avg. Reg. loss: 25.0986 0.0240 sec/batch\n",
      "Epoch 1/5 Iteration: 500 Avg. Training loss: 27.3704 Avg. Reg. loss: 20.1975 0.0229 sec/batch\n",
      "Epoch 1/5 Iteration: 600 Avg. Training loss: 21.3062 Avg. Reg. loss: 14.2504 0.0240 sec/batch\n",
      "Epoch 1/5 Iteration: 700 Avg. Training loss: 18.6573 Avg. Reg. loss: 11.6839 0.0366 sec/batch\n",
      "Epoch 1/5 Iteration: 800 Avg. Training loss: 16.2559 Avg. Reg. loss: 9.3668 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 900 Avg. Training loss: 13.9183 Avg. Reg. loss: 7.1321 0.0242 sec/batch\n",
      "Epoch 1/5 Iteration: 1000 Avg. Training loss: 12.2962 Avg. Reg. loss: 5.5680 0.0223 sec/batch\n",
      "Epoch 1/5 Iteration: 1100 Avg. Training loss: 11.3457 Avg. Reg. loss: 4.6954 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 1200 Avg. Training loss: 10.1895 Avg. Reg. loss: 3.6790 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 1300 Avg. Training loss: 9.3996 Avg. Reg. loss: 2.8758 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 1400 Avg. Training loss: 8.6561 Avg. Reg. loss: 2.1885 0.0248 sec/batch\n",
      "Epoch 1/5 Iteration: 1500 Avg. Training loss: 8.3689 Avg. Reg. loss: 1.9784 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 1600 Avg. Training loss: 7.8344 Avg. Reg. loss: 1.5269 0.0249 sec/batch\n",
      "Epoch 1/5 Iteration: 1700 Avg. Training loss: 7.5231 Avg. Reg. loss: 1.2466 0.0235 sec/batch\n",
      "Epoch 1/5 Iteration: 1800 Avg. Training loss: 7.2267 Avg. Reg. loss: 0.9767 0.0236 sec/batch\n",
      "Epoch 1/5 Iteration: 1900 Avg. Training loss: 7.0010 Avg. Reg. loss: 0.7731 0.0244 sec/batch\n",
      "Epoch 1/5 Iteration: 2000 Avg. Training loss: 6.7859 Avg. Reg. loss: 0.6231 0.0381 sec/batch\n",
      "Epoch 1/5 Iteration: 2100 Avg. Training loss: 6.6625 Avg. Reg. loss: 0.5478 0.0244 sec/batch\n",
      "Epoch 1/5 Iteration: 2200 Avg. Training loss: 6.5439 Avg. Reg. loss: 0.4277 0.0246 sec/batch\n",
      "Epoch 1/5 Iteration: 2300 Avg. Training loss: 6.4697 Avg. Reg. loss: 0.3828 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 2400 Avg. Training loss: 6.3611 Avg. Reg. loss: 0.3039 0.0236 sec/batch\n",
      "Epoch 1/5 Iteration: 2500 Avg. Training loss: 6.2696 Avg. Reg. loss: 0.2830 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 2600 Avg. Training loss: 6.2079 Avg. Reg. loss: 0.2355 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 2700 Avg. Training loss: 6.1822 Avg. Reg. loss: 0.2247 0.0236 sec/batch\n",
      "Epoch 1/5 Iteration: 2800 Avg. Training loss: 6.1710 Avg. Reg. loss: 0.1968 0.0244 sec/batch\n",
      "Epoch 1/5 Iteration: 2900 Avg. Training loss: 6.1275 Avg. Reg. loss: 0.1925 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 3000 Avg. Training loss: 6.0734 Avg. Reg. loss: 0.1800 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 3100 Avg. Training loss: 6.0263 Avg. Reg. loss: 0.1702 0.0235 sec/batch\n",
      "Epoch 1/5 Iteration: 3200 Avg. Training loss: 6.0197 Avg. Reg. loss: 0.1645 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 3300 Avg. Training loss: 5.9705 Avg. Reg. loss: 0.1646 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 3400 Avg. Training loss: 5.9554 Avg. Reg. loss: 0.1525 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 3500 Avg. Training loss: 5.9261 Avg. Reg. loss: 0.1666 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 3600 Avg. Training loss: 5.9202 Avg. Reg. loss: 0.1654 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 3700 Avg. Training loss: 5.9240 Avg. Reg. loss: 0.1576 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 3800 Avg. Training loss: 5.8541 Avg. Reg. loss: 0.1628 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 3900 Avg. Training loss: 5.8600 Avg. Reg. loss: 0.1490 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 4000 Avg. Training loss: 5.8001 Avg. Reg. loss: 0.1472 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 4100 Avg. Training loss: 5.8194 Avg. Reg. loss: 0.1425 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 4200 Avg. Training loss: 5.7677 Avg. Reg. loss: 0.1460 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 4300 Avg. Training loss: 5.7787 Avg. Reg. loss: 0.1477 0.0227 sec/batch\n",
      "Epoch 1/5 Iteration: 4400 Avg. Training loss: 5.7467 Avg. Reg. loss: 0.1423 0.0207 sec/batch\n",
      "Epoch 1/5 Iteration: 4500 Avg. Training loss: 5.7364 Avg. Reg. loss: 0.1636 0.0206 sec/batch\n",
      "Epoch 1/5 Iteration: 4600 Avg. Training loss: 5.7245 Avg. Reg. loss: 0.1532 0.0209 sec/batch\n",
      "Epoch 1/5 Iteration: 4700 Avg. Training loss: 5.6928 Avg. Reg. loss: 0.1421 0.0204 sec/batch\n",
      "Epoch 1/5 Iteration: 4800 Avg. Training loss: 5.7181 Avg. Reg. loss: 0.1587 0.0208 sec/batch\n",
      "Epoch 1/5 Iteration: 4900 Avg. Training loss: 5.6466 Avg. Reg. loss: 0.1537 0.0209 sec/batch\n",
      "Epoch 1/5 Iteration: 5000 Avg. Training loss: 5.6441 Avg. Reg. loss: 0.1380 0.0205 sec/batch\n",
      "Epoch 1/5 Iteration: 5100 Avg. Training loss: 5.5913 Avg. Reg. loss: 0.1462 0.0215 sec/batch\n",
      "Epoch 1/5 Iteration: 5200 Avg. Training loss: 5.6365 Avg. Reg. loss: 0.1535 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 5300 Avg. Training loss: 5.5854 Avg. Reg. loss: 0.1549 0.0237 sec/batch\n",
      "Epoch 1/5 Iteration: 5400 Avg. Training loss: 5.6081 Avg. Reg. loss: 0.1569 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 5500 Avg. Training loss: 5.5438 Avg. Reg. loss: 0.1540 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 5600 Avg. Training loss: 5.5439 Avg. Reg. loss: 0.1481 0.0240 sec/batch\n",
      "Epoch 1/5 Iteration: 5700 Avg. Training loss: 5.5485 Avg. Reg. loss: 0.1489 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 5800 Avg. Training loss: 5.5351 Avg. Reg. loss: 0.1534 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 5900 Avg. Training loss: 5.5573 Avg. Reg. loss: 0.1633 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 6000 Avg. Training loss: 5.5053 Avg. Reg. loss: 0.1560 0.0230 sec/batch\n",
      "Epoch 1/5 Iteration: 6100 Avg. Training loss: 5.4825 Avg. Reg. loss: 0.1493 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 6200 Avg. Training loss: 5.4816 Avg. Reg. loss: 0.1510 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 6300 Avg. Training loss: 5.4410 Avg. Reg. loss: 0.1457 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 6400 Avg. Training loss: 5.4435 Avg. Reg. loss: 0.1502 0.0247 sec/batch\n",
      "Epoch 1/5 Iteration: 6500 Avg. Training loss: 5.4275 Avg. Reg. loss: 0.1498 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 6600 Avg. Training loss: 5.4109 Avg. Reg. loss: 0.1561 0.0346 sec/batch\n",
      "Epoch 1/5 Iteration: 6700 Avg. Training loss: 5.4112 Avg. Reg. loss: 0.1501 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 6800 Avg. Training loss: 5.3797 Avg. Reg. loss: 0.1512 0.0251 sec/batch\n",
      "Epoch 1/5 Iteration: 6900 Avg. Training loss: 5.3779 Avg. Reg. loss: 0.1453 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 7000 Avg. Training loss: 5.3682 Avg. Reg. loss: 0.1545 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 7100 Avg. Training loss: 5.3422 Avg. Reg. loss: 0.1556 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 7200 Avg. Training loss: 5.3346 Avg. Reg. loss: 0.1499 0.0243 sec/batch\n",
      "Epoch 1/5 Iteration: 7300 Avg. Training loss: 5.3352 Avg. Reg. loss: 0.1575 0.0237 sec/batch\n",
      "Epoch 1/5 Iteration: 7400 Avg. Training loss: 5.3321 Avg. Reg. loss: 0.1502 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 7500 Avg. Training loss: 5.3264 Avg. Reg. loss: 0.1457 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 7600 Avg. Training loss: 5.3096 Avg. Reg. loss: 0.1499 0.0248 sec/batch\n",
      "Epoch 1/5 Iteration: 7700 Avg. Training loss: 5.2870 Avg. Reg. loss: 0.1423 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 7800 Avg. Training loss: 5.2547 Avg. Reg. loss: 0.1520 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 7900 Avg. Training loss: 5.2629 Avg. Reg. loss: 0.1507 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 8000 Avg. Training loss: 5.2510 Avg. Reg. loss: 0.1413 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 8100 Avg. Training loss: 5.2843 Avg. Reg. loss: 0.1458 0.0240 sec/batch\n",
      "Epoch 1/5 Iteration: 8200 Avg. Training loss: 5.2216 Avg. Reg. loss: 0.1503 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 8300 Avg. Training loss: 5.2140 Avg. Reg. loss: 0.1443 0.0247 sec/batch\n",
      "Epoch 1/5 Iteration: 8400 Avg. Training loss: 5.2232 Avg. Reg. loss: 0.1539 0.0236 sec/batch\n",
      "Epoch 1/5 Iteration: 8500 Avg. Training loss: 5.2192 Avg. Reg. loss: 0.1482 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 8600 Avg. Training loss: 5.2051 Avg. Reg. loss: 0.1465 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 8700 Avg. Training loss: 5.1764 Avg. Reg. loss: 0.1524 0.0259 sec/batch\n",
      "Epoch 1/5 Iteration: 8800 Avg. Training loss: 5.1715 Avg. Reg. loss: 0.1449 0.0244 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Iteration: 8900 Avg. Training loss: 5.1936 Avg. Reg. loss: 0.1497 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 9000 Avg. Training loss: 5.1623 Avg. Reg. loss: 0.1542 0.0252 sec/batch\n",
      "Epoch 1/5 Iteration: 9100 Avg. Training loss: 5.1455 Avg. Reg. loss: 0.1417 0.0251 sec/batch\n",
      "Epoch 1/5 Iteration: 9200 Avg. Training loss: 5.1398 Avg. Reg. loss: 0.1447 0.0245 sec/batch\n",
      "Epoch 1/5 Iteration: 9300 Avg. Training loss: 5.1323 Avg. Reg. loss: 0.1430 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 9400 Avg. Training loss: 5.1271 Avg. Reg. loss: 0.1485 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 9500 Avg. Training loss: 5.1049 Avg. Reg. loss: 0.1339 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 9600 Avg. Training loss: 5.0893 Avg. Reg. loss: 0.1418 0.0227 sec/batch\n",
      "Epoch 1/5 Iteration: 9700 Avg. Training loss: 5.1137 Avg. Reg. loss: 0.1473 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 9800 Avg. Training loss: 5.0754 Avg. Reg. loss: 0.1482 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 9900 Avg. Training loss: 5.0705 Avg. Reg. loss: 0.1485 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 10000 Avg. Training loss: 5.0955 Avg. Reg. loss: 0.1557 0.0220 sec/batch\n",
      "Epoch 1/5 Iteration: 10100 Avg. Training loss: 5.0958 Avg. Reg. loss: 0.1532 0.0225 sec/batch\n",
      "Epoch 1/5 Iteration: 10200 Avg. Training loss: 5.0718 Avg. Reg. loss: 0.1463 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 10300 Avg. Training loss: 5.0801 Avg. Reg. loss: 0.1445 0.0238 sec/batch\n",
      "Epoch 1/5 Iteration: 10400 Avg. Training loss: 5.0261 Avg. Reg. loss: 0.1370 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 10500 Avg. Training loss: 5.0508 Avg. Reg. loss: 0.1407 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 10600 Avg. Training loss: 5.0467 Avg. Reg. loss: 0.1482 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 10700 Avg. Training loss: 5.0357 Avg. Reg. loss: 0.1461 0.0197 sec/batch\n",
      "Epoch 1/5 Iteration: 10800 Avg. Training loss: 5.0499 Avg. Reg. loss: 0.1377 0.0205 sec/batch\n",
      "Epoch 1/5 Iteration: 10900 Avg. Training loss: 5.0508 Avg. Reg. loss: 0.1500 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 11000 Avg. Training loss: 5.0186 Avg. Reg. loss: 0.1439 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 11100 Avg. Training loss: 4.9858 Avg. Reg. loss: 0.1325 0.0237 sec/batch\n",
      "Epoch 1/5 Iteration: 11200 Avg. Training loss: 5.0023 Avg. Reg. loss: 0.1403 0.0203 sec/batch\n",
      "Epoch 1/5 Iteration: 11300 Avg. Training loss: 5.0177 Avg. Reg. loss: 0.1370 0.0213 sec/batch\n",
      "Epoch 1/5 Iteration: 11400 Avg. Training loss: 4.9958 Avg. Reg. loss: 0.1423 0.0226 sec/batch\n",
      "Epoch 1/5 Iteration: 11500 Avg. Training loss: 4.9880 Avg. Reg. loss: 0.1404 0.0242 sec/batch\n",
      "Epoch 1/5 Iteration: 11600 Avg. Training loss: 4.9962 Avg. Reg. loss: 0.1476 0.0203 sec/batch\n",
      "Epoch 1/5 Iteration: 11700 Avg. Training loss: 4.9804 Avg. Reg. loss: 0.1415 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 11800 Avg. Training loss: 4.9752 Avg. Reg. loss: 0.1402 0.0233 sec/batch\n",
      "Epoch 1/5 Iteration: 11900 Avg. Training loss: 4.9669 Avg. Reg. loss: 0.1410 0.0193 sec/batch\n",
      "Epoch 1/5 Iteration: 12000 Avg. Training loss: 4.9734 Avg. Reg. loss: 0.1324 0.0216 sec/batch\n",
      "Epoch 1/5 Iteration: 12100 Avg. Training loss: 4.9697 Avg. Reg. loss: 0.1435 0.0225 sec/batch\n",
      "Epoch 1/5 Iteration: 12200 Avg. Training loss: 4.9626 Avg. Reg. loss: 0.1362 0.0218 sec/batch\n",
      "Epoch 1/5 Iteration: 12300 Avg. Training loss: 4.9313 Avg. Reg. loss: 0.1289 0.0204 sec/batch\n",
      "Epoch 1/5 Iteration: 12400 Avg. Training loss: 4.9518 Avg. Reg. loss: 0.1440 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 12500 Avg. Training loss: 4.9242 Avg. Reg. loss: 0.1430 0.0230 sec/batch\n",
      "Epoch 1/5 Iteration: 12600 Avg. Training loss: 4.9523 Avg. Reg. loss: 0.1419 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 12700 Avg. Training loss: 4.9571 Avg. Reg. loss: 0.1426 0.0230 sec/batch\n",
      "Epoch 1/5 Iteration: 12800 Avg. Training loss: 4.9439 Avg. Reg. loss: 0.1310 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 12900 Avg. Training loss: 4.9502 Avg. Reg. loss: 0.1420 0.0234 sec/batch\n",
      "Epoch 1/5 Iteration: 13000 Avg. Training loss: 4.9221 Avg. Reg. loss: 0.1479 0.0232 sec/batch\n",
      "Epoch 1/5 Iteration: 13100 Avg. Training loss: 4.9237 Avg. Reg. loss: 0.1432 0.0231 sec/batch\n",
      "Epoch 1/5 Iteration: 13200 Avg. Training loss: 4.8934 Avg. Reg. loss: 0.1374 0.0227 sec/batch\n",
      "Epoch 1/5 Iteration: 13300 Avg. Training loss: 4.9203 Avg. Reg. loss: 0.1474 0.0227 sec/batch\n",
      "Epoch 1/5 Iteration: 13400 Avg. Training loss: 4.9315 Avg. Reg. loss: 0.1361 0.0228 sec/batch\n",
      "Epoch 1/5 Iteration: 13500 Avg. Training loss: 4.9569 Avg. Reg. loss: 0.1347 0.0210 sec/batch\n",
      "Epoch 1/5 Iteration: 13600 Avg. Training loss: 4.9101 Avg. Reg. loss: 0.1458 0.0199 sec/batch\n",
      "Epoch 1/5 Iteration: 13700 Avg. Training loss: 4.9062 Avg. Reg. loss: 0.1462 0.0210 sec/batch\n",
      "Epoch 1/5 Iteration: 13800 Avg. Training loss: 4.8931 Avg. Reg. loss: 0.1385 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 13900 Avg. Training loss: 4.9199 Avg. Reg. loss: 0.1321 0.0228 sec/batch\n",
      "Epoch 1/5 Iteration: 14000 Avg. Training loss: 4.8925 Avg. Reg. loss: 0.1452 0.0224 sec/batch\n",
      "Epoch 1/5 Iteration: 14100 Avg. Training loss: 4.8736 Avg. Reg. loss: 0.1386 0.0213 sec/batch\n",
      "Epoch 1/5 Iteration: 14200 Avg. Training loss: 4.9095 Avg. Reg. loss: 0.1516 0.0224 sec/batch\n",
      "Epoch 1/5 Iteration: 14300 Avg. Training loss: 4.9386 Avg. Reg. loss: 0.1399 0.0240 sec/batch\n",
      "Epoch 1/5 Iteration: 14400 Avg. Training loss: 4.8700 Avg. Reg. loss: 0.1420 0.0237 sec/batch\n",
      "Epoch 1/5 Iteration: 14500 Avg. Training loss: 4.8786 Avg. Reg. loss: 0.1416 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 14600 Avg. Training loss: 4.8951 Avg. Reg. loss: 0.1491 0.0229 sec/batch\n",
      "Epoch 1/5 Iteration: 14700 Avg. Training loss: 4.8613 Avg. Reg. loss: 0.1385 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 14800 Avg. Training loss: 4.8494 Avg. Reg. loss: 0.1252 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 14900 Avg. Training loss: 4.8603 Avg. Reg. loss: 0.1424 0.0241 sec/batch\n",
      "Epoch 1/5 Iteration: 15000 Avg. Training loss: 4.8361 Avg. Reg. loss: 0.1378 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 15100 Avg. Training loss: 4.8688 Avg. Reg. loss: 0.1347 0.0239 sec/batch\n",
      "Epoch 1/5 Iteration: 15200 Avg. Training loss: 4.8572 Avg. Reg. loss: 0.1382 0.0237 sec/batch\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    regular_loss = 0\n",
    "    loss_best = 100\n",
    "    loss_list = []\n",
    "    iteration_best = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        batches = get_batches(idx_pairs_SG, batch_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            sess.run(update_embed_op)\n",
    "            train_loss, _, regu_loss = sess.run([total_cost, optimizer, reg_loss], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            regular_loss += regu_loss\n",
    "            #regular_loss += sess.run(reg_loss, feed_dict = {inputs: x})\n",
    "\n",
    "            if loss < loss_best:\n",
    "                W = sess.run(embedding).tolist()\n",
    "                iteration_best = iteration\n",
    "                loss_best = loss\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                end = time.time()\n",
    "                loss_list.append(loss / 100)\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss / 100),\n",
    "                      \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                      \"{:.4f} sec/batch\".format((end - start) / 100))\n",
    "\n",
    "\n",
    "                loss = 0\n",
    "                regular_loss = 0\n",
    "                start = time.time()\n",
    "            iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69832498, -0.40161774, -0.15408248, -0.15335205,  0.11083074,\n",
       "       -0.2064511 , -0.06023936,  0.18341886,  0.03842158, -0.15629894,\n",
       "       -0.12491406,  0.10320943, -0.03120194, -0.09884547, -0.21968023,\n",
       "       -0.06601377, -0.35994545, -0.07534487,  0.28153044,  0.2593551 ,\n",
       "        0.18049815, -0.18095501, -0.13389163, -0.11190755, -0.09025559,\n",
       "       -0.02896996,  0.0823247 , -0.35764125, -0.16760482, -0.10051528,\n",
       "       -0.05259525, -0.06677286,  0.09823136,  0.16070138, -0.11842185,\n",
       "       -0.03522317,  0.02519188, -0.17191665, -0.1800096 , -0.00420266,\n",
       "       -0.1877479 , -0.16964671, -0.14890842, -0.08514175, -0.04537127,\n",
       "       -0.0524059 ,  0.11016116, -0.28318945, -0.21428087,  0.03470111])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(W)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best result at iteration:122001\n"
     ]
    }
   ],
   "source": [
    "# save embedding matrics\n",
    "np.save('w2v_pivots100_50d.npy',np.array(W))\n",
    "np.save('loss_pivots100_50d.npy',np.array(loss_list))\n",
    "\n",
    "print('best result at iteration:{0}'.format(iteration_best))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
