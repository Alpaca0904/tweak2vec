{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random  \n",
    "from collections import Counter\n",
    "import datetime, time, json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_pairs(int_corpus, window_size, stop_size):\n",
    "    idx_pairs = []\n",
    "    tokens = 0\n",
    "    # for each snetence \n",
    "    for sentence in int_corpus:\n",
    "        # for each center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            center_word_idx = sentence[center_word_pos]\n",
    "            tokens += 1\n",
    "            if tokens >= stop_size:\n",
    "                return idx_pairs, tokens\n",
    "            else:\n",
    "                # for each context word within window\n",
    "                for w in range(-window_size, window_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    # make soure not jump out sentence\n",
    "                    if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                        continue\n",
    "                    context_word_idx = sentence[context_word_pos]\n",
    "                    idx_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "                    \n",
    "    return idx_pairs, tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(idx_pairs, batch_size):\n",
    "    n_batches = len(idx_pairs) // batch_size\n",
    "    idx_pairs = idx_pairs[:n_batches*batch_size]\n",
    "    for idx in range(0, len(idx_pairs), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = idx_pairs[idx:idx+batch_size]\n",
    "        for ii in range (len(batch)):\n",
    "            x.append(batch[ii][0])\n",
    "            y.append(batch[ii][1])        \n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 26324522 word pairs\n",
      "totally 3258438 tokens\n"
     ]
    }
   ],
   "source": [
    "#corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/quora_corpus_int5.npy\").tolist()\n",
    "corpus = np.load(\"/Users/zhang/MscProject_tweak2vec/corpus/pubmed_corpus_int5.npy\").tolist()\n",
    "\n",
    "corpus_shuffle = corpus[:]\n",
    "\n",
    "random.shuffle(corpus_shuffle)\n",
    "pubmed_idx_pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size=7000000)\n",
    "print('totally {0} word pairs'.format(len(pubmed_idx_pairs)))\n",
    "print('totally {0} tokens'.format(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 26324522 word pairs\n",
      "totally 3258438 tokens\n",
      "totally 24235359 word pairs\n",
      "totally 3000000 tokens\n",
      "totally 16158048 word pairs\n",
      "totally 2000000 tokens\n",
      "totally 8080080 word pairs\n",
      "totally 1000000 tokens\n",
      "totally 4041333 word pairs\n",
      "totally 500000 tokens\n",
      "totally 807589 word pairs\n",
      "totally 100000 tokens\n",
      "totally 404625 word pairs\n",
      "totally 50000 tokens\n",
      "totally 80163 word pairs\n",
      "totally 10000 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_lst = [4,3,2,1,0.5,0.1,0.05,0.01]\n",
    "idx_pairs = []\n",
    "for i in tokens_lst:\n",
    "    random.shuffle(corpus_shuffle)\n",
    "    pairs, tokens = create_word_pairs(corpus_shuffle, window_size = 5, stop_size = i * 1000000)\n",
    "    idx_pairs.append(pairs)\n",
    "    print('totally {0} word pairs'.format(len(pairs)))\n",
    "    print('totally {0} tokens'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/quora_vocab5.npy').tolist()\n",
    "# wordlist.append(['UNK',0])\n",
    "# word2idx = {w[0]: wordlist.index(w) for w in wordlist }\n",
    "# idx2word = {wordlist.index(w): w[0] for w in wordlist }\n",
    "\n",
    "wordlist = np.load('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_vocab5.npy').tolist()\n",
    "wordlist.append('UNK')\n",
    "word2idx = {w: wordlist.index(w) for w in wordlist }\n",
    "idx2word = {wordlist.index(w): w for w in wordlist }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pivot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 5000 pivot words\n"
     ]
    }
   ],
   "source": [
    "# f = open('/Users/zhang/MscProject_tweak2vec/corpus/quora_pivots_google_10000.txt','r')\n",
    "f = open('/Users/zhang/MscProject_tweak2vec/corpus/pubmed_pivots_google_5000.txt','r')\n",
    "a = f.read()\n",
    "pivots_dict = eval(a)\n",
    "f.close()\n",
    "print('load {0} pivot words'.format(len(pivots_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_slice = lambda adict, start, end: dict((k, adict[k]) for k in list(adict.keys())[start:end])\n",
    "def get_pivots_slice(pivots_dict, size):\n",
    "    pivots = deepcopy(pivots_dict)\n",
    "    pivots_slice = dict_slice(pivots, 0, size)\n",
    "    pivots_idx = []\n",
    "    pivots_vec = []\n",
    "    for i in pivots_slice.keys():\n",
    "        pivots_idx.append(i)\n",
    "        pivots_vec.append(pivots_slice[i])\n",
    "    return pivots_idx, pivots_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pivots = 1000\n",
    "pivots_idx, pivots_vec = get_pivots_slice(pivots_dict, n_pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a small tf lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "embed = tf.Variable([[0,0],[1,1]])\n",
    "embed_2 = tf.Variable(tf.identity(embed))\n",
    "ao = tf.scatter_update(embed_2,[0],[[-5,5]])\n",
    "diff = tf.reduce_sum((embed-embed_2)**2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(diff))\n",
    "sess.run(ao)\n",
    "print(sess.run(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build graph with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_pretrain = np.load('/Users/zhang/MscProject_tweak2vec/word2vecModel/quora/w2v_google_50d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(word2idx)\n",
    "n_embedding = 50\n",
    "reg_constant = 0.0001\n",
    "n_sampled = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1000 # number of samples each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # input layer\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size], name='inputs')\n",
    "    # labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    \n",
    "    # embedding layer\n",
    "    init_width = 0.5 / n_embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -init_width, init_width))\n",
    "#     embedding = tf.Variable(google_pretrain)\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # add regularization term\n",
    "    embedding_copy = tf.Variable(tf.identity(embedding), trainable=False)\n",
    "    update_embed_op = tf.scatter_update(embedding_copy,pivots_idx,pivots_vec)\n",
    "    embed_copy = tf.nn.embedding_lookup(embedding_copy, inputs)\n",
    "    reg_loss = reg_constant * tf.reduce_sum((embed-embed_copy)**2)\n",
    "    \n",
    "    # sampled softmax layer\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding)), name=\"softmax_weights\")\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\")\n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=n_sampled,\n",
    "        num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "#     total_cost = cost \n",
    "    total_cost = cost + reg_loss\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  3000000\n",
      "Starting training at  2018-07-18 01:19:00.819679\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 6.3035 Avg. Reg. loss: 1.2262 0.0301 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 5.2347 Avg. Reg. loss: 1.3099 0.0212 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 4.8618 Avg. Reg. loss: 1.2842 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.6888 Avg. Reg. loss: 1.2694 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.6066 Avg. Reg. loss: 1.2544 0.0201 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.5476 Avg. Reg. loss: 1.2344 0.0380 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.5044 Avg. Reg. loss: 1.2177 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.4524 Avg. Reg. loss: 1.2100 0.0203 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.4250 Avg. Reg. loss: 1.1858 0.0204 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.4005 Avg. Reg. loss: 1.1625 0.0227 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.3871 Avg. Reg. loss: 1.1488 0.0230 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.3671 Avg. Reg. loss: 1.1306 0.0209 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.3453 Avg. Reg. loss: 1.1081 0.0204 sec/batch\n",
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.3272 Avg. Reg. loss: 1.0986 0.0208 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.3332 Avg. Reg. loss: 1.0793 0.0205 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.3120 Avg. Reg. loss: 1.0642 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 17000 Avg. Training loss: 4.2955 Avg. Reg. loss: 1.0458 0.0208 sec/batch\n",
      "Epoch 1/10 Iteration: 18000 Avg. Training loss: 4.2879 Avg. Reg. loss: 1.0321 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 19000 Avg. Training loss: 4.2789 Avg. Reg. loss: 1.0148 0.0193 sec/batch\n",
      "Epoch 1/10 Iteration: 20000 Avg. Training loss: 4.2665 Avg. Reg. loss: 1.0065 0.0209 sec/batch\n",
      "Epoch 1/10 Iteration: 21000 Avg. Training loss: 4.2674 Avg. Reg. loss: 0.9830 0.0209 sec/batch\n",
      "Epoch 1/10 Iteration: 22000 Avg. Training loss: 4.2443 Avg. Reg. loss: 0.9804 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 23000 Avg. Training loss: 4.2427 Avg. Reg. loss: 0.9692 0.0204 sec/batch\n",
      "Epoch 1/10 Iteration: 24000 Avg. Training loss: 4.2226 Avg. Reg. loss: 0.9497 0.0207 sec/batch\n",
      "Epoch 2/10 Iteration: 25000 Avg. Training loss: 4.2190 Avg. Reg. loss: 0.9490 0.0163 sec/batch\n",
      "Epoch 2/10 Iteration: 26000 Avg. Training loss: 4.2120 Avg. Reg. loss: 0.9436 0.0211 sec/batch\n",
      "Epoch 2/10 Iteration: 27000 Avg. Training loss: 4.1975 Avg. Reg. loss: 0.9292 0.0207 sec/batch\n",
      "Epoch 2/10 Iteration: 28000 Avg. Training loss: 4.1929 Avg. Reg. loss: 0.9162 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 29000 Avg. Training loss: 4.1972 Avg. Reg. loss: 0.9097 0.0222 sec/batch\n",
      "Epoch 2/10 Iteration: 30000 Avg. Training loss: 4.1920 Avg. Reg. loss: 0.8962 0.0208 sec/batch\n",
      "Epoch 2/10 Iteration: 31000 Avg. Training loss: 4.1885 Avg. Reg. loss: 0.8909 0.0204 sec/batch\n",
      "Epoch 2/10 Iteration: 32000 Avg. Training loss: 4.1648 Avg. Reg. loss: 0.8870 0.0207 sec/batch\n",
      "Epoch 2/10 Iteration: 33000 Avg. Training loss: 4.1539 Avg. Reg. loss: 0.8803 0.0202 sec/batch\n",
      "Epoch 2/10 Iteration: 34000 Avg. Training loss: 4.1532 Avg. Reg. loss: 0.8741 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 35000 Avg. Training loss: 4.1584 Avg. Reg. loss: 0.8638 0.0198 sec/batch\n",
      "Epoch 2/10 Iteration: 36000 Avg. Training loss: 4.1482 Avg. Reg. loss: 0.8570 0.0204 sec/batch\n",
      "Epoch 2/10 Iteration: 37000 Avg. Training loss: 4.1448 Avg. Reg. loss: 0.8489 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 38000 Avg. Training loss: 4.1184 Avg. Reg. loss: 0.8491 0.0203 sec/batch\n",
      "Epoch 2/10 Iteration: 39000 Avg. Training loss: 4.1436 Avg. Reg. loss: 0.8400 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 40000 Avg. Training loss: 4.1260 Avg. Reg. loss: 0.8359 0.0212 sec/batch\n",
      "Epoch 2/10 Iteration: 41000 Avg. Training loss: 4.1325 Avg. Reg. loss: 0.8261 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 42000 Avg. Training loss: 4.1184 Avg. Reg. loss: 0.8228 0.0201 sec/batch\n",
      "Epoch 2/10 Iteration: 43000 Avg. Training loss: 4.0984 Avg. Reg. loss: 0.8158 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 44000 Avg. Training loss: 4.1067 Avg. Reg. loss: 0.8128 0.0206 sec/batch\n",
      "Epoch 2/10 Iteration: 45000 Avg. Training loss: 4.1080 Avg. Reg. loss: 0.8037 0.0204 sec/batch\n",
      "Epoch 2/10 Iteration: 46000 Avg. Training loss: 4.1048 Avg. Reg. loss: 0.8001 0.0206 sec/batch\n",
      "Epoch 2/10 Iteration: 47000 Avg. Training loss: 4.0875 Avg. Reg. loss: 0.8004 0.0205 sec/batch\n",
      "Epoch 2/10 Iteration: 48000 Avg. Training loss: 4.0849 Avg. Reg. loss: 0.7914 0.0206 sec/batch\n",
      "Epoch 3/10 Iteration: 49000 Avg. Training loss: 4.0831 Avg. Reg. loss: 0.7921 0.0111 sec/batch\n",
      "Epoch 3/10 Iteration: 50000 Avg. Training loss: 4.0792 Avg. Reg. loss: 0.7914 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 51000 Avg. Training loss: 4.0748 Avg. Reg. loss: 0.7855 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 52000 Avg. Training loss: 4.0650 Avg. Reg. loss: 0.7777 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 53000 Avg. Training loss: 4.0997 Avg. Reg. loss: 0.7772 0.0207 sec/batch\n",
      "Epoch 3/10 Iteration: 54000 Avg. Training loss: 4.0650 Avg. Reg. loss: 0.7710 0.0201 sec/batch\n",
      "Epoch 3/10 Iteration: 55000 Avg. Training loss: 4.0755 Avg. Reg. loss: 0.7655 0.0201 sec/batch\n",
      "Epoch 3/10 Iteration: 56000 Avg. Training loss: 4.0545 Avg. Reg. loss: 0.7630 0.0202 sec/batch\n",
      "Epoch 3/10 Iteration: 57000 Avg. Training loss: 4.0437 Avg. Reg. loss: 0.7627 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 58000 Avg. Training loss: 4.0435 Avg. Reg. loss: 0.7596 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 59000 Avg. Training loss: 4.0524 Avg. Reg. loss: 0.7573 0.0186 sec/batch\n",
      "Epoch 3/10 Iteration: 60000 Avg. Training loss: 4.0390 Avg. Reg. loss: 0.7571 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 61000 Avg. Training loss: 4.0385 Avg. Reg. loss: 0.7493 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 62000 Avg. Training loss: 4.0265 Avg. Reg. loss: 0.7486 0.0199 sec/batch\n",
      "Epoch 3/10 Iteration: 63000 Avg. Training loss: 4.0364 Avg. Reg. loss: 0.7465 0.0206 sec/batch\n",
      "Epoch 3/10 Iteration: 64000 Avg. Training loss: 4.0437 Avg. Reg. loss: 0.7428 0.0203 sec/batch\n",
      "Epoch 3/10 Iteration: 65000 Avg. Training loss: 4.0289 Avg. Reg. loss: 0.7402 0.0196 sec/batch\n",
      "Epoch 3/10 Iteration: 66000 Avg. Training loss: 4.0201 Avg. Reg. loss: 0.7382 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 67000 Avg. Training loss: 4.0204 Avg. Reg. loss: 0.7318 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 68000 Avg. Training loss: 4.0119 Avg. Reg. loss: 0.7340 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 69000 Avg. Training loss: 4.0198 Avg. Reg. loss: 0.7297 0.0204 sec/batch\n",
      "Epoch 3/10 Iteration: 70000 Avg. Training loss: 4.0122 Avg. Reg. loss: 0.7266 0.0206 sec/batch\n",
      "Epoch 3/10 Iteration: 71000 Avg. Training loss: 4.0069 Avg. Reg. loss: 0.7279 0.0205 sec/batch\n",
      "Epoch 3/10 Iteration: 72000 Avg. Training loss: 4.0038 Avg. Reg. loss: 0.7232 0.0204 sec/batch\n",
      "Epoch 4/10 Iteration: 73000 Avg. Training loss: 4.0117 Avg. Reg. loss: 0.7245 0.0063 sec/batch\n",
      "Epoch 4/10 Iteration: 74000 Avg. Training loss: 4.0132 Avg. Reg. loss: 0.7240 0.0204 sec/batch\n",
      "Epoch 4/10 Iteration: 75000 Avg. Training loss: 4.0009 Avg. Reg. loss: 0.7190 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 76000 Avg. Training loss: 3.9919 Avg. Reg. loss: 0.7163 0.0191 sec/batch\n",
      "Epoch 4/10 Iteration: 77000 Avg. Training loss: 4.0036 Avg. Reg. loss: 0.7182 0.0202 sec/batch\n",
      "Epoch 4/10 Iteration: 78000 Avg. Training loss: 3.9969 Avg. Reg. loss: 0.7147 0.0202 sec/batch\n",
      "Epoch 4/10 Iteration: 79000 Avg. Training loss: 4.0041 Avg. Reg. loss: 0.7130 0.0203 sec/batch\n",
      "Epoch 4/10 Iteration: 80000 Avg. Training loss: 3.9925 Avg. Reg. loss: 0.7086 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 81000 Avg. Training loss: 3.9812 Avg. Reg. loss: 0.7100 0.0195 sec/batch\n",
      "Epoch 4/10 Iteration: 82000 Avg. Training loss: 3.9820 Avg. Reg. loss: 0.7095 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 83000 Avg. Training loss: 3.9910 Avg. Reg. loss: 0.7055 0.0206 sec/batch\n",
      "Epoch 4/10 Iteration: 84000 Avg. Training loss: 3.9886 Avg. Reg. loss: 0.7098 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 85000 Avg. Training loss: 3.9781 Avg. Reg. loss: 0.7044 0.0204 sec/batch\n",
      "Epoch 4/10 Iteration: 86000 Avg. Training loss: 3.9613 Avg. Reg. loss: 0.7052 0.0205 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Iteration: 87000 Avg. Training loss: 3.9761 Avg. Reg. loss: 0.7035 0.0203 sec/batch\n",
      "Epoch 4/10 Iteration: 88000 Avg. Training loss: 3.9883 Avg. Reg. loss: 0.7013 0.0196 sec/batch\n",
      "Epoch 4/10 Iteration: 89000 Avg. Training loss: 3.9719 Avg. Reg. loss: 0.6986 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 90000 Avg. Training loss: 3.9816 Avg. Reg. loss: 0.7007 0.0206 sec/batch\n",
      "Epoch 4/10 Iteration: 91000 Avg. Training loss: 3.9648 Avg. Reg. loss: 0.6937 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 92000 Avg. Training loss: 3.9649 Avg. Reg. loss: 0.6954 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 93000 Avg. Training loss: 3.9704 Avg. Reg. loss: 0.6931 0.0205 sec/batch\n",
      "Epoch 4/10 Iteration: 94000 Avg. Training loss: 3.9683 Avg. Reg. loss: 0.6922 0.0204 sec/batch\n",
      "Epoch 4/10 Iteration: 95000 Avg. Training loss: 3.9588 Avg. Reg. loss: 0.6942 0.0204 sec/batch\n",
      "Epoch 4/10 Iteration: 96000 Avg. Training loss: 3.9608 Avg. Reg. loss: 0.6906 0.0204 sec/batch\n",
      "Epoch 5/10 Iteration: 97000 Avg. Training loss: 3.9636 Avg. Reg. loss: 0.6909 0.0015 sec/batch\n",
      "Epoch 5/10 Iteration: 98000 Avg. Training loss: 3.9570 Avg. Reg. loss: 0.6900 0.0201 sec/batch\n",
      "Epoch 5/10 Iteration: 99000 Avg. Training loss: 3.9599 Avg. Reg. loss: 0.6917 0.0204 sec/batch\n",
      "Epoch 5/10 Iteration: 100000 Avg. Training loss: 3.9574 Avg. Reg. loss: 0.6882 0.0198 sec/batch\n",
      "Epoch 5/10 Iteration: 101000 Avg. Training loss: 3.9547 Avg. Reg. loss: 0.6882 0.0205 sec/batch\n",
      "Epoch 5/10 Iteration: 102000 Avg. Training loss: 3.9614 Avg. Reg. loss: 0.6882 0.0195 sec/batch\n",
      "Epoch 5/10 Iteration: 103000 Avg. Training loss: 3.9562 Avg. Reg. loss: 0.6864 0.0204 sec/batch\n",
      "Epoch 5/10 Iteration: 104000 Avg. Training loss: 3.9643 Avg. Reg. loss: 0.6835 0.0203 sec/batch\n",
      "Epoch 5/10 Iteration: 105000 Avg. Training loss: 3.9425 Avg. Reg. loss: 0.6845 0.0205 sec/batch\n",
      "Epoch 5/10 Iteration: 106000 Avg. Training loss: 3.9391 Avg. Reg. loss: 0.6849 0.0212 sec/batch\n",
      "Epoch 5/10 Iteration: 107000 Avg. Training loss: 3.9488 Avg. Reg. loss: 0.6833 0.0202 sec/batch\n",
      "Epoch 5/10 Iteration: 108000 Avg. Training loss: 3.9454 Avg. Reg. loss: 0.6851 0.0209 sec/batch\n",
      "Epoch 5/10 Iteration: 109000 Avg. Training loss: 3.9428 Avg. Reg. loss: 0.6820 0.0204 sec/batch\n",
      "Epoch 5/10 Iteration: 110000 Avg. Training loss: 3.9338 Avg. Reg. loss: 0.6812 0.0205 sec/batch\n",
      "Epoch 5/10 Iteration: 111000 Avg. Training loss: 3.9292 Avg. Reg. loss: 0.6813 0.0209 sec/batch\n",
      "Epoch 5/10 Iteration: 112000 Avg. Training loss: 3.9499 Avg. Reg. loss: 0.6800 0.0206 sec/batch\n",
      "Epoch 5/10 Iteration: 113000 Avg. Training loss: 3.9480 Avg. Reg. loss: 0.6798 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 114000 Avg. Training loss: 3.9337 Avg. Reg. loss: 0.6789 0.0206 sec/batch\n",
      "Epoch 5/10 Iteration: 115000 Avg. Training loss: 3.9308 Avg. Reg. loss: 0.6781 0.0194 sec/batch\n",
      "Epoch 5/10 Iteration: 116000 Avg. Training loss: 3.9283 Avg. Reg. loss: 0.6778 0.0175 sec/batch\n",
      "Epoch 5/10 Iteration: 117000 Avg. Training loss: 3.9363 Avg. Reg. loss: 0.6759 0.0189 sec/batch\n",
      "Epoch 5/10 Iteration: 118000 Avg. Training loss: 3.9476 Avg. Reg. loss: 0.6763 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 119000 Avg. Training loss: 3.9268 Avg. Reg. loss: 0.6767 0.0205 sec/batch\n",
      "Epoch 5/10 Iteration: 120000 Avg. Training loss: 3.9283 Avg. Reg. loss: 0.6771 0.0203 sec/batch\n",
      "Epoch 5/10 Iteration: 121000 Avg. Training loss: 3.9232 Avg. Reg. loss: 0.6726 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 122000 Avg. Training loss: 3.9280 Avg. Reg. loss: 0.6741 0.0173 sec/batch\n",
      "Epoch 6/10 Iteration: 123000 Avg. Training loss: 3.9251 Avg. Reg. loss: 0.6788 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 124000 Avg. Training loss: 3.9223 Avg. Reg. loss: 0.6744 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 125000 Avg. Training loss: 3.9276 Avg. Reg. loss: 0.6747 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 126000 Avg. Training loss: 3.9315 Avg. Reg. loss: 0.6771 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 127000 Avg. Training loss: 3.9220 Avg. Reg. loss: 0.6752 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 128000 Avg. Training loss: 3.9287 Avg. Reg. loss: 0.6732 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 129000 Avg. Training loss: 3.9140 Avg. Reg. loss: 0.6728 0.0175 sec/batch\n",
      "Epoch 6/10 Iteration: 130000 Avg. Training loss: 3.9090 Avg. Reg. loss: 0.6739 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 131000 Avg. Training loss: 3.9262 Avg. Reg. loss: 0.6755 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 132000 Avg. Training loss: 3.9235 Avg. Reg. loss: 0.6747 0.0198 sec/batch\n",
      "Epoch 6/10 Iteration: 133000 Avg. Training loss: 3.9273 Avg. Reg. loss: 0.6725 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 134000 Avg. Training loss: 3.9143 Avg. Reg. loss: 0.6722 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 135000 Avg. Training loss: 3.9059 Avg. Reg. loss: 0.6717 0.0198 sec/batch\n",
      "Epoch 6/10 Iteration: 136000 Avg. Training loss: 3.9203 Avg. Reg. loss: 0.6730 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 137000 Avg. Training loss: 3.9136 Avg. Reg. loss: 0.6717 0.0209 sec/batch\n",
      "Epoch 6/10 Iteration: 138000 Avg. Training loss: 3.9213 Avg. Reg. loss: 0.6702 0.0182 sec/batch\n",
      "Epoch 6/10 Iteration: 139000 Avg. Training loss: 3.9071 Avg. Reg. loss: 0.6689 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 140000 Avg. Training loss: 3.9067 Avg. Reg. loss: 0.6691 0.0195 sec/batch\n",
      "Epoch 6/10 Iteration: 141000 Avg. Training loss: 3.9129 Avg. Reg. loss: 0.6705 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 142000 Avg. Training loss: 3.9108 Avg. Reg. loss: 0.6693 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 143000 Avg. Training loss: 3.9077 Avg. Reg. loss: 0.6692 0.0206 sec/batch\n",
      "Epoch 6/10 Iteration: 144000 Avg. Training loss: 3.9082 Avg. Reg. loss: 0.6703 0.0208 sec/batch\n",
      "Epoch 6/10 Iteration: 145000 Avg. Training loss: 3.8996 Avg. Reg. loss: 0.6666 0.0199 sec/batch\n",
      "Epoch 7/10 Iteration: 146000 Avg. Training loss: 3.9076 Avg. Reg. loss: 0.6705 0.0123 sec/batch\n",
      "Epoch 7/10 Iteration: 147000 Avg. Training loss: 3.9116 Avg. Reg. loss: 0.6727 0.0188 sec/batch\n",
      "Epoch 7/10 Iteration: 148000 Avg. Training loss: 3.8981 Avg. Reg. loss: 0.6692 0.0193 sec/batch\n",
      "Epoch 7/10 Iteration: 149000 Avg. Training loss: 3.8910 Avg. Reg. loss: 0.6699 0.0202 sec/batch\n",
      "Epoch 7/10 Iteration: 150000 Avg. Training loss: 3.9145 Avg. Reg. loss: 0.6723 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 151000 Avg. Training loss: 3.9036 Avg. Reg. loss: 0.6720 0.0205 sec/batch\n",
      "Epoch 7/10 Iteration: 152000 Avg. Training loss: 3.9088 Avg. Reg. loss: 0.6692 0.0204 sec/batch\n",
      "Epoch 7/10 Iteration: 153000 Avg. Training loss: 3.8944 Avg. Reg. loss: 0.6679 0.0199 sec/batch\n",
      "Epoch 7/10 Iteration: 154000 Avg. Training loss: 3.8886 Avg. Reg. loss: 0.6689 0.0204 sec/batch\n",
      "Epoch 7/10 Iteration: 155000 Avg. Training loss: 3.8909 Avg. Reg. loss: 0.6711 0.0204 sec/batch\n",
      "Epoch 7/10 Iteration: 156000 Avg. Training loss: 3.9083 Avg. Reg. loss: 0.6701 0.0199 sec/batch\n",
      "Epoch 7/10 Iteration: 157000 Avg. Training loss: 3.8899 Avg. Reg. loss: 0.6734 0.0167 sec/batch\n",
      "Epoch 7/10 Iteration: 158000 Avg. Training loss: 3.8963 Avg. Reg. loss: 0.6693 0.0197 sec/batch\n",
      "Epoch 7/10 Iteration: 159000 Avg. Training loss: 3.8829 Avg. Reg. loss: 0.6687 0.0204 sec/batch\n",
      "Epoch 7/10 Iteration: 160000 Avg. Training loss: 3.9023 Avg. Reg. loss: 0.6713 0.0190 sec/batch\n",
      "Epoch 7/10 Iteration: 161000 Avg. Training loss: 3.8986 Avg. Reg. loss: 0.6707 0.0203 sec/batch\n",
      "Epoch 7/10 Iteration: 162000 Avg. Training loss: 3.8948 Avg. Reg. loss: 0.6698 0.0206 sec/batch\n",
      "Epoch 7/10 Iteration: 163000 Avg. Training loss: 3.8896 Avg. Reg. loss: 0.6671 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 164000 Avg. Training loss: 3.8897 Avg. Reg. loss: 0.6660 0.0206 sec/batch\n",
      "Epoch 7/10 Iteration: 165000 Avg. Training loss: 3.8937 Avg. Reg. loss: 0.6695 0.0207 sec/batch\n",
      "Epoch 7/10 Iteration: 166000 Avg. Training loss: 3.8919 Avg. Reg. loss: 0.6696 0.0204 sec/batch\n",
      "Epoch 7/10 Iteration: 167000 Avg. Training loss: 3.8888 Avg. Reg. loss: 0.6678 0.0208 sec/batch\n",
      "Epoch 7/10 Iteration: 168000 Avg. Training loss: 3.8876 Avg. Reg. loss: 0.6707 0.0203 sec/batch\n",
      "Epoch 7/10 Iteration: 169000 Avg. Training loss: 3.8980 Avg. Reg. loss: 0.6661 0.0205 sec/batch\n",
      "Epoch 8/10 Iteration: 170000 Avg. Training loss: 3.8882 Avg. Reg. loss: 0.6683 0.0076 sec/batch\n",
      "Epoch 8/10 Iteration: 171000 Avg. Training loss: 3.8954 Avg. Reg. loss: 0.6697 0.0196 sec/batch\n",
      "Epoch 8/10 Iteration: 172000 Avg. Training loss: 3.8788 Avg. Reg. loss: 0.6671 0.0266 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Iteration: 173000 Avg. Training loss: 3.8880 Avg. Reg. loss: 0.6679 0.0202 sec/batch\n",
      "Epoch 8/10 Iteration: 174000 Avg. Training loss: 3.8910 Avg. Reg. loss: 0.6692 0.0200 sec/batch\n",
      "Epoch 8/10 Iteration: 175000 Avg. Training loss: 3.8870 Avg. Reg. loss: 0.6724 0.0205 sec/batch\n",
      "Epoch 8/10 Iteration: 176000 Avg. Training loss: 3.8908 Avg. Reg. loss: 0.6697 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 177000 Avg. Training loss: 3.8881 Avg. Reg. loss: 0.6661 0.0208 sec/batch\n",
      "Epoch 8/10 Iteration: 178000 Avg. Training loss: 3.8764 Avg. Reg. loss: 0.6679 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 179000 Avg. Training loss: 3.8769 Avg. Reg. loss: 0.6702 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 180000 Avg. Training loss: 3.8827 Avg. Reg. loss: 0.6700 0.0208 sec/batch\n",
      "Epoch 8/10 Iteration: 181000 Avg. Training loss: 3.8791 Avg. Reg. loss: 0.6748 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 182000 Avg. Training loss: 3.8809 Avg. Reg. loss: 0.6707 0.0203 sec/batch\n",
      "Epoch 8/10 Iteration: 183000 Avg. Training loss: 3.8752 Avg. Reg. loss: 0.6693 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 184000 Avg. Training loss: 3.8849 Avg. Reg. loss: 0.6712 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 185000 Avg. Training loss: 3.8951 Avg. Reg. loss: 0.6707 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 186000 Avg. Training loss: 3.8838 Avg. Reg. loss: 0.6690 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 187000 Avg. Training loss: 3.8872 Avg. Reg. loss: 0.6715 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 188000 Avg. Training loss: 3.8746 Avg. Reg. loss: 0.6659 0.0202 sec/batch\n",
      "Epoch 8/10 Iteration: 189000 Avg. Training loss: 3.8769 Avg. Reg. loss: 0.6714 0.0204 sec/batch\n",
      "Epoch 8/10 Iteration: 190000 Avg. Training loss: 3.8830 Avg. Reg. loss: 0.6698 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 191000 Avg. Training loss: 3.8835 Avg. Reg. loss: 0.6680 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 192000 Avg. Training loss: 3.8720 Avg. Reg. loss: 0.6718 0.0207 sec/batch\n",
      "Epoch 8/10 Iteration: 193000 Avg. Training loss: 3.8724 Avg. Reg. loss: 0.6672 0.0205 sec/batch\n",
      "Epoch 9/10 Iteration: 194000 Avg. Training loss: 3.8725 Avg. Reg. loss: 0.6703 0.0027 sec/batch\n",
      "Epoch 9/10 Iteration: 195000 Avg. Training loss: 3.8906 Avg. Reg. loss: 0.6706 0.0205 sec/batch\n",
      "Epoch 9/10 Iteration: 196000 Avg. Training loss: 3.8746 Avg. Reg. loss: 0.6705 0.0206 sec/batch\n",
      "Epoch 9/10 Iteration: 197000 Avg. Training loss: 3.8807 Avg. Reg. loss: 0.6696 0.0204 sec/batch\n",
      "Epoch 9/10 Iteration: 198000 Avg. Training loss: 3.8819 Avg. Reg. loss: 0.6733 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 199000 Avg. Training loss: 3.8848 Avg. Reg. loss: 0.6733 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 200000 Avg. Training loss: 3.8708 Avg. Reg. loss: 0.6751 0.0206 sec/batch\n",
      "Epoch 9/10 Iteration: 201000 Avg. Training loss: 3.8820 Avg. Reg. loss: 0.6694 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 202000 Avg. Training loss: 3.8677 Avg. Reg. loss: 0.6718 0.0191 sec/batch\n",
      "Epoch 9/10 Iteration: 203000 Avg. Training loss: 3.8618 Avg. Reg. loss: 0.6720 0.0204 sec/batch\n",
      "Epoch 9/10 Iteration: 204000 Avg. Training loss: 3.8762 Avg. Reg. loss: 0.6717 0.0209 sec/batch\n",
      "Epoch 9/10 Iteration: 205000 Avg. Training loss: 3.8733 Avg. Reg. loss: 0.6767 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 206000 Avg. Training loss: 3.8714 Avg. Reg. loss: 0.6732 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 207000 Avg. Training loss: 3.8605 Avg. Reg. loss: 0.6732 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 208000 Avg. Training loss: 3.8684 Avg. Reg. loss: 0.6737 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 209000 Avg. Training loss: 3.8862 Avg. Reg. loss: 0.6742 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 210000 Avg. Training loss: 3.8808 Avg. Reg. loss: 0.6727 0.0208 sec/batch\n",
      "Epoch 9/10 Iteration: 211000 Avg. Training loss: 3.8654 Avg. Reg. loss: 0.6731 0.0202 sec/batch\n",
      "Epoch 9/10 Iteration: 212000 Avg. Training loss: 3.8801 Avg. Reg. loss: 0.6710 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 213000 Avg. Training loss: 3.8610 Avg. Reg. loss: 0.6734 0.0205 sec/batch\n",
      "Epoch 9/10 Iteration: 214000 Avg. Training loss: 3.8692 Avg. Reg. loss: 0.6744 0.0205 sec/batch\n",
      "Epoch 9/10 Iteration: 215000 Avg. Training loss: 3.8717 Avg. Reg. loss: 0.6736 0.0196 sec/batch\n",
      "Epoch 9/10 Iteration: 216000 Avg. Training loss: 3.8613 Avg. Reg. loss: 0.6723 0.0206 sec/batch\n",
      "Epoch 9/10 Iteration: 217000 Avg. Training loss: 3.8689 Avg. Reg. loss: 0.6754 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 218000 Avg. Training loss: 3.8730 Avg. Reg. loss: 0.6716 0.0188 sec/batch\n",
      "Epoch 10/10 Iteration: 219000 Avg. Training loss: 3.8676 Avg. Reg. loss: 0.6720 0.0184 sec/batch\n",
      "Epoch 10/10 Iteration: 220000 Avg. Training loss: 3.8655 Avg. Reg. loss: 0.6745 0.0205 sec/batch\n",
      "Epoch 10/10 Iteration: 221000 Avg. Training loss: 3.8642 Avg. Reg. loss: 0.6735 0.0196 sec/batch\n",
      "Epoch 10/10 Iteration: 222000 Avg. Training loss: 3.8652 Avg. Reg. loss: 0.6763 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 223000 Avg. Training loss: 3.8714 Avg. Reg. loss: 0.6763 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 224000 Avg. Training loss: 3.8650 Avg. Reg. loss: 0.6786 0.0204 sec/batch\n",
      "Epoch 10/10 Iteration: 225000 Avg. Training loss: 3.8710 Avg. Reg. loss: 0.6753 0.0204 sec/batch\n",
      "Epoch 10/10 Iteration: 226000 Avg. Training loss: 3.8574 Avg. Reg. loss: 0.6734 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 227000 Avg. Training loss: 3.8593 Avg. Reg. loss: 0.6757 0.0204 sec/batch\n",
      "Epoch 10/10 Iteration: 228000 Avg. Training loss: 3.8684 Avg. Reg. loss: 0.6778 0.0202 sec/batch\n",
      "Epoch 10/10 Iteration: 229000 Avg. Training loss: 3.8675 Avg. Reg. loss: 0.6789 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 230000 Avg. Training loss: 3.8601 Avg. Reg. loss: 0.6782 0.0206 sec/batch\n",
      "Epoch 10/10 Iteration: 231000 Avg. Training loss: 3.8603 Avg. Reg. loss: 0.6774 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 232000 Avg. Training loss: 3.8521 Avg. Reg. loss: 0.6783 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 233000 Avg. Training loss: 3.8669 Avg. Reg. loss: 0.6804 0.0206 sec/batch\n",
      "Epoch 10/10 Iteration: 234000 Avg. Training loss: 3.8644 Avg. Reg. loss: 0.6785 0.0206 sec/batch\n",
      "Epoch 10/10 Iteration: 235000 Avg. Training loss: 3.8640 Avg. Reg. loss: 0.6769 0.0204 sec/batch\n",
      "Epoch 10/10 Iteration: 236000 Avg. Training loss: 3.8650 Avg. Reg. loss: 0.6767 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 237000 Avg. Training loss: 3.8456 Avg. Reg. loss: 0.6771 0.0205 sec/batch\n",
      "Epoch 10/10 Iteration: 238000 Avg. Training loss: 3.8596 Avg. Reg. loss: 0.6798 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 239000 Avg. Training loss: 3.8642 Avg. Reg. loss: 0.6788 0.0202 sec/batch\n",
      "Epoch 10/10 Iteration: 240000 Avg. Training loss: 3.8606 Avg. Reg. loss: 0.6794 0.0194 sec/batch\n",
      "Epoch 10/10 Iteration: 241000 Avg. Training loss: 3.8506 Avg. Reg. loss: 0.6793 0.0201 sec/batch\n",
      "Epoch 10/10 Iteration: 242000 Avg. Training loss: 3.8587 Avg. Reg. loss: 0.6754 0.0199 sec/batch\n",
      "Finish training at  2018-07-18 02:41:50.275766\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  2000000\n",
      "Starting training at  2018-07-18 02:41:50.288065\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 6.2914 Avg. Reg. loss: 1.2657 0.0208 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 5.2435 Avg. Reg. loss: 1.3909 0.0199 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 4.8552 Avg. Reg. loss: 1.3898 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.6831 Avg. Reg. loss: 1.3638 0.0215 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.6015 Avg. Reg. loss: 1.3587 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.5420 Avg. Reg. loss: 1.3368 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.4901 Avg. Reg. loss: 1.3039 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.4414 Avg. Reg. loss: 1.2964 0.0201 sec/batch\n",
      "Epoch 1/10 Iteration: 9000 Avg. Training loss: 4.4266 Avg. Reg. loss: 1.2618 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 10000 Avg. Training loss: 4.4106 Avg. Reg. loss: 1.2554 0.0201 sec/batch\n",
      "Epoch 1/10 Iteration: 11000 Avg. Training loss: 4.3754 Avg. Reg. loss: 1.2336 0.0199 sec/batch\n",
      "Epoch 1/10 Iteration: 12000 Avg. Training loss: 4.3562 Avg. Reg. loss: 1.2061 0.0202 sec/batch\n",
      "Epoch 1/10 Iteration: 13000 Avg. Training loss: 4.3424 Avg. Reg. loss: 1.1939 0.0197 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 14000 Avg. Training loss: 4.3335 Avg. Reg. loss: 1.1570 0.0198 sec/batch\n",
      "Epoch 1/10 Iteration: 15000 Avg. Training loss: 4.3352 Avg. Reg. loss: 1.1509 0.0200 sec/batch\n",
      "Epoch 1/10 Iteration: 16000 Avg. Training loss: 4.2969 Avg. Reg. loss: 1.1272 0.0201 sec/batch\n",
      "Epoch 2/10 Iteration: 17000 Avg. Training loss: 4.2740 Avg. Reg. loss: 1.1070 0.0170 sec/batch\n",
      "Epoch 2/10 Iteration: 18000 Avg. Training loss: 4.2693 Avg. Reg. loss: 1.1150 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 19000 Avg. Training loss: 4.2641 Avg. Reg. loss: 1.0948 0.0195 sec/batch\n",
      "Epoch 2/10 Iteration: 20000 Avg. Training loss: 4.2543 Avg. Reg. loss: 1.0734 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 21000 Avg. Training loss: 4.2544 Avg. Reg. loss: 1.0659 0.0185 sec/batch\n",
      "Epoch 2/10 Iteration: 22000 Avg. Training loss: 4.2245 Avg. Reg. loss: 1.0630 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 23000 Avg. Training loss: 4.2246 Avg. Reg. loss: 1.0291 0.0201 sec/batch\n",
      "Epoch 2/10 Iteration: 24000 Avg. Training loss: 4.2128 Avg. Reg. loss: 1.0330 0.0203 sec/batch\n",
      "Epoch 2/10 Iteration: 25000 Avg. Training loss: 4.2110 Avg. Reg. loss: 1.0127 0.0218 sec/batch\n",
      "Epoch 2/10 Iteration: 26000 Avg. Training loss: 4.2182 Avg. Reg. loss: 1.0128 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 27000 Avg. Training loss: 4.1968 Avg. Reg. loss: 0.9991 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 28000 Avg. Training loss: 4.1978 Avg. Reg. loss: 0.9859 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 29000 Avg. Training loss: 4.1853 Avg. Reg. loss: 0.9823 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 30000 Avg. Training loss: 4.1672 Avg. Reg. loss: 0.9641 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 31000 Avg. Training loss: 4.1788 Avg. Reg. loss: 0.9570 0.0197 sec/batch\n",
      "Epoch 2/10 Iteration: 32000 Avg. Training loss: 4.1542 Avg. Reg. loss: 0.9490 0.0200 sec/batch\n",
      "Epoch 3/10 Iteration: 33000 Avg. Training loss: 4.1393 Avg. Reg. loss: 0.9370 0.0138 sec/batch\n",
      "Epoch 3/10 Iteration: 34000 Avg. Training loss: 4.1390 Avg. Reg. loss: 0.9428 0.0202 sec/batch\n",
      "Epoch 3/10 Iteration: 35000 Avg. Training loss: 4.1382 Avg. Reg. loss: 0.9355 0.0201 sec/batch\n",
      "Epoch 3/10 Iteration: 36000 Avg. Training loss: 4.1302 Avg. Reg. loss: 0.9201 0.0196 sec/batch\n",
      "Epoch 3/10 Iteration: 37000 Avg. Training loss: 4.1418 Avg. Reg. loss: 0.9166 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 38000 Avg. Training loss: 4.1163 Avg. Reg. loss: 0.9156 0.0182 sec/batch\n",
      "Epoch 3/10 Iteration: 39000 Avg. Training loss: 4.1116 Avg. Reg. loss: 0.8960 0.0193 sec/batch\n",
      "Epoch 3/10 Iteration: 40000 Avg. Training loss: 4.1088 Avg. Reg. loss: 0.8984 0.0195 sec/batch\n",
      "Epoch 3/10 Iteration: 41000 Avg. Training loss: 4.1021 Avg. Reg. loss: 0.8888 0.0199 sec/batch\n",
      "Epoch 3/10 Iteration: 42000 Avg. Training loss: 4.1138 Avg. Reg. loss: 0.8887 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 43000 Avg. Training loss: 4.1110 Avg. Reg. loss: 0.8812 0.0195 sec/batch\n",
      "Epoch 3/10 Iteration: 44000 Avg. Training loss: 4.0817 Avg. Reg. loss: 0.8740 0.0200 sec/batch\n",
      "Epoch 3/10 Iteration: 45000 Avg. Training loss: 4.0910 Avg. Reg. loss: 0.8718 0.0196 sec/batch\n",
      "Epoch 3/10 Iteration: 46000 Avg. Training loss: 4.0784 Avg. Reg. loss: 0.8616 0.0202 sec/batch\n",
      "Epoch 3/10 Iteration: 47000 Avg. Training loss: 4.0868 Avg. Reg. loss: 0.8566 0.0197 sec/batch\n",
      "Epoch 3/10 Iteration: 48000 Avg. Training loss: 4.0710 Avg. Reg. loss: 0.8536 0.0198 sec/batch\n",
      "Epoch 4/10 Iteration: 49000 Avg. Training loss: 4.0664 Avg. Reg. loss: 0.8449 0.0106 sec/batch\n",
      "Epoch 4/10 Iteration: 50000 Avg. Training loss: 4.0509 Avg. Reg. loss: 0.8524 0.0203 sec/batch\n",
      "Epoch 4/10 Iteration: 51000 Avg. Training loss: 4.0571 Avg. Reg. loss: 0.8511 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 52000 Avg. Training loss: 4.0563 Avg. Reg. loss: 0.8391 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 53000 Avg. Training loss: 4.0634 Avg. Reg. loss: 0.8339 0.0199 sec/batch\n",
      "Epoch 4/10 Iteration: 54000 Avg. Training loss: 4.0389 Avg. Reg. loss: 0.8382 0.0197 sec/batch\n",
      "Epoch 4/10 Iteration: 55000 Avg. Training loss: 4.0459 Avg. Reg. loss: 0.8245 0.0197 sec/batch\n",
      "Epoch 4/10 Iteration: 56000 Avg. Training loss: 4.0457 Avg. Reg. loss: 0.8213 0.0198 sec/batch\n",
      "Epoch 4/10 Iteration: 57000 Avg. Training loss: 4.0429 Avg. Reg. loss: 0.8192 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 58000 Avg. Training loss: 4.0405 Avg. Reg. loss: 0.8191 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 59000 Avg. Training loss: 4.0522 Avg. Reg. loss: 0.8168 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 60000 Avg. Training loss: 4.0186 Avg. Reg. loss: 0.8090 0.0199 sec/batch\n",
      "Epoch 4/10 Iteration: 61000 Avg. Training loss: 4.0312 Avg. Reg. loss: 0.8094 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 62000 Avg. Training loss: 4.0197 Avg. Reg. loss: 0.8037 0.0199 sec/batch\n",
      "Epoch 4/10 Iteration: 63000 Avg. Training loss: 4.0248 Avg. Reg. loss: 0.7984 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 64000 Avg. Training loss: 4.0171 Avg. Reg. loss: 0.7998 0.0200 sec/batch\n",
      "Epoch 5/10 Iteration: 65000 Avg. Training loss: 3.9996 Avg. Reg. loss: 0.7924 0.0075 sec/batch\n",
      "Epoch 5/10 Iteration: 66000 Avg. Training loss: 4.0034 Avg. Reg. loss: 0.7998 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 67000 Avg. Training loss: 4.0025 Avg. Reg. loss: 0.7980 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 68000 Avg. Training loss: 4.0069 Avg. Reg. loss: 0.7931 0.0200 sec/batch\n",
      "Epoch 5/10 Iteration: 69000 Avg. Training loss: 4.0050 Avg. Reg. loss: 0.7867 0.0198 sec/batch\n",
      "Epoch 5/10 Iteration: 70000 Avg. Training loss: 3.9871 Avg. Reg. loss: 0.7912 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 71000 Avg. Training loss: 4.0003 Avg. Reg. loss: 0.7840 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 72000 Avg. Training loss: 3.9914 Avg. Reg. loss: 0.7814 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 73000 Avg. Training loss: 3.9821 Avg. Reg. loss: 0.7812 0.0194 sec/batch\n",
      "Epoch 5/10 Iteration: 74000 Avg. Training loss: 3.9924 Avg. Reg. loss: 0.7767 0.0195 sec/batch\n",
      "Epoch 5/10 Iteration: 75000 Avg. Training loss: 3.9973 Avg. Reg. loss: 0.7789 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 76000 Avg. Training loss: 3.9820 Avg. Reg. loss: 0.7722 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 77000 Avg. Training loss: 3.9832 Avg. Reg. loss: 0.7707 0.0198 sec/batch\n",
      "Epoch 5/10 Iteration: 78000 Avg. Training loss: 3.9774 Avg. Reg. loss: 0.7705 0.0202 sec/batch\n",
      "Epoch 5/10 Iteration: 79000 Avg. Training loss: 3.9924 Avg. Reg. loss: 0.7666 0.0201 sec/batch\n",
      "Epoch 5/10 Iteration: 80000 Avg. Training loss: 3.9759 Avg. Reg. loss: 0.7656 0.0201 sec/batch\n",
      "Epoch 6/10 Iteration: 81000 Avg. Training loss: 3.9639 Avg. Reg. loss: 0.7592 0.0043 sec/batch\n",
      "Epoch 6/10 Iteration: 82000 Avg. Training loss: 3.9550 Avg. Reg. loss: 0.7667 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 83000 Avg. Training loss: 3.9616 Avg. Reg. loss: 0.7664 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 84000 Avg. Training loss: 3.9671 Avg. Reg. loss: 0.7636 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 85000 Avg. Training loss: 3.9623 Avg. Reg. loss: 0.7581 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 86000 Avg. Training loss: 3.9653 Avg. Reg. loss: 0.7600 0.0198 sec/batch\n",
      "Epoch 6/10 Iteration: 87000 Avg. Training loss: 3.9615 Avg. Reg. loss: 0.7559 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 88000 Avg. Training loss: 3.9619 Avg. Reg. loss: 0.7539 0.0200 sec/batch\n",
      "Epoch 6/10 Iteration: 89000 Avg. Training loss: 3.9522 Avg. Reg. loss: 0.7529 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 90000 Avg. Training loss: 3.9589 Avg. Reg. loss: 0.7485 0.0197 sec/batch\n",
      "Epoch 6/10 Iteration: 91000 Avg. Training loss: 3.9686 Avg. Reg. loss: 0.7562 0.0192 sec/batch\n",
      "Epoch 6/10 Iteration: 92000 Avg. Training loss: 3.9490 Avg. Reg. loss: 0.7481 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 93000 Avg. Training loss: 3.9447 Avg. Reg. loss: 0.7468 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 94000 Avg. Training loss: 3.9509 Avg. Reg. loss: 0.7456 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 95000 Avg. Training loss: 3.9459 Avg. Reg. loss: 0.7432 0.0199 sec/batch\n",
      "Epoch 6/10 Iteration: 96000 Avg. Training loss: 3.9460 Avg. Reg. loss: 0.7441 0.0199 sec/batch\n",
      "Epoch 7/10 Iteration: 97000 Avg. Training loss: 3.9357 Avg. Reg. loss: 0.7426 0.0012 sec/batch\n",
      "Epoch 7/10 Iteration: 98000 Avg. Training loss: 3.9278 Avg. Reg. loss: 0.7435 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 99000 Avg. Training loss: 3.9457 Avg. Reg. loss: 0.7465 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 100000 Avg. Training loss: 3.9370 Avg. Reg. loss: 0.7431 0.0192 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Iteration: 101000 Avg. Training loss: 3.9311 Avg. Reg. loss: 0.7392 0.0189 sec/batch\n",
      "Epoch 7/10 Iteration: 102000 Avg. Training loss: 3.9398 Avg. Reg. loss: 0.7404 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 103000 Avg. Training loss: 3.9330 Avg. Reg. loss: 0.7407 0.0198 sec/batch\n",
      "Epoch 7/10 Iteration: 104000 Avg. Training loss: 3.9346 Avg. Reg. loss: 0.7380 0.0202 sec/batch\n",
      "Epoch 7/10 Iteration: 105000 Avg. Training loss: 3.9263 Avg. Reg. loss: 0.7355 0.0199 sec/batch\n",
      "Epoch 7/10 Iteration: 106000 Avg. Training loss: 3.9318 Avg. Reg. loss: 0.7314 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 107000 Avg. Training loss: 3.9380 Avg. Reg. loss: 0.7406 0.0202 sec/batch\n",
      "Epoch 7/10 Iteration: 108000 Avg. Training loss: 3.9236 Avg. Reg. loss: 0.7324 0.0198 sec/batch\n",
      "Epoch 7/10 Iteration: 109000 Avg. Training loss: 3.9304 Avg. Reg. loss: 0.7323 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 110000 Avg. Training loss: 3.9250 Avg. Reg. loss: 0.7314 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 111000 Avg. Training loss: 3.9211 Avg. Reg. loss: 0.7329 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 112000 Avg. Training loss: 3.9282 Avg. Reg. loss: 0.7285 0.0200 sec/batch\n",
      "Epoch 7/10 Iteration: 113000 Avg. Training loss: 3.9088 Avg. Reg. loss: 0.7295 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 114000 Avg. Training loss: 3.9006 Avg. Reg. loss: 0.7299 0.0179 sec/batch\n",
      "Epoch 8/10 Iteration: 115000 Avg. Training loss: 3.9154 Avg. Reg. loss: 0.7311 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 116000 Avg. Training loss: 3.9155 Avg. Reg. loss: 0.7336 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 117000 Avg. Training loss: 3.9148 Avg. Reg. loss: 0.7272 0.0201 sec/batch\n",
      "Epoch 8/10 Iteration: 118000 Avg. Training loss: 3.9133 Avg. Reg. loss: 0.7271 0.0198 sec/batch\n",
      "Epoch 8/10 Iteration: 119000 Avg. Training loss: 3.9086 Avg. Reg. loss: 0.7313 0.0195 sec/batch\n",
      "Epoch 8/10 Iteration: 120000 Avg. Training loss: 3.9055 Avg. Reg. loss: 0.7279 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 121000 Avg. Training loss: 3.9020 Avg. Reg. loss: 0.7248 0.0192 sec/batch\n",
      "Epoch 8/10 Iteration: 122000 Avg. Training loss: 3.9045 Avg. Reg. loss: 0.7226 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 123000 Avg. Training loss: 3.9203 Avg. Reg. loss: 0.7306 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 124000 Avg. Training loss: 3.9109 Avg. Reg. loss: 0.7219 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 125000 Avg. Training loss: 3.9046 Avg. Reg. loss: 0.7208 0.0200 sec/batch\n",
      "Epoch 8/10 Iteration: 126000 Avg. Training loss: 3.9045 Avg. Reg. loss: 0.7238 0.0196 sec/batch\n",
      "Epoch 8/10 Iteration: 127000 Avg. Training loss: 3.8931 Avg. Reg. loss: 0.7220 0.0199 sec/batch\n",
      "Epoch 8/10 Iteration: 128000 Avg. Training loss: 3.9105 Avg. Reg. loss: 0.7208 0.0191 sec/batch\n",
      "Epoch 8/10 Iteration: 129000 Avg. Training loss: 3.8950 Avg. Reg. loss: 0.7197 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 130000 Avg. Training loss: 3.9081 Avg. Reg. loss: 0.7190 0.0148 sec/batch\n",
      "Epoch 9/10 Iteration: 131000 Avg. Training loss: 3.9023 Avg. Reg. loss: 0.7235 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 132000 Avg. Training loss: 3.8923 Avg. Reg. loss: 0.7244 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 133000 Avg. Training loss: 3.8985 Avg. Reg. loss: 0.7211 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 134000 Avg. Training loss: 3.9023 Avg. Reg. loss: 0.7188 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 135000 Avg. Training loss: 3.8829 Avg. Reg. loss: 0.7226 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 136000 Avg. Training loss: 3.8959 Avg. Reg. loss: 0.7204 0.0199 sec/batch\n",
      "Epoch 9/10 Iteration: 137000 Avg. Training loss: 3.8967 Avg. Reg. loss: 0.7176 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 138000 Avg. Training loss: 3.8899 Avg. Reg. loss: 0.7164 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 139000 Avg. Training loss: 3.9029 Avg. Reg. loss: 0.7230 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 140000 Avg. Training loss: 3.9012 Avg. Reg. loss: 0.7176 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 141000 Avg. Training loss: 3.8836 Avg. Reg. loss: 0.7158 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 142000 Avg. Training loss: 3.8844 Avg. Reg. loss: 0.7172 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 143000 Avg. Training loss: 3.8805 Avg. Reg. loss: 0.7178 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 144000 Avg. Training loss: 3.8949 Avg. Reg. loss: 0.7155 0.0196 sec/batch\n",
      "Epoch 9/10 Iteration: 145000 Avg. Training loss: 3.8797 Avg. Reg. loss: 0.7163 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 146000 Avg. Training loss: 3.8772 Avg. Reg. loss: 0.7145 0.0117 sec/batch\n",
      "Epoch 10/10 Iteration: 147000 Avg. Training loss: 3.8777 Avg. Reg. loss: 0.7186 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 148000 Avg. Training loss: 3.8817 Avg. Reg. loss: 0.7205 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 149000 Avg. Training loss: 3.8744 Avg. Reg. loss: 0.7188 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 150000 Avg. Training loss: 3.8921 Avg. Reg. loss: 0.7151 0.0198 sec/batch\n",
      "Epoch 10/10 Iteration: 151000 Avg. Training loss: 3.8687 Avg. Reg. loss: 0.7183 0.0197 sec/batch\n",
      "Epoch 10/10 Iteration: 152000 Avg. Training loss: 3.8833 Avg. Reg. loss: 0.7172 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 153000 Avg. Training loss: 3.8737 Avg. Reg. loss: 0.7148 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 154000 Avg. Training loss: 3.8743 Avg. Reg. loss: 0.7139 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 155000 Avg. Training loss: 3.8898 Avg. Reg. loss: 0.7170 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 156000 Avg. Training loss: 3.8936 Avg. Reg. loss: 0.7157 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 157000 Avg. Training loss: 3.8707 Avg. Reg. loss: 0.7119 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 158000 Avg. Training loss: 3.8828 Avg. Reg. loss: 0.7153 0.0199 sec/batch\n",
      "Epoch 10/10 Iteration: 159000 Avg. Training loss: 3.8661 Avg. Reg. loss: 0.7130 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 160000 Avg. Training loss: 3.8822 Avg. Reg. loss: 0.7139 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 161000 Avg. Training loss: 3.8697 Avg. Reg. loss: 0.7128 0.0199 sec/batch\n",
      "Finish training at  2018-07-18 03:35:30.364183\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  1000000\n",
      "Starting training at  2018-07-18 03:35:30.364454\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 6.2540 Avg. Reg. loss: 1.2135 0.0194 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 5.2333 Avg. Reg. loss: 1.2553 0.0211 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 4.8583 Avg. Reg. loss: 1.2733 0.0197 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.6818 Avg. Reg. loss: 1.2677 0.0202 sec/batch\n",
      "Epoch 1/10 Iteration: 5000 Avg. Training loss: 4.5931 Avg. Reg. loss: 1.2495 0.0203 sec/batch\n",
      "Epoch 1/10 Iteration: 6000 Avg. Training loss: 4.5418 Avg. Reg. loss: 1.2342 0.0191 sec/batch\n",
      "Epoch 1/10 Iteration: 7000 Avg. Training loss: 4.4864 Avg. Reg. loss: 1.2118 0.0207 sec/batch\n",
      "Epoch 1/10 Iteration: 8000 Avg. Training loss: 4.4470 Avg. Reg. loss: 1.1929 0.0197 sec/batch\n",
      "Epoch 2/10 Iteration: 9000 Avg. Training loss: 4.3916 Avg. Reg. loss: 1.1847 0.0185 sec/batch\n",
      "Epoch 2/10 Iteration: 10000 Avg. Training loss: 4.3701 Avg. Reg. loss: 1.1637 0.0198 sec/batch\n",
      "Epoch 2/10 Iteration: 11000 Avg. Training loss: 4.3465 Avg. Reg. loss: 1.1541 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 12000 Avg. Training loss: 4.3204 Avg. Reg. loss: 1.1430 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 13000 Avg. Training loss: 4.3162 Avg. Reg. loss: 1.1268 0.0203 sec/batch\n",
      "Epoch 2/10 Iteration: 14000 Avg. Training loss: 4.3039 Avg. Reg. loss: 1.1061 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 15000 Avg. Training loss: 4.2855 Avg. Reg. loss: 1.0857 0.0200 sec/batch\n",
      "Epoch 2/10 Iteration: 16000 Avg. Training loss: 4.2751 Avg. Reg. loss: 1.0747 0.0200 sec/batch\n",
      "Epoch 3/10 Iteration: 17000 Avg. Training loss: 4.2505 Avg. Reg. loss: 1.0628 0.0170 sec/batch\n",
      "Epoch 3/10 Iteration: 18000 Avg. Training loss: 4.2482 Avg. Reg. loss: 1.0471 0.0198 sec/batch\n",
      "Epoch 3/10 Iteration: 19000 Avg. Training loss: 4.2345 Avg. Reg. loss: 1.0402 0.0201 sec/batch\n",
      "Epoch 3/10 Iteration: 20000 Avg. Training loss: 4.2144 Avg. Reg. loss: 1.0361 0.0203 sec/batch\n",
      "Epoch 3/10 Iteration: 21000 Avg. Training loss: 4.2240 Avg. Reg. loss: 1.0221 0.0189 sec/batch\n",
      "Epoch 3/10 Iteration: 22000 Avg. Training loss: 4.2073 Avg. Reg. loss: 1.0094 0.0201 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Iteration: 23000 Avg. Training loss: 4.1952 Avg. Reg. loss: 0.9943 0.0200 sec/batch\n",
      "Epoch 3/10 Iteration: 24000 Avg. Training loss: 4.1840 Avg. Reg. loss: 0.9870 0.0202 sec/batch\n",
      "Epoch 4/10 Iteration: 25000 Avg. Training loss: 4.1717 Avg. Reg. loss: 0.9819 0.0153 sec/batch\n",
      "Epoch 4/10 Iteration: 26000 Avg. Training loss: 4.1643 Avg. Reg. loss: 0.9721 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 27000 Avg. Training loss: 4.1583 Avg. Reg. loss: 0.9663 0.0200 sec/batch\n",
      "Epoch 4/10 Iteration: 28000 Avg. Training loss: 4.1369 Avg. Reg. loss: 0.9628 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 29000 Avg. Training loss: 4.1493 Avg. Reg. loss: 0.9569 0.0201 sec/batch\n",
      "Epoch 4/10 Iteration: 30000 Avg. Training loss: 4.1436 Avg. Reg. loss: 0.9462 0.0203 sec/batch\n",
      "Epoch 4/10 Iteration: 31000 Avg. Training loss: 4.1252 Avg. Reg. loss: 0.9365 0.0202 sec/batch\n",
      "Epoch 4/10 Iteration: 32000 Avg. Training loss: 4.1156 Avg. Reg. loss: 0.9340 0.0203 sec/batch\n",
      "Epoch 5/10 Iteration: 33000 Avg. Training loss: 4.1086 Avg. Reg. loss: 0.9272 0.0138 sec/batch\n",
      "Epoch 5/10 Iteration: 34000 Avg. Training loss: 4.1019 Avg. Reg. loss: 0.9237 0.0216 sec/batch\n",
      "Epoch 5/10 Iteration: 35000 Avg. Training loss: 4.0985 Avg. Reg. loss: 0.9183 0.0201 sec/batch\n",
      "Epoch 5/10 Iteration: 36000 Avg. Training loss: 4.0928 Avg. Reg. loss: 0.9159 0.0201 sec/batch\n",
      "Epoch 5/10 Iteration: 37000 Avg. Training loss: 4.0946 Avg. Reg. loss: 0.9110 0.0202 sec/batch\n",
      "Epoch 5/10 Iteration: 38000 Avg. Training loss: 4.0860 Avg. Reg. loss: 0.9063 0.0199 sec/batch\n",
      "Epoch 5/10 Iteration: 39000 Avg. Training loss: 4.0739 Avg. Reg. loss: 0.8974 0.0194 sec/batch\n",
      "Epoch 5/10 Iteration: 40000 Avg. Training loss: 4.0649 Avg. Reg. loss: 0.8950 0.0201 sec/batch\n",
      "Epoch 6/10 Iteration: 41000 Avg. Training loss: 4.0608 Avg. Reg. loss: 0.8891 0.0122 sec/batch\n",
      "Epoch 6/10 Iteration: 42000 Avg. Training loss: 4.0522 Avg. Reg. loss: 0.8893 0.0203 sec/batch\n",
      "Epoch 6/10 Iteration: 43000 Avg. Training loss: 4.0582 Avg. Reg. loss: 0.8846 0.0202 sec/batch\n",
      "Epoch 6/10 Iteration: 44000 Avg. Training loss: 4.0390 Avg. Reg. loss: 0.8871 0.0203 sec/batch\n",
      "Epoch 6/10 Iteration: 45000 Avg. Training loss: 4.0416 Avg. Reg. loss: 0.8842 0.0201 sec/batch\n",
      "Epoch 6/10 Iteration: 46000 Avg. Training loss: 4.0321 Avg. Reg. loss: 0.8777 0.0202 sec/batch\n",
      "Epoch 6/10 Iteration: 47000 Avg. Training loss: 4.0408 Avg. Reg. loss: 0.8731 0.0201 sec/batch\n",
      "Epoch 6/10 Iteration: 48000 Avg. Training loss: 4.0228 Avg. Reg. loss: 0.8708 0.0203 sec/batch\n",
      "Epoch 7/10 Iteration: 49000 Avg. Training loss: 4.0164 Avg. Reg. loss: 0.8671 0.0105 sec/batch\n",
      "Epoch 7/10 Iteration: 50000 Avg. Training loss: 4.0242 Avg. Reg. loss: 0.8644 0.0196 sec/batch\n",
      "Epoch 7/10 Iteration: 51000 Avg. Training loss: 4.0137 Avg. Reg. loss: 0.8622 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 52000 Avg. Training loss: 3.9954 Avg. Reg. loss: 0.8623 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 53000 Avg. Training loss: 4.0066 Avg. Reg. loss: 0.8634 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 54000 Avg. Training loss: 3.9998 Avg. Reg. loss: 0.8574 0.0201 sec/batch\n",
      "Epoch 7/10 Iteration: 55000 Avg. Training loss: 4.0036 Avg. Reg. loss: 0.8533 0.0194 sec/batch\n",
      "Epoch 7/10 Iteration: 56000 Avg. Training loss: 3.9932 Avg. Reg. loss: 0.8506 0.0200 sec/batch\n",
      "Epoch 8/10 Iteration: 57000 Avg. Training loss: 3.9844 Avg. Reg. loss: 0.8478 0.0089 sec/batch\n",
      "Epoch 8/10 Iteration: 58000 Avg. Training loss: 3.9768 Avg. Reg. loss: 0.8469 0.0201 sec/batch\n",
      "Epoch 8/10 Iteration: 59000 Avg. Training loss: 3.9852 Avg. Reg. loss: 0.8463 0.0200 sec/batch\n",
      "Epoch 8/10 Iteration: 60000 Avg. Training loss: 3.9670 Avg. Reg. loss: 0.8466 0.0202 sec/batch\n",
      "Epoch 8/10 Iteration: 61000 Avg. Training loss: 3.9727 Avg. Reg. loss: 0.8476 0.0202 sec/batch\n",
      "Epoch 8/10 Iteration: 62000 Avg. Training loss: 3.9651 Avg. Reg. loss: 0.8447 0.0200 sec/batch\n",
      "Epoch 8/10 Iteration: 63000 Avg. Training loss: 3.9818 Avg. Reg. loss: 0.8419 0.0201 sec/batch\n",
      "Epoch 8/10 Iteration: 64000 Avg. Training loss: 3.9612 Avg. Reg. loss: 0.8364 0.0198 sec/batch\n",
      "Epoch 9/10 Iteration: 65000 Avg. Training loss: 3.9587 Avg. Reg. loss: 0.8348 0.0072 sec/batch\n",
      "Epoch 9/10 Iteration: 66000 Avg. Training loss: 3.9470 Avg. Reg. loss: 0.8356 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 67000 Avg. Training loss: 3.9594 Avg. Reg. loss: 0.8339 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 68000 Avg. Training loss: 3.9491 Avg. Reg. loss: 0.8345 0.0216 sec/batch\n",
      "Epoch 9/10 Iteration: 69000 Avg. Training loss: 3.9469 Avg. Reg. loss: 0.8367 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 70000 Avg. Training loss: 3.9381 Avg. Reg. loss: 0.8341 0.0201 sec/batch\n",
      "Epoch 9/10 Iteration: 71000 Avg. Training loss: 3.9525 Avg. Reg. loss: 0.8304 0.0200 sec/batch\n",
      "Epoch 9/10 Iteration: 72000 Avg. Training loss: 3.9354 Avg. Reg. loss: 0.8273 0.0198 sec/batch\n",
      "Epoch 10/10 Iteration: 73000 Avg. Training loss: 3.9340 Avg. Reg. loss: 0.8235 0.0057 sec/batch\n",
      "Epoch 10/10 Iteration: 74000 Avg. Training loss: 3.9234 Avg. Reg. loss: 0.8266 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 75000 Avg. Training loss: 3.9399 Avg. Reg. loss: 0.8239 0.0202 sec/batch\n",
      "Epoch 10/10 Iteration: 76000 Avg. Training loss: 3.9221 Avg. Reg. loss: 0.8252 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 77000 Avg. Training loss: 3.9236 Avg. Reg. loss: 0.8253 0.0217 sec/batch\n",
      "Epoch 10/10 Iteration: 78000 Avg. Training loss: 3.9207 Avg. Reg. loss: 0.8279 0.0219 sec/batch\n",
      "Epoch 10/10 Iteration: 79000 Avg. Training loss: 3.9251 Avg. Reg. loss: 0.8222 0.0203 sec/batch\n",
      "Epoch 10/10 Iteration: 80000 Avg. Training loss: 3.9168 Avg. Reg. loss: 0.8203 0.0205 sec/batch\n",
      "Finish training at  2018-07-18 04:02:38.569593\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  500000.0\n",
      "Starting training at  2018-07-18 04:02:38.569956\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 6.3049 Avg. Reg. loss: 1.2558 0.0208 sec/batch\n",
      "Epoch 1/10 Iteration: 2000 Avg. Training loss: 5.2566 Avg. Reg. loss: 1.3443 0.0222 sec/batch\n",
      "Epoch 1/10 Iteration: 3000 Avg. Training loss: 4.8440 Avg. Reg. loss: 1.3260 0.0206 sec/batch\n",
      "Epoch 1/10 Iteration: 4000 Avg. Training loss: 4.6862 Avg. Reg. loss: 1.3046 0.0199 sec/batch\n",
      "Epoch 2/10 Iteration: 5000 Avg. Training loss: 4.5355 Avg. Reg. loss: 1.2935 0.0198 sec/batch\n",
      "Epoch 2/10 Iteration: 6000 Avg. Training loss: 4.4736 Avg. Reg. loss: 1.2765 0.0207 sec/batch\n",
      "Epoch 2/10 Iteration: 7000 Avg. Training loss: 4.4312 Avg. Reg. loss: 1.2568 0.0208 sec/batch\n",
      "Epoch 2/10 Iteration: 8000 Avg. Training loss: 4.3959 Avg. Reg. loss: 1.2388 0.0210 sec/batch\n",
      "Epoch 3/10 Iteration: 9000 Avg. Training loss: 4.3373 Avg. Reg. loss: 1.2349 0.0190 sec/batch\n",
      "Epoch 3/10 Iteration: 10000 Avg. Training loss: 4.3294 Avg. Reg. loss: 1.2160 0.0190 sec/batch\n",
      "Epoch 3/10 Iteration: 11000 Avg. Training loss: 4.2906 Avg. Reg. loss: 1.2000 0.0206 sec/batch\n",
      "Epoch 3/10 Iteration: 12000 Avg. Training loss: 4.2898 Avg. Reg. loss: 1.1859 0.0208 sec/batch\n",
      "Epoch 4/10 Iteration: 13000 Avg. Training loss: 4.2422 Avg. Reg. loss: 1.1822 0.0181 sec/batch\n",
      "Epoch 4/10 Iteration: 14000 Avg. Training loss: 4.2341 Avg. Reg. loss: 1.1672 0.0208 sec/batch\n",
      "Epoch 4/10 Iteration: 15000 Avg. Training loss: 4.2171 Avg. Reg. loss: 1.1573 0.0206 sec/batch\n",
      "Epoch 4/10 Iteration: 16000 Avg. Training loss: 4.2099 Avg. Reg. loss: 1.1425 0.0207 sec/batch\n",
      "Epoch 5/10 Iteration: 17000 Avg. Training loss: 4.1879 Avg. Reg. loss: 1.1435 0.0175 sec/batch\n",
      "Epoch 5/10 Iteration: 18000 Avg. Training loss: 4.1711 Avg. Reg. loss: 1.1295 0.0221 sec/batch\n",
      "Epoch 5/10 Iteration: 19000 Avg. Training loss: 4.1621 Avg. Reg. loss: 1.1224 0.0208 sec/batch\n",
      "Epoch 5/10 Iteration: 20000 Avg. Training loss: 4.1596 Avg. Reg. loss: 1.1100 0.0202 sec/batch\n",
      "Epoch 6/10 Iteration: 21000 Avg. Training loss: 4.1298 Avg. Reg. loss: 1.1097 0.0166 sec/batch\n",
      "Epoch 6/10 Iteration: 22000 Avg. Training loss: 4.1251 Avg. Reg. loss: 1.0986 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 23000 Avg. Training loss: 4.1170 Avg. Reg. loss: 1.0927 0.0207 sec/batch\n",
      "Epoch 6/10 Iteration: 24000 Avg. Training loss: 4.1139 Avg. Reg. loss: 1.0834 0.0210 sec/batch\n",
      "Epoch 7/10 Iteration: 25000 Avg. Training loss: 4.0895 Avg. Reg. loss: 1.0818 0.0161 sec/batch\n",
      "Epoch 7/10 Iteration: 26000 Avg. Training loss: 4.0905 Avg. Reg. loss: 1.0715 0.0213 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Iteration: 27000 Avg. Training loss: 4.0798 Avg. Reg. loss: 1.0661 0.0207 sec/batch\n",
      "Epoch 7/10 Iteration: 28000 Avg. Training loss: 4.0695 Avg. Reg. loss: 1.0595 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 29000 Avg. Training loss: 4.0536 Avg. Reg. loss: 1.0585 0.0147 sec/batch\n",
      "Epoch 8/10 Iteration: 30000 Avg. Training loss: 4.0583 Avg. Reg. loss: 1.0484 0.0206 sec/batch\n",
      "Epoch 8/10 Iteration: 31000 Avg. Training loss: 4.0450 Avg. Reg. loss: 1.0471 0.0203 sec/batch\n",
      "Epoch 8/10 Iteration: 32000 Avg. Training loss: 4.0375 Avg. Reg. loss: 1.0405 0.0208 sec/batch\n",
      "Epoch 9/10 Iteration: 33000 Avg. Training loss: 4.0184 Avg. Reg. loss: 1.0419 0.0139 sec/batch\n",
      "Epoch 9/10 Iteration: 34000 Avg. Training loss: 4.0208 Avg. Reg. loss: 1.0361 0.0203 sec/batch\n",
      "Epoch 9/10 Iteration: 35000 Avg. Training loss: 4.0178 Avg. Reg. loss: 1.0327 0.0207 sec/batch\n",
      "Epoch 9/10 Iteration: 36000 Avg. Training loss: 4.0060 Avg. Reg. loss: 1.0264 0.0200 sec/batch\n",
      "Epoch 10/10 Iteration: 37000 Avg. Training loss: 3.9926 Avg. Reg. loss: 1.0280 0.0131 sec/batch\n",
      "Epoch 10/10 Iteration: 38000 Avg. Training loss: 3.9885 Avg. Reg. loss: 1.0240 0.0207 sec/batch\n",
      "Epoch 10/10 Iteration: 39000 Avg. Training loss: 3.9904 Avg. Reg. loss: 1.0202 0.0205 sec/batch\n",
      "Epoch 10/10 Iteration: 40000 Avg. Training loss: 3.9769 Avg. Reg. loss: 1.0159 0.0206 sec/batch\n",
      "Finish training at  2018-07-18 04:16:35.720634\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  100000.0\n",
      "Starting training at  2018-07-18 04:16:35.720934\n",
      "Epoch 2/10 Iteration: 1000 Avg. Training loss: 6.2903 Avg. Reg. loss: 1.2125 0.0040 sec/batch\n",
      "Epoch 3/10 Iteration: 2000 Avg. Training loss: 5.1271 Avg. Reg. loss: 1.2786 0.0079 sec/batch\n",
      "Epoch 4/10 Iteration: 3000 Avg. Training loss: 4.6718 Avg. Reg. loss: 1.3176 0.0118 sec/batch\n",
      "Epoch 5/10 Iteration: 4000 Avg. Training loss: 4.4397 Avg. Reg. loss: 1.3418 0.0157 sec/batch\n",
      "Epoch 7/10 Iteration: 5000 Avg. Training loss: 4.3084 Avg. Reg. loss: 1.3770 0.0030 sec/batch\n",
      "Epoch 8/10 Iteration: 6000 Avg. Training loss: 4.2076 Avg. Reg. loss: 1.4063 0.0072 sec/batch\n",
      "Epoch 9/10 Iteration: 7000 Avg. Training loss: 4.1426 Avg. Reg. loss: 1.4331 0.0111 sec/batch\n",
      "Epoch 10/10 Iteration: 8000 Avg. Training loss: 4.0898 Avg. Reg. loss: 1.4588 0.0151 sec/batch\n",
      "Finish training at  2018-07-18 04:19:23.169804\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  50000.0\n",
      "Starting training at  2018-07-18 04:19:23.170263\n",
      "Epoch 3/10 Iteration: 1000 Avg. Training loss: 6.2341 Avg. Reg. loss: 1.2469 0.0039 sec/batch\n",
      "Epoch 5/10 Iteration: 2000 Avg. Training loss: 4.9773 Avg. Reg. loss: 1.4052 0.0068 sec/batch\n",
      "Epoch 8/10 Iteration: 3000 Avg. Training loss: 4.4674 Avg. Reg. loss: 1.4637 0.0035 sec/batch\n",
      "Epoch 10/10 Iteration: 4000 Avg. Training loss: 4.2176 Avg. Reg. loss: 1.5276 0.0074 sec/batch\n",
      "Finish training at  2018-07-18 04:20:46.006651\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "Tokens:  10000.0\n",
      "Starting training at  2018-07-18 04:20:46.007104\n",
      "Finish training at  2018-07-18 04:21:02.623604\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(idx_pairs)):\n",
    "    current_tokens = tokens_lst[i] * 1000000\n",
    "    \n",
    "    print(\"Tokens: \", current_tokens)\n",
    "    print(\"Starting training at \", datetime.datetime.now())\n",
    "    t0 = time.time()\n",
    "\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        iteration = 1\n",
    "        loss = 0\n",
    "        regular_loss = 0\n",
    "        loss_best = 100\n",
    "        loss_list = []\n",
    "        iteration_best = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            batches = get_batches(idx_pairs[i], batch_size)\n",
    "            start = time.time()\n",
    "            for x, y in batches:\n",
    "                feed = {inputs: x,\n",
    "                        labels: np.array(y)[:, None]}\n",
    "                sess.run(update_embed_op)\n",
    "                train_loss, _, regu_loss = sess.run([total_cost, optimizer, reg_loss], feed_dict=feed)\n",
    "#                 train_loss, _ = sess.run([total_cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                loss += train_loss\n",
    "                regular_loss += regu_loss\n",
    "\n",
    "                if loss < loss_best:\n",
    "                    W = sess.run(embedding).tolist()\n",
    "                    iteration_best = iteration\n",
    "                    loss_best = loss\n",
    "\n",
    "                if iteration % 1000 == 0:\n",
    "                    end = time.time()\n",
    "                    loss_list.append(loss / 1000)\n",
    "                    print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(loss / 1000),\n",
    "                          \"Avg. Reg. loss: {:.4f}\".format(regular_loss / 100),\n",
    "                          \"{:.4f} sec/batch\".format((end - start) / 1000))\n",
    "\n",
    "\n",
    "                    loss = 0\n",
    "                    regular_loss = 0\n",
    "                    start = time.time()\n",
    "                iteration += 1\n",
    "                \n",
    "        np.save('w2v_pivots1000_'+str(tokens_lst[i])+'m.npy',np.array(W))        \n",
    "        print(\"Finish training at \", datetime.datetime.now()) \n",
    "        print(\"-------------------------------------------------------------------------\") \n",
    "        print(\"-------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.696653425693512, -0.4016351103782654, -0.15490229427814484, -0.153431236743927, 0.11273664981126785, -0.20632797479629517, -0.05852844938635826, 0.18393464386463165, 0.04037215933203697, -0.15603983402252197, -0.12679000198841095, 0.10461419820785522, -0.03136150911450386, -0.09917640686035156, -0.21953696012496948, -0.06557910144329071, -0.3572455048561096, -0.07304935902357101, 0.2829059362411499, 0.25940605998039246, 0.18046262860298157, -0.18454191088676453, -0.13335512578487396, -0.11446908116340637, -0.09217895567417145, -0.028645846992731094, 0.07994083315134048, -0.3566879630088806, -0.16788771748542786, -0.09856567531824112, -0.05210083723068237, -0.06661748886108398, 0.09986916929483414, 0.1596103459596634, -0.1205173209309578, -0.03440592437982559, 0.028155574575066566, -0.17301133275032043, -0.17946180701255798, -0.0042143226601183414, -0.18912769854068756, -0.17107552289962769, -0.14589069783687592, -0.08563197404146194, -0.043947286903858185, -0.053388938307762146, 0.11201006174087524, -0.2840996980667114, -0.21458014845848083, 0.03424297273159027]\n",
      "[-0.6714518666267395, -0.397854745388031, -0.22400067746639252, -0.11230164021253586, 0.03792639076709747, -0.23540274798870087, 0.010916607454419136, 0.14502541720867157, 0.14910753071308136, -0.1241375207901001, -0.06717413663864136, 0.10475543141365051, -0.06811563670635223, -0.17020389437675476, -0.24623776972293854, 0.026321668177843094, -0.3198947608470917, 0.016895517706871033, 0.12025859951972961, 0.23876900970935822, 0.273358553647995, -0.2074965089559555, -0.03477171063423157, -0.013472441583871841, -0.038002073764801025, -0.07621145248413086, 0.027983732521533966, -0.24950556457042694, -0.16780859231948853, -0.04019904509186745, -0.16331572830677032, 0.00899406336247921, 0.12188966572284698, 0.043372541666030884, -0.14826881885528564, -0.08657801151275635, 0.01910969987511635, -0.12576808035373688, -0.17746634781360626, -0.008646461181342602, -0.1491350531578064, -0.09115835279226303, -0.1268480122089386, -0.04936075955629349, 0.02550850622355938, -0.226204052567482, 0.061961252242326736, -0.2975235879421234, -0.18389356136322021, 0.07822413742542267]\n"
     ]
    }
   ],
   "source": [
    "print(pivots_dict[1])\n",
    "print(W[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
